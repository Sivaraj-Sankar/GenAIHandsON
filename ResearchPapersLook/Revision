Revision and Preparation
1) How the Event Architecture Working 
2) What is Context 
3) MCP Context giving 
4) How to interact with Database
5) How to Run the Docker Compose File
6) Check the OpenAI and ChatOpenAI and ConversationChain 

FastAPI 
1) Request and Response Schema   vs Django 
2) Database Schema 
3) Authentication and Authorization 
OAuth2 / JWT Tokens	Token-based authentication
API Keys (Header/Query)	Simple service-to-service authentication
Session Cookies	Browser-based login
Custom Middleware	Low-level request inspection

from fastapi.security import OAuth2PasswordBearer
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="token")
@app.get("/users/me")
async def read_users_me(token: str = Depends(oauth2_scheme)):
    return {"token": token}

Middleware	Global checks for every route	Medium	API token headers, IP blocking
Dependencies	Per-route security	Low	JWT, OAuth, API Key
SecurityUtils	Built-in security flows	High	OAuth2, password login, etc.

‚úÖ Example Use Cases for Middleware
‚úÖ Check if the request is coming from a trusted IP
‚úÖ Validate static API tokens
‚úÖ Add logging or tracing
‚úÖ Modify request/response headers

üö´ ‚ùå Avoid putting complex logic (like DB validation) in middleware ‚Äî use route dependencies instead


4) Session Management  - again user logins - how to reload the resume of their work - That's why we need session management 
4.1) User Management 
4.2) State Mangement 
5) Different User Logins at same time - means Thread 
6) Database connection for different database and schema handled by FastAPI, or I have to create the query for No-SQL Database 
7) Middle ware in FastAPI
8) Plugin and Dependencies 
üß± Common Plugin-Like Patterns in FastAPI
1. üîÑ Reusable Middleware
Add cross-cutting logic like auth, rate limiting, logging.

python
Copy
Edit
app.add_middleware(SomeMiddleware)
2. üì¶ Dependency Injection
Inject reusable logic like DB sessions, JWT validation, etc.

python
Copy
Edit
@app.get("/items")
def read_items(user=Depends(get_current_user)):
    ...
3. üß© Modular Routers
Break your app into reusable route modules.

python
Copy
Edit
from fastapi import APIRouter

router = APIRouter()

@router.get("/health")
def health_check():
    return {"status": "ok"}

# Plug into main app
app.include_router(router)

Real Examples of ‚ÄúPlugin‚Äù Libraries
Plugin Type	Example Library	What It Adds
Auth/JWT	fastapi-jwt-auth, fastapi-users	Auth systems, user mgmt, token support
Rate Limiting	slowapi	Add rate limiting via middleware
CORS	CORSMiddleware from fastapi	Cross-origin request support
Monitoring	prometheus_fastapi_instrumentator	Metrics for Prometheus/Grafana
Tracing/Logs	opentelemetry-instrumentation-fastapi	Tracing support (OpenTelemetry)
Admin UI	fastapi-admin	Admin dashboards


React - Router 
--------------
import { useParams } from 'react-router-dom';
const { record_id } = useParams();
fetch(`/records/${record_id}`, { method: 'POST' });

Sometimes you‚Äôve previously loaded a record or selected it, and the ID is in app state (React state, Redux, etc).
const recordId = selectedRecord.id;
fetch(`/records/${recordId}`, { method: 'POST' });




Preparation
1) Resume the MultiAgentic System 
2) Memory Component - Done [ How to save the Memory to Generate the Justification]
3) 



Memory Component 
----------------
Working - [similar to human short-term memory]
        - It coordinates information flow between various memory systems.
Episodic
Semantic
Procedural memory - differ from other memory
Semantic memory
Dynamic Few-Shot Prompting 


Simulate human-like problem-solving and contextual understanding,
**Utimately improving AI interactions and learning from past experiences

Working Memory
- Working memory is the first type of memory explored, crucial for maintaining context during interactions
- The AI assistant's working memory is built by appending user messages to a list, allowing for continuous updates and context retrieval during conversations
Episodic Memory [Part of long-term explicit memory] [comprises a person‚Äôs unique recollection of experiences, events, and situations]
- Episodic memory captures historical experiences and their takeaways, allowing chatbots to learn from past interactions. 
- Episodic memory enables the AI to recall past interactions
- Extracting valuable insights and learnings that inform its responses in future conversations.
- This leads to more personalized and contextually aware responses
- By using Vector databases for semantic recall, the system can store and retrieve episodic memories efficiently
- The **dynamic modification of system prompts helps integrate the latest memories into conversations, ensuring that past interactions inform current discussions effectively.
This enables better contextualization in future conversations.
Semantic Memory
- Semantic memory provides knowledge context and factual grounding, essential for effective communication
Dynamic Few-shot Prompting 
- Dynamic **few-shot prompting helps the AI generate relevant responses by automatically providing successful examples and instructions based on prior interactions
Procedural Memory 
- Procedural memory is distinct from other memory types, focusing on how we remember to perform tasks without conscious recall.
- This memory type enables the execution of complex actions automatically.
- Examples of procedural memory include riding a bike or typing, activities we perform **unconsciously.
- These tasks demonstrate our ability to remember actions without active thought.
- Implementing procedural memory in AI involves fine-tuning language models, which can be complex and time-consuming. This process requires updating the core code to enhance system behavior.
- Research in AI has explored systems like **retrospective language models that internalize memories for improved responses.
  - These innovations aim to enhance the learning capabilities of language models.

Steps for Memories in Conversation
- The conversational flow utilizes episodic memory to enhance interactions, allowing the AI to remember past user inputs for more personalized responses. This dynamic updating process improves the user experience by retaining context across conversations.
- **The system converts working memory into episodic memory at the **end of conversations, ensuring that relevant information is stored for future reference. This process allows for continuity in user interactions.

1.Working memory acts as the immediate cognitive workspace.
   - conversational context observed through real-time interactions
   - Remembering and Learning from Short-Term memory 

    Learning from short-term memory - dynamic integration of new messages into the active context 
      - Updating the stateful representation of the entire conversation 
2.Episodic memory retains historical experiences and insights.
    - essentially being able to recall over a series of Prior Working Memory 
    - Included Raw Conversations, 
    - Included Analytical Understanding from raw data
   - Remembering in episodic memory - done with few-shot prompting 
   - Learning from past recall as well - 
     - Two Processes 
     Automatic Storage of past complete conversation 
     Generation of Post Conversation Analysis 
3.Semantic memory provides a representation of knowledge separate from personal experiences, enabling models to access explicit facts that ground their responses and integrate dynamic knowledge.
4.Procedural memory encompasses the model's weights and code structure, influencing how other memory systems operate and fundamentally altering the agent's functional behavior.

Understanding different memory systems can significantly enhance the **cognitive capabilities of language models
Implementing advanced **cognitive architectures can lead to better contextualization in language models, addressing limitations commonly found in less complex systems.


OpenAI Response 
<OpenAIObject chat.completion id=chatcmpl-AVCsmqFCt4FILgYpy7wDxpjahOZvC at 0x7f9d08a55270> JSON: {
  "id": "chatcmpl-AVCsmqFCt4FILgYpy7wDxpjahOZvC",
  "object": "chat.completion",
  "created": 1732001052,
  "model": "gpt-3.5-turbo-0125",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": null,
        "function_call": {
          "name": "get_current_weather",
          "arguments": "{\"location\":\"Boston\",\"unit\":\"celsius\"}"
        },
        "refusal": null
      },
      "logprobs": null,
      "finish_reason": "function_call"
    }
  ],
  "usage": {
    "prompt_tokens": 82,
    "completion_tokens": 20,
    "total_tokens": 102,
    "prompt_tokens_details": {
      "cached_tokens": 0,
      "audio_tokens": 0
    },
    "completion_tokens_details": {
      "reasoning_tokens": 0,
      "audio_tokens": 0,
      "accepted_prediction_tokens": 0,
      "rejected_prediction_tokens": 0
    }
  },
  "system_fingerprint": null
}
response_message = response["choices"][0]["message"]
<OpenAIObject at 0x7f9cd9678b80> JSON: {
  "role": "assistant",
  "content": null,
  "function_call": {
    "name": "get_current_weather",
    "arguments": "{\"location\":\"Boston\",\"unit\":\"celsius\"}"
  },
  "refusal": null
}

response_message["content"]

response_message["function_call"]
<OpenAIObject at 0x7f9cd9678bd0> JSON: {
  "name": "get_current_weather",
  "arguments": "{\"location\":\"Boston\",\"unit\":\"celsius\"}"
}

json.loads(response_message["function_call"]["arguments"])
{'location': 'Boston', 'unit': 'celsius'}

Langchain Prompt Template - from Template
ChatPromptTemplate(input_variables=['topic'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], template='tell me a short joke about {topic}'))])
Langchain OpenAI - ChatOpenAI
AIMessage(content='Why did the bear bring a flashlight to the party? \n\nBecause he heard it was going to be a "beary" good time!')

Key Things to Discuss for MCP 
------------------------------
1) How Functions Converting to the Pydantic Objects 
2) 


Structured Ouput 
JSON Mode  - response_format="json"
Function Call / Tool Use
JSON Schema 

OpenAI Different Endpoint
pip install openai
from openai import OpenAI
client = OpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)
response = client.responses.create(
    model="gpt-4o",
    instructions="You are a coding assistant that talks like a pirate.",
    input="How do I check if a Python object is an instance of a class?",
)
print(response.output_text)

completion = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "developer", "content": "Talk like a pirate."},
        {
            "role": "user",
            "content": "How do I check if a Python object is an instance of a class?",
        },
    ],
)

print(completion.choices[0].message.content)
prompt = "What is in this image?"
img_url = "https://upload.wikimedia.org/wikipedia/commons/thumb/d/d5/2023_06_08_Raccoon1.jpg/1599px-2023_06_08_Raccoon1.jpg"

response = client.responses.create(
    model="gpt-4o-mini",
    input=[
        {
            "role": "user",
            "content": [
                {"type": "input_text", "text": prompt},
                {"type": "input_image", "image_url": f"{img_url}"},
            ],
        }
    ],
)
import base64
from openai import OpenAI

client = OpenAI()

prompt = "What is in this image?"
with open("path/to/image.png", "rb") as image_file:
    b64_image = base64.b64encode(image_file.read()).decode("utf-8")

response = client.responses.create(
    model="gpt-4o-mini",
    input=[
        {
            "role": "user",
            "content": [
                {"type": "input_text", "text": prompt},
                {"type": "input_image", "image_url": f"data:image/png;base64,{b64_image}"},
            ],
        }
    ],
)
import os
import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)


async def main() -> None:
    response = await client.responses.create(
        model="gpt-4o", input="Explain disestablishmentarianism to a smart five year old."
    )
    print(response.output_text)


asyncio.run(main())

OpenAI Old Version - pip install openai==0.28 
---------------------------------------------
Installing collected packages: openai
  Attempting uninstall: openai
    Found existing installation: openai 1.85.0
    Uninstalling openai-1.85.0:
      Successfully uninstalled openai-1.85.0

llama-index-agent-openai 0.4.11 requires openai>=1.14.0, but you have openai 0.28.0 which is incompatible.
llama-index-embeddings-openai 0.3.1 requires openai>=1.1.0, but you have openai 0.28.0 which is incompatible.
llama-index-llms-openai 0.4.5 requires openai<2,>=1.81.0, but you have openai 0.28.0 which is incompatible.


import openai

openai.api_key = "sk-your-api-key"

response = openai.ChatCompletion.create(
    model="gpt-4o",  # Or any GPT-4.5, GPT-4-turbo, GPT-3.5-turbo
    messages=[
        {"role": "user", "content": "Generate a product JSON with name, description, price."}
    ],
    response_format="json"  # <- THIS forces valid JSON response
)

print(response.choices[0].message.content)


import openai

openai.api_key = "sk-your-api-key"

schema = {
    "type": "object",
    "properties": {
        "name": { "type": "string", "description": "Name of the product" },
        "description": { "type": "string", "description": "Description of the product" },
        "price": { "type": "number", "description": "Price of the product in USD" }
    },
    "required": ["name", "description", "price"]
}

response = openai.ChatCompletion.create(
    model="gpt-4o",  # or "gpt-4.5-turbo", etc.
    messages=[
        {"role": "user", "content": "Suggest a product"}
    ],
    response_format="json_schema",  # <- JSON Schema mode
    parameters=schema  # <- Your schema
)

print(response.choices[0].message.content)


Function_Calling 

functions = [
    {
        "name": "create_product",
        "description": "Generate a product name, description and price.",
        "parameters": {
            "type": "object",
            "properties": {
                "name": {"type": "string", "description": "Name of the product"},
                "description": {"type": "string", "description": "Description of the product"},
                "price": {"type": "number", "description": "Price of the product in USD"}
            },
            "required": ["name", "description", "price"]
        }
    }
]


‚úÖ Summary of the flow
1Ô∏è‚É£ You define function metadata (name + parameters)
2Ô∏è‚É£ You call ChatCompletion with tool_choice="required" and tools=functions
3Ô∏è‚É£ The model will return arguments for the function in structured JSON ‚Äî no extra text
4Ô∏è‚É£ You can parse this output ‚Üí run backend code, store in a DB, call APIs, etc.

import openai

openai.api_key = "sk-your-api-key"

response = openai.ChatCompletion.create(
    model="gpt-4o",  # Or "gpt-4.5-turbo", "gpt-4-turbo", "gpt-3.5-turbo-0125", etc.
    messages=[
        {"role": "user", "content": "Suggest a new coffee mug product for our store"}
    ],
    tools=[
        {
            "type": "function",
            "function": functions[0]  # Or use the whole functions list
        }
    ],
    tool_choice="required"  # Force the model to use the function
)

# Print out the structured function call
print(response.choices[0].message.tool_calls[0])

{
  "function": "create_product",
  "arguments": {
    "name": "Modern Double-Walled Coffee Mug",
    "description": "A sleek double-walled glass mug that keeps your coffee hot while staying cool to the touch.",
    "price": 18.99
  }
}


Function Call
{
  "tool_choice": "required",
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "create_product",
        "parameters": {
          "type": "object",
          "properties": {
            "name": { "type": "string" },
            "description": { "type": "string" },
            "price": { "type": "number" }
          },
          "required": ["name", "description", "price"]
        }
      }
    }
  ]
}

Response 
{
  "tool_calls": [
    {
      "function": "create_product",
      "arguments": {
        "name": "Classic Ceramic Coffee Mug",
        "description": "A sturdy ceramic mug perfect for your morning coffee. Dishwasher and microwave safe.",
        "price": 12.99
      }
    }
  ]
}

Json_Schema

{
  "response_format": "json_schema",
  "parameters": {
    "type": "object",
    "properties": {
      "name": { "type": "string" },
      "description": { "type": "string" },
      "price": { "type": "number" }
    },
    "required": ["name", "description", "price"]
  }
}

{
  "name": "Classic Ceramic Coffee Mug",
  "description": "A sturdy ceramic mug perfect for your morning coffee. Dishwasher and microwave safe.",
  "price": 12.99
}


Document Generation 
Method	Pros	Cons
python-docx	Easy, popular	No direct column control
python-docx-template	Works with column templates	Manual template needed
Pandoc	Converts LaTeX/Markdown perfectly	Needs install, CLI tool
Word COM (win32com)	Full control	Windows-only, needs MS Word installed

Method 3 ‚Äî Using Word COM Automation (Windows-only):
python
Copy
Edit
import win32com.client

word = win32com.client.Dispatch("Word.Application")
word.Visible = False

doc = word.Documents.Add()

# Set columns
section = doc.Sections(1)
section.PageSetup.TextColumns.SetCount(2)

# Add text
range = doc.Range(0,0)
range.InsertAfter("The Impact of Artificial Intelligence on Modern Healthcare Systems\n")
range.InsertAfter("Abstract\nArtificial Intelligence (AI) is revolutionizing healthcare...\n")

# Save doc
doc.SaveAs(r"generated_two_column.docx")
doc.Close()
word.Quit()

-----------------
pandoc paper.md -o paper.docx --reference-doc=template.docx

Method 2 ‚Äî Using Pandoc (best for LaTeX ‚Üí DOCX conversion)
If you write the paper in Markdown or LaTeX, you can generate a two-column DOCX like this:

bash
Copy
Edit
pandoc paper.md -o paper.docx --reference-doc=template.docx
Or:

bash
Copy
Edit
pandoc paper.tex -o paper.docx --reference-doc=template.docx

Pandoc is great for:
‚úÖ Converting LaTeX to Word
‚úÖ Converting Markdown to Word
‚úÖ Applying two-column styles via reference DOCX


-----------------------
Great question ‚Äî if you want to generate a .docx Word file with a two-column layout programmatically, you can do it with Python using libraries like:

python-docx ‚Üí general DOCX creation/editing (but does not support columns natively)

python-docx-template ‚Üí if you pre-make a .docx template with columns

pydocx ‚Üí convert HTML to .docx (you can style columns in HTML)

docxtpl + jinja2 ‚Üí generate dynamic DOCX content with templates

win32com.client ‚Üí control MS Word COM API (only on Windows ‚Äî full Word feature control)

pandoc ‚Üí convert LaTeX / Markdown ‚Üí Word with columns (very powerful!)


In Word, LaTeX, or Google Docs, this is done by using:

Two-column layout (Page Layout ‚Üí Columns ‚Üí Two)

Sometimes, section breaks to control where columns start

Figures and tables can either span both columns or stay in one

-----------------------
Running and Downloading 
-----------------------
Saving the Script as .py file 

# Save script to file
with open("generated_docx_script.py", "w") as f:
    f.write(generated_script_text)

# Run the script
import subprocess
subprocess.run(["python", "generated_docx_script.py"])

-------
2Ô∏è‚É£ Execute Generated Script In-Memory (No .py File)
If you don‚Äôt want to write it to a file, you can execute it in-memory using exec():

python
Copy
Edit
# Assume generated_script_text is the Python-docx code as a string

exec(generated_script_text)
Caution: exec() runs any code, so make sure your script is trusted.

---------------
If you can change the format of what the LLM generates, ask it to produce a function, instead of a full script. Then you can just call it:

python
Copy
Edit
# Example: generated function from LLM
generated_code = """
def generate_report():
    from docx import Document
    doc = Document()
    doc.add_heading('LLM Generated Report', 0)
    doc.add_paragraph('This is the content...')
    doc.save('final_report.docx')
"""

exec(generated_code)
generate_report()


---------------------------------
Flask Script 
from flask import Flask, request, send_file
import subprocess
import tempfile
import os

app = Flask(__name__)

@app.route('/generate-report', methods=['POST'])
def generate_report():
    # Example: receiving the LLM-generated python-docx script
    generated_script_text = request.json.get('script')
    
    # Create a temp directory to store the script and output file
    with tempfile.TemporaryDirectory() as tmpdirname:
        script_path = os.path.join(tmpdirname, "generated_script.py")
        output_docx_path = os.path.join(tmpdirname, "final_report.docx")
        
        # Modify the script to save docx to our path
        generated_script_text += f"\ndoc.save(r'{output_docx_path}')\n"
        
        # Write script to file
        with open(script_path, "w") as f:
            f.write(generated_script_text)
        
        # Run the script
        subprocess.run(["python", script_path], check=True)
        
        # Send the .docx as a download
        return send_file(output_docx_path, as_attachment=True, download_name="final_report.docx")

if __name__ == '__main__':
    app.run(debug=True)




POST /generate-report
Content-Type: application/json

{
    "script": "
from docx import Document
doc = Document()
doc.add_heading('My Report', 0)
doc.add_paragraph('This report was generated by an LLM.')
"
}

---------------------------------------------
You are helping create report content.
Please respond in this JSON format:

{
    "title": "Report Title Here",
    "sections": [
        {
            "heading": "Section 1 Heading",
            "content": "Section 1 content goes here..."
        },
        {
            "heading": "Section 2 Heading",
            "content": "Section 2 content goes here..."
        }
    ]
}

--------------
from flask import Flask, request, send_file, jsonify
from docx import Document
import tempfile
import os

app = Flask(__name__)

@app.route('/generate-report', methods=['POST'])
def generate_report():
    # Receive structured content (safe, no code)
    data = request.json
    
    # Generate docx using fixed python-docx logic
    with tempfile.TemporaryDirectory() as tmpdirname:
        output_docx_path = os.path.join(tmpdirname, "final_report.docx")
        
        # Use your trusted template logic
        doc = Document()
        doc.add_heading(data['title'], 0)
        
        for section in data.get('sections', []):
            doc.add_heading(section['heading'], level=1)
            doc.add_paragraph(section['content'])
        
        doc.save(output_docx_path)
        
        return send_file(output_docx_path, as_attachment=True, download_name="final_report.docx")

if __name__ == '__main__':
    app.run(debug=True)




POST /generate-report
Content-Type: application/json

{
    "title": "Monthly Sales Report",
    "sections": [
        {
            "heading": "Executive Summary",
            "content": "This month we saw a 10% increase in sales."
        },
        {
            "heading": "Details",
            "content": "Product A led the growth, with strong performance in region X."
        }
    ]
}


----------------------------------------------------
Please generate a complete python-docx script that creates a .docx report with the following content:

Title: "Monthly Sales Report"

Sections:
1. Executive Summary - "This month we saw a 10% increase in sales."
2. Details - "Product A led the growth, with strong performance in region X."

The script should:
- Import python-docx
- Create a Document
- Add headings and paragraphs
- Save the document as 'final_report.docx'

-----------------------------------------------
from docx import Document

doc = Document()
doc.add_heading('Monthly Sales Report', 0)

doc.add_heading('Executive Summary', level=1)
doc.add_paragraph('This month we saw a 10% increase in sales.')

doc.add_heading('Details', level=1)
doc.add_paragraph('Product A led the growth, with strong performance in region X.')

doc.save('final_report.docx')


--------------------------
Expert Prompt 


You are an expert Python developer.  
Please generate a complete and working Python script using the python-docx library.

Requirements:
- Create a Document
- Add a title: "Monthly Sales Report"
- Add two sections:
    - Section 1 Heading: "Executive Summary"
      Content: "This month we saw a 10% increase in sales."
    - Section 2 Heading: "Details"
      Content: "Product A led the growth, with strong performance in region X."
- Save the document as 'final_report.docx'

Rules:
- Only output valid Python code
- Do not use any markdown formatting (no ```python)
- Do not add explanations
- The script must be runnable as a standalone .py file



-----------
Safe Content Prompt 



You are generating structured report content.  
Please return the response in this exact JSON format:

{
    "title": "Report Title Here",
    "sections": [
        {
            "heading": "Section 1 Heading",
            "content": "Section 1 content goes here..."
        },
        {
            "heading": "Section 2 Heading",
            "content": "Section 2 content goes here..."
        }
    ]
}

Report type: Monthly Sales Report  
Report content:
- Title: "Monthly Sales Report"
- Section 1 Heading: "Executive Summary"
  Content: "This month we saw a 10% increase in sales."
- Section 2 Heading: "Details"
  Content: "Product A led the growth, with strong performance in region X."

Rules:
- Only return valid JSON
- Do not include any explanations
- Do not include markdown or comments


