Revision and Preparation
1) How the Event Architecture Working 
2) What is Context 
3) MCP Context giving 
4) How to interact with Database
5) How to Run the Docker Compose File
6) Check the OpenAI and ChatOpenAI and ConversationChain 

Preparation
1) Resume the MultiAgentic System 
2) Memory Component - Done [ How to save the Memory to Generate the Justification]
3) 



Memory Component 
----------------
Working - [similar to human short-term memory]
        - It coordinates information flow between various memory systems.
Episodic
Semantic
Procedural memory - differ from other memory
Semantic memory
Dynamic Few-Shot Prompting 


Simulate human-like problem-solving and contextual understanding,
**Utimately improving AI interactions and learning from past experiences

Working Memory
- Working memory is the first type of memory explored, crucial for maintaining context during interactions
- The AI assistant's working memory is built by appending user messages to a list, allowing for continuous updates and context retrieval during conversations
Episodic Memory [Part of long-term explicit memory] [comprises a person’s unique recollection of experiences, events, and situations]
- Episodic memory captures historical experiences and their takeaways, allowing chatbots to learn from past interactions. 
- Episodic memory enables the AI to recall past interactions
- Extracting valuable insights and learnings that inform its responses in future conversations.
- This leads to more personalized and contextually aware responses
- By using Vector databases for semantic recall, the system can store and retrieve episodic memories efficiently
- The **dynamic modification of system prompts helps integrate the latest memories into conversations, ensuring that past interactions inform current discussions effectively.
This enables better contextualization in future conversations.
Semantic Memory
- Semantic memory provides knowledge context and factual grounding, essential for effective communication
Dynamic Few-shot Prompting 
- Dynamic **few-shot prompting helps the AI generate relevant responses by automatically providing successful examples and instructions based on prior interactions
Procedural Memory 
- Procedural memory is distinct from other memory types, focusing on how we remember to perform tasks without conscious recall.
- This memory type enables the execution of complex actions automatically.
- Examples of procedural memory include riding a bike or typing, activities we perform **unconsciously.
- These tasks demonstrate our ability to remember actions without active thought.
- Implementing procedural memory in AI involves fine-tuning language models, which can be complex and time-consuming. This process requires updating the core code to enhance system behavior.
- Research in AI has explored systems like **retrospective language models that internalize memories for improved responses.
  - These innovations aim to enhance the learning capabilities of language models.

Steps for Memories in Conversation
- The conversational flow utilizes episodic memory to enhance interactions, allowing the AI to remember past user inputs for more personalized responses. This dynamic updating process improves the user experience by retaining context across conversations.
- **The system converts working memory into episodic memory at the **end of conversations, ensuring that relevant information is stored for future reference. This process allows for continuity in user interactions.

1.Working memory acts as the immediate cognitive workspace.
   - conversational context observed through real-time interactions
   - Remembering and Learning from Short-Term memory 

    Learning from short-term memory - dynamic integration of new messages into the active context 
      - Updating the stateful representation of the entire conversation 
2.Episodic memory retains historical experiences and insights.
    - essentially being able to recall over a series of Prior Working Memory 
    - Included Raw Conversations, 
    - Included Analytical Understanding from raw data
   - Remembering in episodic memory - done with few-shot prompting 
   - Learning from past recall as well - 
     - Two Processes 
     Automatic Storage of past complete conversation 
     Generation of Post Conversation Analysis 
3.Semantic memory provides a representation of knowledge separate from personal experiences, enabling models to access explicit facts that ground their responses and integrate dynamic knowledge.
4.Procedural memory encompasses the model's weights and code structure, influencing how other memory systems operate and fundamentally altering the agent's functional behavior.

Understanding different memory systems can significantly enhance the **cognitive capabilities of language models
Implementing advanced **cognitive architectures can lead to better contextualization in language models, addressing limitations commonly found in less complex systems.


OpenAI Response 
<OpenAIObject chat.completion id=chatcmpl-AVCsmqFCt4FILgYpy7wDxpjahOZvC at 0x7f9d08a55270> JSON: {
  "id": "chatcmpl-AVCsmqFCt4FILgYpy7wDxpjahOZvC",
  "object": "chat.completion",
  "created": 1732001052,
  "model": "gpt-3.5-turbo-0125",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": null,
        "function_call": {
          "name": "get_current_weather",
          "arguments": "{\"location\":\"Boston\",\"unit\":\"celsius\"}"
        },
        "refusal": null
      },
      "logprobs": null,
      "finish_reason": "function_call"
    }
  ],
  "usage": {
    "prompt_tokens": 82,
    "completion_tokens": 20,
    "total_tokens": 102,
    "prompt_tokens_details": {
      "cached_tokens": 0,
      "audio_tokens": 0
    },
    "completion_tokens_details": {
      "reasoning_tokens": 0,
      "audio_tokens": 0,
      "accepted_prediction_tokens": 0,
      "rejected_prediction_tokens": 0
    }
  },
  "system_fingerprint": null
}
response_message = response["choices"][0]["message"]
<OpenAIObject at 0x7f9cd9678b80> JSON: {
  "role": "assistant",
  "content": null,
  "function_call": {
    "name": "get_current_weather",
    "arguments": "{\"location\":\"Boston\",\"unit\":\"celsius\"}"
  },
  "refusal": null
}

response_message["content"]

response_message["function_call"]
<OpenAIObject at 0x7f9cd9678bd0> JSON: {
  "name": "get_current_weather",
  "arguments": "{\"location\":\"Boston\",\"unit\":\"celsius\"}"
}

json.loads(response_message["function_call"]["arguments"])
{'location': 'Boston', 'unit': 'celsius'}

Langchain Prompt Template - from Template
ChatPromptTemplate(input_variables=['topic'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], template='tell me a short joke about {topic}'))])
Langchain OpenAI - ChatOpenAI
AIMessage(content='Why did the bear bring a flashlight to the party? \n\nBecause he heard it was going to be a "beary" good time!')

Key Things to Discuss for MCP 
------------------------------
1) How Functions Converting to the Pydantic Objects 
2) 


Structured Ouput 
JSON Mode  - response_format="json"
Function Call / Tool Use
JSON Schema 

OpenAI Different Endpoint
pip install openai
from openai import OpenAI
client = OpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)
response = client.responses.create(
    model="gpt-4o",
    instructions="You are a coding assistant that talks like a pirate.",
    input="How do I check if a Python object is an instance of a class?",
)
print(response.output_text)

completion = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "developer", "content": "Talk like a pirate."},
        {
            "role": "user",
            "content": "How do I check if a Python object is an instance of a class?",
        },
    ],
)

print(completion.choices[0].message.content)
prompt = "What is in this image?"
img_url = "https://upload.wikimedia.org/wikipedia/commons/thumb/d/d5/2023_06_08_Raccoon1.jpg/1599px-2023_06_08_Raccoon1.jpg"

response = client.responses.create(
    model="gpt-4o-mini",
    input=[
        {
            "role": "user",
            "content": [
                {"type": "input_text", "text": prompt},
                {"type": "input_image", "image_url": f"{img_url}"},
            ],
        }
    ],
)
import base64
from openai import OpenAI

client = OpenAI()

prompt = "What is in this image?"
with open("path/to/image.png", "rb") as image_file:
    b64_image = base64.b64encode(image_file.read()).decode("utf-8")

response = client.responses.create(
    model="gpt-4o-mini",
    input=[
        {
            "role": "user",
            "content": [
                {"type": "input_text", "text": prompt},
                {"type": "input_image", "image_url": f"data:image/png;base64,{b64_image}"},
            ],
        }
    ],
)
import os
import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)


async def main() -> None:
    response = await client.responses.create(
        model="gpt-4o", input="Explain disestablishmentarianism to a smart five year old."
    )
    print(response.output_text)


asyncio.run(main())

OpenAI Old Version - pip install openai==0.28 
---------------------------------------------
Installing collected packages: openai
  Attempting uninstall: openai
    Found existing installation: openai 1.85.0
    Uninstalling openai-1.85.0:
      Successfully uninstalled openai-1.85.0

llama-index-agent-openai 0.4.11 requires openai>=1.14.0, but you have openai 0.28.0 which is incompatible.
llama-index-embeddings-openai 0.3.1 requires openai>=1.1.0, but you have openai 0.28.0 which is incompatible.
llama-index-llms-openai 0.4.5 requires openai<2,>=1.81.0, but you have openai 0.28.0 which is incompatible.


import openai

openai.api_key = "sk-your-api-key"

response = openai.ChatCompletion.create(
    model="gpt-4o",  # Or any GPT-4.5, GPT-4-turbo, GPT-3.5-turbo
    messages=[
        {"role": "user", "content": "Generate a product JSON with name, description, price."}
    ],
    response_format="json"  # <- THIS forces valid JSON response
)

print(response.choices[0].message.content)


import openai

openai.api_key = "sk-your-api-key"

schema = {
    "type": "object",
    "properties": {
        "name": { "type": "string", "description": "Name of the product" },
        "description": { "type": "string", "description": "Description of the product" },
        "price": { "type": "number", "description": "Price of the product in USD" }
    },
    "required": ["name", "description", "price"]
}

response = openai.ChatCompletion.create(
    model="gpt-4o",  # or "gpt-4.5-turbo", etc.
    messages=[
        {"role": "user", "content": "Suggest a product"}
    ],
    response_format="json_schema",  # <- JSON Schema mode
    parameters=schema  # <- Your schema
)

print(response.choices[0].message.content)


Function_Calling 

functions = [
    {
        "name": "create_product",
        "description": "Generate a product name, description and price.",
        "parameters": {
            "type": "object",
            "properties": {
                "name": {"type": "string", "description": "Name of the product"},
                "description": {"type": "string", "description": "Description of the product"},
                "price": {"type": "number", "description": "Price of the product in USD"}
            },
            "required": ["name", "description", "price"]
        }
    }
]


✅ Summary of the flow
1️⃣ You define function metadata (name + parameters)
2️⃣ You call ChatCompletion with tool_choice="required" and tools=functions
3️⃣ The model will return arguments for the function in structured JSON — no extra text
4️⃣ You can parse this output → run backend code, store in a DB, call APIs, etc.

import openai

openai.api_key = "sk-your-api-key"

response = openai.ChatCompletion.create(
    model="gpt-4o",  # Or "gpt-4.5-turbo", "gpt-4-turbo", "gpt-3.5-turbo-0125", etc.
    messages=[
        {"role": "user", "content": "Suggest a new coffee mug product for our store"}
    ],
    tools=[
        {
            "type": "function",
            "function": functions[0]  # Or use the whole functions list
        }
    ],
    tool_choice="required"  # Force the model to use the function
)

# Print out the structured function call
print(response.choices[0].message.tool_calls[0])

{
  "function": "create_product",
  "arguments": {
    "name": "Modern Double-Walled Coffee Mug",
    "description": "A sleek double-walled glass mug that keeps your coffee hot while staying cool to the touch.",
    "price": 18.99
  }
}


Function Call
{
  "tool_choice": "required",
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "create_product",
        "parameters": {
          "type": "object",
          "properties": {
            "name": { "type": "string" },
            "description": { "type": "string" },
            "price": { "type": "number" }
          },
          "required": ["name", "description", "price"]
        }
      }
    }
  ]
}

Response 
{
  "tool_calls": [
    {
      "function": "create_product",
      "arguments": {
        "name": "Classic Ceramic Coffee Mug",
        "description": "A sturdy ceramic mug perfect for your morning coffee. Dishwasher and microwave safe.",
        "price": 12.99
      }
    }
  ]
}

Json_Schema

{
  "response_format": "json_schema",
  "parameters": {
    "type": "object",
    "properties": {
      "name": { "type": "string" },
      "description": { "type": "string" },
      "price": { "type": "number" }
    },
    "required": ["name", "description", "price"]
  }
}

{
  "name": "Classic Ceramic Coffee Mug",
  "description": "A sturdy ceramic mug perfect for your morning coffee. Dishwasher and microwave safe.",
  "price": 12.99
}


Document Generation 
Method	Pros	Cons
python-docx	Easy, popular	No direct column control
python-docx-template	Works with column templates	Manual template needed
Pandoc	Converts LaTeX/Markdown perfectly	Needs install, CLI tool
Word COM (win32com)	Full control	Windows-only, needs MS Word installed

Method 3 — Using Word COM Automation (Windows-only):
python
Copy
Edit
import win32com.client

word = win32com.client.Dispatch("Word.Application")
word.Visible = False

doc = word.Documents.Add()

# Set columns
section = doc.Sections(1)
section.PageSetup.TextColumns.SetCount(2)

# Add text
range = doc.Range(0,0)
range.InsertAfter("The Impact of Artificial Intelligence on Modern Healthcare Systems\n")
range.InsertAfter("Abstract\nArtificial Intelligence (AI) is revolutionizing healthcare...\n")

# Save doc
doc.SaveAs(r"generated_two_column.docx")
doc.Close()
word.Quit()

-----------------
pandoc paper.md -o paper.docx --reference-doc=template.docx

Method 2 — Using Pandoc (best for LaTeX → DOCX conversion)
If you write the paper in Markdown or LaTeX, you can generate a two-column DOCX like this:

bash
Copy
Edit
pandoc paper.md -o paper.docx --reference-doc=template.docx
Or:

bash
Copy
Edit
pandoc paper.tex -o paper.docx --reference-doc=template.docx

Pandoc is great for:
✅ Converting LaTeX to Word
✅ Converting Markdown to Word
✅ Applying two-column styles via reference DOCX


-----------------------
Great question — if you want to generate a .docx Word file with a two-column layout programmatically, you can do it with Python using libraries like:

python-docx → general DOCX creation/editing (but does not support columns natively)

python-docx-template → if you pre-make a .docx template with columns

pydocx → convert HTML to .docx (you can style columns in HTML)

docxtpl + jinja2 → generate dynamic DOCX content with templates

win32com.client → control MS Word COM API (only on Windows — full Word feature control)

pandoc → convert LaTeX / Markdown → Word with columns (very powerful!)


In Word, LaTeX, or Google Docs, this is done by using:

Two-column layout (Page Layout → Columns → Two)

Sometimes, section breaks to control where columns start

Figures and tables can either span both columns or stay in one

