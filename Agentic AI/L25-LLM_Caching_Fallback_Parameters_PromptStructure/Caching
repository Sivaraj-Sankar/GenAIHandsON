from langchain_core.globals import set_llm_cache
from langchain_openai import OpenAI

# To make the caching really obvious, let's use a slower and older model.
# Caching supports newer chat models as well.
llm = OpenAI(model="gpt-3.5-turbo-instruct", n=2, best_of=2)
n --> return 
best_of --> internally generates the response 
----------------------------------------------
from langchain_core.caches import InMemoryCache
set_llm_cache(InMemoryCache())

# The first time, it is not yet in cache, so it should take longer
llm.invoke("Tell me a joke")
---------------------------------------------
SQLite database (a lightweight file-based database).
# We can do the same thing with a SQLite cache
from langchain_community.cache import SQLiteCache

set_llm_cache(SQLiteCache(database_path=".langchain.db"))


Redis Caching 
-------------
Redis (Remote Dictionary Server)
is an open-source in-memory storage
used as a distributed, in-memory key–value database, cache and message broker
with optional durability
Because it holds all data in memory and because of its design, Redis offers low-latency reads and writes,
making it particularly suitable for use cases that require a cache.
Redis is the most popular NoSQL database, and one of the most popular databases overall.

pip install redis
docker run --name langchain-redis -d -p 6379:6379 redis redis-server --save 60 1 --loglevel warning
docker stop langchain-redis
docker start langchain-redis

We need a redis url connection string to connect to the database support either a stand alone Redis server or a High-Availability setup with Replication and Redis Sentinels.
"from_url()" method Redis.from_url
Example: redis_url = "redis://:secret-pass@localhost:6379/0"

from langchain.cache import RedisCache
from langchain.globals import set_llm_cache
import redis

redis_client = redis.Redis.from_url(...)
set_llm_cache(RedisCache(redis_client))

Semantic caching allows users to retrieve cached prompts based on semantic similarity between the user input and previously cached results. Under the hood it blends Redis as both a cache and a vectorstore
from langchain.cache import RedisSemanticCache

from langchain.globals import set_llm_cache
import redis

# use any embedding provider...
from tests.integration_tests.vectorstores.fake_embeddings import FakeEmbeddings

redis_url = "redis://localhost:6379"

set_llm_cache(RedisSemanticCache(
    embedding=FakeEmbeddings(),
    redis_url=redis_url
))

The vectorstore wrapper turns Redis into a low-latency vector database for semantic search or LLM content retrieval.
from langchain_community.vectorstores import Redis


Links
----
https://python.langchain.com/docs/how_to/llm_caching/?utm_source=chatgpt.com
https://python.langchain.com/api_reference/core/globals/langchain_core.globals.set_llm_cache.html
https://python.langchain.com/docs/integrations/providers/redis/
VectorStore
https://python.langchain.com/docs/integrations/vectorstores/redis/
Retriever
Memory
Vector Store Retriever Memory - https://python.langchain.com/api_reference/langchain/memory/langchain.memory.vectorstore.VectorStoreRetrieverMemory.html
Chat Message History Memory   - https://python.langchain.com/docs/integrations/memory/redis_chat_message_history/
Redis can be used to persist LLM conversations.


https://python.langchain.com/api_reference/langchain/memory/langchain.memory.vectorstore.VectorStoreRetrieverMemory.html

https://python.langchain.com/docs/integrations/vectorstores/redis/
Redis is a popular open-source, in-memory data structure store that can be used as a database, cache, message broker, and queue.
It now includes vector similarity search capabilities, making it suitable for use as a vector store.
Redis provides additional capabilities like the Search and Query capability that allows users to create secondary index structures within Redis. This allows Redis to be a Vector Database, at the speed of a cache.
Redis is primarily an in-memory database, and its vector store capabilities (via Redis Stack / RediSearch) are designed for in-memory vector search.
Redis Stack (or Redis with the redisearch and redisjson modules
1.Store vectors in Redis
You can store dense vectors (e.g., embeddings from OpenAI, HuggingFace, or other ML models) as fields in a Redis hash or JSON document.
2.Index vectors with RediSearch
Using the FT.CREATE command, you can create an index with a VECTOR field, specifying:
3.Perform similarity search
With FT.SEARCH or FT.AGGREGATE, you can query for the top-K nearest neighbors given a query vector. Redis efficiently retrieves the most similar embeddings.



Chat Memory Storage - https://python.langchain.com/v0.1/docs/modules/memory/types/buffer/
from langchain.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain.schema import messages_from_dict, messages_to_dict
import sqlite3
import json

# --- Set up SQLite DB for persistent storage ---
conn = sqlite3.connect("chat_history.db")
c = conn.cursor()
c.execute('''
    CREATE TABLE IF NOT EXISTS messages (
        id INTEGER PRIMARY KEY,
        role TEXT,
        content TEXT
    )
''')
conn.commit()

# --- Helper functions to load and save messages ---
def save_message(role, content):
    c.execute("INSERT INTO messages (role, content) VALUES (?, ?)", (role, content))
    conn.commit()

def load_messages():
    c.execute("SELECT role, content FROM messages")
    rows = c.fetchall()
    return [{"type": row[0], "data": {"content": row[1]}} for row in rows]

# --- Load previous messages into memory ---
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

# If there’s previous history, load it
previous_messages = load_messages()
if previous_messages:
    memory.chat_memory.messages = messages_from_dict(previous_messages)

# --- Initialize LLM ---
llm = ChatOpenAI(temperature=0)

# --- Example interaction ---
user_input = "Hello, how are you?"

# Get response
response = llm.predict_messages([{"type": "human", "data": {"content": user_input}}])

# Store messages persistently
save_message("human", user_input)
for msg in response:
    save_message("ai", msg.content)

print(response[0].content)



from langchain.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain.schema import messages_from_dict, messages_to_dict
import sqlite3
import json

# --- Set up SQLite DB for persistent storage ---
conn = sqlite3.connect("chat_history.db")
c = conn.cursor()
c.execute('''
    CREATE TABLE IF NOT EXISTS messages (
        id INTEGER PRIMARY KEY,
        role TEXT,
        content TEXT
    )
''')
conn.commit()

# --- Helper functions to load and save messages ---
def save_message(role, content):
    c.execute("INSERT INTO messages (role, content) VALUES (?, ?)", (role, content))
    conn.commit()

def load_messages():
    c.execute("SELECT role, content FROM messages")
    rows = c.fetchall()
    return [{"type": row[0], "data": {"content": row[1]}} for row in rows]

# --- Load previous messages into memory ---
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

# If there’s previous history, load it
previous_messages = load_messages()
if previous_messages:
    memory.chat_memory.messages = messages_from_dict(previous_messages)

# --- Initialize LLM ---
llm = ChatOpenAI(temperature=0)

# --- Example interaction ---
user_input = "Hello, how are you?"

# Get response
response = llm.predict_messages([{"type": "human", "data": {"content": user_input}}])

# Store messages persistently
save_message("human", user_input)
for msg in response:
    save_message("ai", msg.content)

print(response[0].content)



Redis Chat Memory History 
------------------------
https://python.langchain.com/docs/how_to/chat_model_caching/?utm_source=chatgpt.com
https://python.langchain.com/docs/how_to/chat_model_caching/?utm_source=chatgpt.com
http://python.langchain.com/docs/integrations/memory/redis_chat_message_history/
https://python.langchain.com/docs/integrations/llm_caching/?utm_source=chatgpt.com
https://python.langchain.com/docs/integrations/llm_caching/?utm_source=chatgpt.com
GPTCache - https://github.com/zilliztech/GPTCache


Production Level Coding for Cache 
---------------------------------
import hashlib
import json
import redis
from langchain_openai import ChatOpenAI
from langchain_core.caches import BaseCache
from langchain_core.globals import set_llm_cache


class RedisCache(BaseCache):
    """Production-level cache for LLM queries using Redis."""

    def __init__(self, redis_url: str = "redis://localhost:6379/0", ttl: int = 3600):
        self.redis = redis.Redis.from_url(redis_url, decode_responses=True)
        self.ttl = ttl  # Time-to-live for cached entries

    def _make_key(self, prompt: str) -> str:
        """Create a stable hash key from the prompt text."""
        return f"llm_cache:{hashlib.sha256(prompt.encode('utf-8')).hexdigest()}"

    def lookup(self, prompt: str):
        """Retrieve cached response if it exists."""
        key = self._make_key(prompt)
        cached = self.redis.get(key)
        if cached:
            return json.loads(cached)
        return None

    def update(self, prompt: str, response):
        """Store response in cache with TTL."""
        key = self._make_key(prompt)
        self.redis.set(key, json.dumps(response), ex=self.ttl)


# Attach cache globally
set_llm_cache(RedisCache())

# Example usage
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
query = "What are the advantages of caching in LangChain?"
response = llm.invoke(query)
print("Response:", response)
Why this is Production-Ready ✅
Redis persistence – Survives process restarts.
Hashing of prompts – Efficient keys, avoids length issues.
JSON serialization – Works across LLM response objects.
TTL support – Prevents unbounded growth.
Thread-safe – Redis handles concurrency safely.
Tip: For multi-modal or structured outputs, ensure you serialize consistently (e.g., response.dict()
Tip: For distributed systems, Redis is strongly preferred.
Would you like me to also show you a middleware wrapper that logs cache hits/misses (so you can monitor cache performance in production)?

Perfect ✅ Let’s add a cache-aware middleware wrapper around the LLM that:
Logs cache hits and misses.
Collects metrics for monitoring.
Makes it easy to plug into observability tools (Prometheus, Datadog, etc.).


import logging
import time
from langchain_openai import ChatOpenAI
from langchain_core.language_models.chat import BaseChatModel
from langchain_core.caches import BaseCache

logger = logging.getLogger("LLMCache")
logging.basicConfig(level=logging.INFO)


class CachingLLMWrapper(BaseChatModel):
    """Middleware wrapper for LLM with caching + logging."""

    def __init__(self, llm: BaseChatModel, cache: BaseCache):
        super().__init__()
        self.llm = llm
        self.cache = cache

    def invoke(self, prompt: str, **kwargs):
        start_time = time.time()

        # 1. Try cache
        cached_response = self.cache.lookup(prompt)
        if cached_response:
            logger.info(f"[CACHE HIT] prompt_hash={hash(prompt)}")
            return cached_response

        # 2. Query LLM if not cached
        logger.info(f"[CACHE MISS] prompt_hash={hash(prompt)} -> querying LLM")
        response = self.llm.invoke(prompt, **kwargs)

        # 3. Store in cache
        self.cache.update(prompt, response)

        elapsed = (time.time() - start_time) * 1000
        logger.info(f"[LLM CALL] duration_ms={elapsed:.2f}")
        return response


# Use RedisCache from the earlier example
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
cached_llm = CachingLLMWrapper(llm, RedisCache(ttl=3600))

query = "Explain the difference between supervised and unsupervised learning."
response = cached_llm.invoke(query)

print("Response:", response)

Benefits
Transparency → You know when responses come from cache vs fresh calls.
Performance tracking → Monitor latency of uncached requests.
Easy observability → Send logs to ELK / Datadog / Prometheus.
Drop-in replacement → Works with any ChatOpenAI or other LangChain model.

Do you want me to extend this with Prometheus metrics (cache_hit_count, cache_miss_count, avg_latency) so it’s fully ready for observability dashboards?

Great choice 🚀 — adding Prometheus metrics on top of the cache wrapper gives you production-grade observability.
You’ll be able to monitor:
Cache hit rate vs miss rate
Latency of fresh LLM calls
Throughput of queries


import logging
import time
from prometheus_client import Counter, Histogram, start_http_server
from langchain_openai import ChatOpenAI
from langchain_core.language_models.chat import BaseChatModel
from langchain_core.caches import BaseCache


# ---------- Logging ----------
logger = logging.getLogger("LLMCache")
logging.basicConfig(level=logging.INFO)


# ---------- Prometheus Metrics ----------
CACHE_HITS = Counter("llm_cache_hits_total", "Total number of cache hits")
CACHE_MISSES = Counter("llm_cache_misses_total", "Total number of cache misses")
LLM_LATENCY = Histogram("llm_call_latency_seconds", "LLM call latency in seconds")
TOTAL_REQUESTS = Counter("llm_requests_total", "Total number of LLM requests")


class CachingLLMWrapper(BaseChatModel):
    """LLM wrapper with Redis/SQLite cache + logging + Prometheus metrics."""

    def __init__(self, llm: BaseChatModel, cache: BaseCache):
        super().__init__()
        self.llm = llm
        self.cache = cache

    def invoke(self, prompt: str, **kwargs):
        TOTAL_REQUESTS.inc()
        start_time = time.time()

        # Check cache
        cached_response = self.cache.lookup(prompt)
        if cached_response:
            CACHE_HITS.inc()
            logger.info(f"[CACHE HIT] prompt_hash={hash(prompt)}")
            return cached_response

        # Cache miss → call LLM
        CACHE_MISSES.inc()
        logger.info(f"[CACHE MISS] prompt_hash={hash(prompt)} -> querying LLM")
        with LLM_LATENCY.time():
            response = self.llm.invoke(prompt, **kwargs)

        # Store result in cache
        self.cache.update(prompt, response)

        elapsed = (time.time() - start_time) * 1000
        logger.info(f"[LLM CALL] duration_ms={elapsed:.2f}")
        return response


from prometheus_client import start_http_server
from your_cache_module import RedisCache  # Use RedisCache we built earlier

if __name__ == "__main__":
    # Start Prometheus server (scraped at http://localhost:8000/metrics)
    start_http_server(8000)

    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    cached_llm = CachingLLMWrapper(llm, RedisCache(ttl=3600))

    query = "What is the difference between AI, ML, and DL?"
    response = cached_llm.invoke(query)

    print("Response:", response)


# HELP llm_cache_hits_total Total number of cache hits
# TYPE llm_cache_hits_total counter
llm_cache_hits_total 5.0

# HELP llm_cache_misses_total Total number of cache misses
# TYPE llm_cache_misses_total counter
llm_cache_misses_total 2.0

# HELP llm_call_latency_seconds LLM call latency in seconds
# TYPE llm_call_latency_seconds histogram
llm_call_latency_seconds_bucket{le="0.5"} 1.0
llm_call_latency_seconds_bucket{le="1.0"} 2.0
...
llm_call_latency_seconds_count 3.0
llm_call_latency_seconds_sum 2.134

# HELP llm_requests_total Total number of LLM requests
# TYPE llm_requests_total counter
llm_requests_total 7.0


# HELP llm_cache_hits_total Total number of cache hits
# TYPE llm_cache_hits_total counter
llm_cache_hits_total 5.0

# HELP llm_cache_misses_total Total number of cache misses
# TYPE llm_cache_misses_total counter
llm_cache_misses_total 2.0

# HELP llm_call_latency_seconds LLM call latency in seconds
# TYPE llm_call_latency_seconds histogram
llm_call_latency_seconds_bucket{le="0.5"} 1.0
llm_call_latency_seconds_bucket{le="1.0"} 2.0
...
llm_call_latency_seconds_count 3.0
llm_call_latency_seconds_sum 2.134

# HELP llm_requests_total Total number of LLM requests
# TYPE llm_requests_total counter
llm_requests_total 7.0


Why This is Production-Ready ✅
Logging + Metrics → Human-readable + machine-readable monitoring.
Prometheus-compatible → Plug into Grafana dashboards easily.
Extensible → Works with Redis, SQLite, or custom cache.
Low overhead → Prometheus client is efficient.

Would you like me to also show you a Grafana dashboard JSON template you can import directly to visualize cache hit ratio, request volume, and LLM latency?


Semantic Cache Production Level Coding
--------------------------------------
exact (prompt) cache + semantic cache fallback
embeddings (pluggable; example uses OpenAIEmbeddings)
vector store backends (Redis / FAISS via LangChain)
configurable similarity threshold, top-k, TTL for exact cache entries
deduplication & metadata storage (prompt, response, timestamp, provenance)
concurrency-safe upserts (simple lock strategy)
Prometheus metrics (hits / misses / upserts / similarity distributions)
logging and simple unit-test examples
notes on deployment and tuning


"""
semantic_cache.py
Production-ready semantic cache wrapper for LangChain LLM usage.

Features:
- Exact prompt cache (Redis or in-memory fallback)
- Semantic cache using LangChain VectorStore (Redis or FAISS)
- Configurable thresholds for reusing a cached response
- Pluggable embeddings
- Prometheus metrics + logging
- Simple concurrency lock using Redis or in-process lock
"""

import time
import hashlib
import json
import logging
from typing import Optional, Dict, Any, List, Tuple

# LangChain imports (adjust to your installed version)
from langchain.embeddings.base import Embeddings
from langchain.schema import BaseLanguageModel
from langchain.vectorstores import FAISS, Redis as LangchainRedis
from langchain.embeddings import OpenAIEmbeddings  # example
from langchain.chat_models import ChatOpenAI  # example LLM, replace as desired

# Persistence for exact prompt cache
try:
    import redis
    _HAS_REDIS = True
except Exception:
    _HAS_REDIS = False

# Prometheus
try:
    from prometheus_client import Counter, Histogram, start_http_server
    _HAS_PROM = True
except Exception:
    _HAS_PROM = False

# FAISS
try:
    import faiss
    _HAS_FAISS = True
except Exception:
    _HAS_FAISS = False

# Logging
logger = logging.getLogger("semantic_cache")
logging.basicConfig(level=logging.INFO)


# --------------------------
# Prometheus metrics
# --------------------------
if _HAS_PROM:
    PROM_CACHE_HITS = Counter("semantic_cache_hits_total", "Semantic cache hits")
    PROM_CACHE_MISSES = Counter("semantic_cache_misses_total", "Semantic cache misses")
    PROM_EXACT_HITS = Counter("semantic_exact_hits_total", "Exact prompt cache hits")
    PROM_UPSERTS = Counter("semantic_cache_upserts_total", "Number of upserts to semantic cache")
    PROM_SIM_SCORE = Histogram("semantic_similarity_score", "Similarity scores returned from search")
else:
    class _NoOp:
        def inc(self, *a, **k): pass
        def observe(self, *a, **k): pass
    PROM_CACHE_HITS = PROM_CACHE_MISSES = PROM_EXACT_HITS = PROM_UPSERTS = PROM_SIM_SCORE = _NoOp()


# --------------------------
# Utilities
# --------------------------
def _prompt_hash(prompt: str) -> str:
    """Stable compact key for exact-match cache."""
    return hashlib.sha256(prompt.encode("utf-8")).hexdigest()


def _now_ts() -> float:
    return time.time()


# --------------------------
# Exact Prompt Cache (Redis-backed with in-memory fallback)
# --------------------------
class ExactPromptCache:
    def __init__(self, redis_url: Optional[str] = None, ttl_seconds: int = 3600):
        self.ttl = ttl_seconds
        self._in_memory = {}
        self._use_redis = False
        if redis_url and _HAS_REDIS:
            try:
                self._redis = redis.Redis.from_url(redis_url, decode_responses=True)
                # ping to verify connection
                self._redis.ping()
                self._use_redis = True
                logger.info("ExactPromptCache: using Redis at %s", redis_url)
            except Exception as e:
                logger.warning("ExactPromptCache: Redis unavailable, falling back to in-memory cache: %s", e)
                self._use_redis = False
        else:
            if redis_url:
                logger.warning("ExactPromptCache: redis package not installed, falling back to in-memory cache.")
            else:
                logger.info("ExactPromptCache: no redis_url provided, using in-memory cache.")

    def get(self, prompt: str) -> Optional[Dict[str, Any]]:
        key = f"exact:{_prompt_hash(prompt)}"
        if self._use_redis:
            raw = self._redis.get(key)
            if raw:
                try:
                    data = json.loads(raw)
                    PROM_EXACT_HITS.inc()
                    return data
                except Exception:
                    logger.exception("ExactPromptCache: failed to decode JSON from redis")
                    return None
            return None
        else:
            entry = self._in_memory.get(key)
            if not entry:
                return None
            ts, payload = entry
            if self.ttl and (time.time() - ts) > self.ttl:
                del self._in_memory[key]
                return None
            PROM_EXACT_HITS.inc()
            return payload

    def set(self, prompt: str, response: Dict[str, Any]) -> None:
        key = f"exact:{_prompt_hash(prompt)}"
        payload = json.dumps(response)
        if self._use_redis:
            try:
                self._redis.set(key, payload, ex=self.ttl)
            except Exception:
                logger.exception("ExactPromptCache: failed to set key in redis")
        else:
            self._in_memory[key] = (time.time(), response)


# --------------------------
# Semantic Cache Wrapper
# --------------------------
class SemanticCache:
    """
    SemanticCache wraps an LLM and provides:
    - exact prompt caching
    - semantic retrieval from vector store
    - fallback to LLM call and upsert

    Config:
      - vector_store_type: "redis" or "faiss"
      - similarity_threshold: float in [0,1] (bigger = stricter)
      - top_k: number of nearest neighbors to consider
      - dedup_min_similarity: if existing vector is very similar, consider deduping
    """
    def __init__(
        self,
        llm: BaseLanguageModel,
        embeddings: Embeddings,
        vector_store_type: str = "faiss",
        vector_store_config: Optional[Dict[str, Any]] = None,
        exact_cache: Optional[ExactPromptCache] = None,
        similarity_threshold: float = 0.8,
        top_k: int = 3,
        dedup_min_similarity: float = 0.995,
    ):
        self.llm = llm
        self.embeddings = embeddings
        self.similarity_threshold = similarity_threshold
        self.top_k = top_k
        self.dedup_min_similarity = dedup_min_similarity
        self.exact_cache = exact_cache or ExactPromptCache()
        self.vector_store_type = vector_store_type
        self.vector_store_config = vector_store_config or {}
        self._init_vector_store()

        # lightweight in-process lock for upserts; in distributed env use a proper distributed lock
        self._write_lock = {}

    def _init_vector_store(self):
        if self.vector_store_type == "redis":
            # expects vector_store_config: {"redis_url": "...", "index_name": "llm_semantic_cache"}
            if not _HAS_REDIS:
                raise RuntimeError("Redis not available at runtime. Install redis-py or choose faiss.")
            cfg = self.vector_store_config
            redis_url = cfg.get("redis_url")
            index_name = cfg.get("index_name", "llm_semantic_cache")
            self.vs = LangchainRedis.from_existing_index(
                embedding=self.embeddings,
                index_name=index_name,
                redis_url=redis_url
            )
            logger.info("SemanticCache: using LangChain Redis vector store index=%s", index_name)
        elif self.vector_store_type == "faiss":
            # FAISS store will be initialized empty; provide path in vector_store_config['faiss_path'] to persist
            self._faiss_path = self.vector_store_config.get("faiss_path")
            self.vs = None
            # Lazy-init: create empty FAISS store on first upsert
            logger.info("SemanticCache: configured to use FAISS; persistence path=%s", self._faiss_path)
        else:
            raise ValueError("vector_store_type must be 'redis' or 'faiss'")

    # -----------------------
    # Internal helpers
    # -----------------------
    def _embed(self, text: str) -> List[float]:
        return self.embeddings.embed_query(text)

    def _nearest(self, query_embedding: List[float], k: Optional[int] = None) -> List[Dict[str, Any]]:
        k = k or self.top_k
        if self.vector_store_type == "redis":
            results = self.vs.similarity_search_by_vector(query_embedding, k=k, return_documents=True)
            # LangChain Redis returns Document objects; we convert to dicts with score if available
            out = []
            for doc in results:
                # doc.metadata expected to contain stored metadata and maybe 'score' depending on implementation
                meta = doc.metadata or {}
                text = doc.page_content
                score = meta.get("score") or meta.get("_score") or None
                out.append({"text": text, "metadata": meta, "score": score})
            return out
        elif self.vector_store_type == "faiss":
            if self.vs is None:
                return []
            docs_and_scores = self.vs.similarity_search_by_vector(query_embedding, k=k, return_documents=True)
            out = []
            for doc in docs_and_scores:
                meta = doc.metadata or {}
                out.append({"text": doc.page_content, "metadata": meta, "score": None})
            return out
        return []

    def _init_faiss_store(self):
        # create empty FAISS store if not present
        if self.vs is None:
            # Use LangChain FAISS wrapper; requires texts and embeddings to build index
            # We'll create an empty index by upserting a tiny dummy vector then deleting it.
            from langchain.vectorstores import FAISS as LCFAISS
            self.vs = LCFAISS.from_texts(
                texts=["__dummy__"],
                embedding=self.embeddings,
                metadatas=[{"created_at": _now_ts()}]
            )
            # If persistence path provided, save
            if self._faiss_path:
                try:
                    self.vs.save_local(self._faiss_path)
                    logger.info("SemanticCache: saved FAISS index to %s", self._faiss_path)
                except Exception:
                    logger.exception("SemanticCache: failed to save FAISS index")

    # -----------------------
    # Public API
    # -----------------------
    def query(self, prompt: str, use_exact_first: bool = True, **llm_kwargs) -> Dict[str, Any]:
        """
        Returns a dict with keys:
          - "response": LLM response text
          - "from_cache": "exact" | "semantic" | "llm"
          - "matched_metadata": metadata of matched semantic doc (if any)
          - "similarity": score (if available)
        """
        TOTAL_START = time.time()

        # 1) Try exact prompt cache
        if use_exact_first:
            exact = self.exact_cache.get(prompt)
            if exact:
                PROM_CACHE_HITS.inc()
                logger.info("semantic_cache: exact hit")
                return {"response": exact["response"], "from_cache": "exact", "matched_metadata": exact.get("metadata"), "similarity": 1.0}

        # 2) compute embedding and search vector store
        emb = self._embed(prompt)
        neighbors = self._nearest(emb, k=self.top_k)
        # Evaluate neighbors to see if any meets similarity threshold. Different vector stores return different score semantics.
        best = None
        best_score = -1.0
        for n in neighbors:
            score = n.get("score")
            # if score is None, we cannot decide based on it; treat None as unknown — we can still look at metadata
            if score is not None:
                # assume higher score == more similar (LangChain/Redis returns cosine similarity if configured)
                if score > best_score:
                    best = n
                    best_score = score

        # If best score available and exceeds threshold, return cached response
        if best and best_score >= self.similarity_threshold:
            PROM_CACHE_HITS.inc()
            if _HAS_PROM:
                PROM_SIM_SCORE.observe(best_score)
            logger.info("semantic_cache: semantic hit (score=%.4f)", best_score)
            # expected metadata structure: { "prompt": "...", "response": "...", "created_at": ..., "provenance": ... }
            meta = best.get("metadata", {})
            resp = meta.get("response") or best.get("text")
            return {"response": resp, "from_cache": "semantic", "matched_metadata": meta, "similarity": best_score}

        # 3) Cache miss -> call LLM and upsert
        PROM_CACHE_MISSES.inc()
        logger.info("semantic_cache: miss -> calling LLM")
        llm_start = time.time()
        # llm call - may raise; callers should handle exceptions
        llm_result = self.llm.generate([prompt]) if hasattr(self.llm, "generate") else self.llm(prompt, **llm_kwargs)
        # Normalize llm_result to a string response
        if isinstance(llm_result, dict):
            response_text = llm_result.get("text") or llm_result.get("response")
        else:
            # Support both LangChain chat model call signatures or simple callables
            try:
                # LangChain Chat model: llm.generate returns ChatGeneration objects; this is an example normalization
                if hasattr(llm_result, "generations"):
                    # Take first generation's text
                    response_text = llm_result.generations[0][0].text
                else:
                    response_text = str(llm_result)
            except Exception:
                response_text = str(llm_result)

        llm_latency = time.time() - llm_start
        logger.info("semantic_cache: llm call latency=%.3fs", llm_latency)

        # 4) upsert exact cache & semantic cache
        metadata = {
            "prompt": prompt,
            "response": response_text,
            "created_at": _now_ts(),
            "llm_latency_s": llm_latency
        }
        # exact cache
        try:
            self.exact_cache.set(prompt, {"response": response_text, "metadata": metadata})
        except Exception:
            logger.exception("semantic_cache: failed to set exact cache")

        # semantic upsert (embedding + metadata)
        try:
            self.upsert(prompt, response_text, metadata, embedding=emb)
        except Exception:
            logger.exception("semantic_cache: upsert failed")

        return {"response": response_text, "from_cache": "llm", "matched_metadata": metadata, "similarity": None}

    def upsert(self, prompt: str, response_text: str, metadata: Optional[Dict[str, Any]] = None, embedding: Optional[List[float]] = None):
        """Upsert a prompt/response pair into the vector store.
        - If vector_store_type == faiss and not initialized, initialize it.
        - Deduplicate if a very similar vector already exists (within dedup_min_similarity).
        """
        PROM_UPSERTS.inc()
        metadata = metadata or {}
        metadata = dict(metadata)
        metadata.update({"prompt_hash": _prompt_hash(prompt)})
        if embedding is None:
            embedding = self._embed(prompt)

        if self.vector_store_type == "faiss":
            # lazy init
            self._init_faiss_store()
            # LangChain FAISS wrapper can add_texts
            texts = [response_text]
            metadatas = [metadata]
            self.vs.add_texts(texts=texts, metadatas=metadatas)
            if self._faiss_path:
                try:
                    self.vs.save_local(self._faiss_path)
                except Exception:
                    logger.exception("SemanticCache: failed to save faiss index")
        elif self.vector_store_type == "redis":
            # LangChain Redis vectorstore uses add_texts/upsert
            self.vs.add_texts(texts=[response_text], metadatas=[metadata])
        else:
            raise RuntimeError("unsupported vector_store_type")

    # Optional admin helper
    def delete_all(self):
        """Dangerous: wipes whole semantic cache (vector store) and exact cache"""
        # Exact cache wipe (only in-memory or redis keys with prefix)
        if getattr(self.exact_cache, "_use_redis", False):
            r = self.exact_cache._redis
            keys = r.keys("exact:*")
            if keys:
                r.delete(*keys)
        else:
            self.exact_cache._in_memory.clear()

        # Vector store wipe depends on backend
        if self.vector_store_type == "redis" and self.vs:
            try:
                # LangChain Redis wrapper exposes client for index removal; easiest: flush index via Redis commands,
                # but be careful in production: restrict index namespace to avoid accidental deletion.
                r = self.vs.client
                # The implementation details vary by LangChain version; this is just an example placeholder.
                logger.warning("SemanticCache.delete_all: please manually delete Redis search index using your Redis admin tools.")
            except Exception:
                logger.exception("SemanticCache.delete_all: error while trying to clear redis index")
        elif self.vector_store_type == "faiss" and self.vs:
            # drop FAISS index by setting to None and removing persisted file
            self.vs = None
            if getattr(self, "_faiss_path", None):
                import shutil, os
                try:
                    shutil.rmtree(self._faiss_path)
                except Exception:
                    logger.exception("SemanticCache.delete_all: failed to remove faiss path")


# example_usage.py
from semantic_cache import SemanticCache, ExactPromptCache
from langchain.chat_models import ChatOpenAI
from langchain.embeddings import OpenAIEmbeddings

# Start Prometheus exporter in a real process (optional)
from prometheus_client import start_http_server
start_http_server(9000)  # /metrics

llm = ChatOpenAI(openai_api_key="...")  # configure via env or secrets manager
emb = OpenAIEmbeddings(openai_api_key="...")

# Use FAISS for the example; for production, prefer RedisVector or managed vector DB (Pinecone/Milvus/Redis)
cache = SemanticCache(
    llm=llm,
    embeddings=emb,
    vector_store_type="faiss",
    vector_store_config={"faiss_path": "./faiss_index"},
    similarity_threshold=0.78,
    top_k=4,
    dedup_min_similarity=0.995,
    exact_cache=ExactPromptCache(redis_url="redis://localhost:6379/1", ttl_seconds=3600)
)

# Query - first call will invoke LLM and upsert
out = cache.query("Explain what overfitting is in simple terms.")
print(out["from_cache"], out["response"][:200])

# Query again with a paraphrase - test semantic hit
out2 = cache.query("Explain overfitting like I'm 10 years old.")
print(out2["from_cache"], out2["matched_metadata"])



# test_semantic_cache.py
import tempfile
from semantic_cache import SemanticCache, ExactPromptCache
from langchain.embeddings import FakeEmbeddings  # if you have a fake embeddings for tests
from langchain.chat_models import ChatOpenAI

def test_exact_cache_roundtrip():
    ec = ExactPromptCache(redis_url=None, ttl_seconds=2)
    prompt = "hello"
    ec.set(prompt, {"response": "world", "metadata": {}})
    assert ec.get(prompt)["response"] == "world"
    # TTL expiry
    import time; time.sleep(2.1)
    assert ec.get(prompt) is None

def test_semantic_cache_with_fake_embeddings(monkeypatch):
    from langchain.embeddings import FakeEmbeddings
    # Fake LLM that returns prompt back
    class FakeLLM:
        def __call__(self, prompt, **k):
            return {"text": f"FAKE_RESPONSE: {prompt}"}
    llm = FakeLLM()
    emb = FakeEmbeddings()
    cache = SemanticCache(llm=llm, embeddings=emb, vector_store_type="faiss", vector_store_config={"faiss_path": None})
    out = cache.query("test prompt")
    assert out["from_cache"] == "llm"
    # calling again should be semantic hit if fake embeddings are deterministic and vectorstore stored
    out2 = cache.query("test prompt")
    assert out2["from_cache"] in ("exact", "semantic")




Production notes / deployment checklist

Vector DB selection

Small teams or local: FAISS with persistent storage (fast, cheap) — but you must manage replication and backups.

Production/distributed: Redis Vector (RedisStack), Milvus, or managed vector DB (Pinecone/Weaviate) — choose based on latency, scale, and cloud provider.

Ensure vector DB supports cosine similarity (or convert to inner-product) and stores metadata.
Embeddings
Use a single embedding model consistently. Changing embedding model invalidates similarity semantics.
Normalize embeddings if your vector DB expects normalized vectors for cosine similarity.
Threshold tuning
similarity_threshold is workload-specific. Start 0.75–0.85 and tune using A/B tests. Use human labeling for correctness
Monitor false positives (wrong cached response) vs. cost saved.
Concurrency & consistency
For upserts in distributed deployments, use distributed locks (Redlock on Redis or DB row locks) to prevent race conditions during near-simultaneous upserts.
Consider eventual consistency: allow short-term duplicates; dedup on a background job.
Eviction & lifecycle
Exact cache TTL prevents stale exact matches.
For semantic entries, consider periodic reindexing, TTL in metadata, or LRU policies based on last_used counters (stored in metadata).
Auditing & provenance
Store created_at, llm_model, model_version, and cost_estimate in metadata to audit cached content.
Allow invalidation by prompt hash or metadata filters (e.g., "invalidate all cached answers produced by model v1").
Monitoring
Expose Prometheus metrics: hits, misses, upserts, similarity histogram.
Track the distribution of similarity for accepted hits to detect drift.
Security & privacy
If prompts/responses contain PII, avoid persisting them to disk without encryption / policy checks.
Use encryption at rest for persisted vector DBs and metadata.
Provide a “PII scrubber” pipeline stage before storing.
Testing
Unit tests with fake embeddings/LLM.
Integration tests against a staging vector DB.
Load tests to validate tail latencies.


Quick tuning guide
If you see incorrect cached responses: increase similarity_threshold.
If you see too many fresh LLM calls and correct cached responses exist: lower similarity_threshold or increase top_k.
If vector store returns score=None: ensure your vector store / LangChain wrapper is configured to return similarity scores (Redis/FAISS options).
Monitor PROM_SIM_SCORE histogram to select an informed threshold.

If you want, I can:
provide a concrete RedisVector example (index schema + RedisSearch commands) for true production RedisStack usage,
generate a Dockerfile + docker-compose that runs Prometheus + Redis + python app for local testing,
or produce a Grafana dashboard JSON for the Prometheus metrics above.
Which one would you like next?







All keys/values are in memory (fast read/write).
Redis can be configured with a max memory limit.
When memory is full, eviction policies apply (e.g., volatile-lru, allkeys-lru, noeviction).
Persistence (RDB/AOF) just saves snapshots or logs on disk, but runtime operations are always in memory.

Redis supports two persistence mechanisms:
1. RDB (Redis Database Backup snapshots)
Redis takes periodic snapshots of the dataset and writes them to disk (dump.rdb file).
Configurable in redis.conf with save directives, e.g.:
save 900 1    # Save every 900s (15min) if at least 1 key changed
save 300 10   # Save every 300s (5min) if at least 10 keys changed
save 60 10000 # Save every 60s if at least 10,000 keys changed
Pro: lightweight, good for backups.
Con: if Redis crashes between snapshots, you can lose recent writes.


AOF (Append-Only File)
Every write command is logged to an append-only file.
On restart, Redis replays the log to rebuild the dataset.
Configurable in redis.conf:
appendonly yes
appendfsync everysec   # fsync once per second (balance safety vs performance)
Pro: More durable (minimal data loss, depending on fsync mode).
Con: File can grow large (though Redis rewrites it in the background).

Hybrid Approach (RDB + AOF) ✅
Many production setups enable both.
RDB gives faster recovery on large datasets, AOF gives stronger durability.


How This Affects Your Cache
By default, your RedisCache will survive your Python app restarts (since Redis is external).
To make sure it survives Redis restarts, you need persistence enabled (RDB, AOF, or both).
Without persistence, Redis is purely in-memory → restart wipes everything (still fine if your cache is “ephemeral,” like LLM caching, but not if you want long-term retention).

Your cache survives app restarts because Redis runs as a separate process.
To survive Redis restarts, you need to enable RDB snapshots and/or AOF logging in your Redis config.


Production-Ready Persistence Config
Here’s a sane setup if you’re using Redis mainly as a cache for your app (LangChain, LLM responses, etc.):

####################################
# Redis Persistence Configuration
####################################

# --- RDB snapshots (lightweight backups) ---
save 900 1       # Save if at least 1 key changed in 15 minutes
save 300 10      # Save if at least 10 keys changed in 5 minutes
save 60 10000    # Save if at least 10k keys changed in 1 minute
stop-writes-on-bgsave-error yes
rdbcompression yes
dbfilename dump.rdb

# --- Append-Only File (AOF, durable logging) ---
appendonly yes
appendfilename "appendonly.aof"
appendfsync everysec   # fsync once per second (good balance of safety/perf)
no-appendfsync-on-rewrite yes
Choosing Strategy
If caching is ephemeral (like LLM cache, CDN-style data):
You may not need persistence at all (just leave Redis in-memory only).
→ Faster, less disk I/O, but cache will reset on Redis restart.
If caching is expensive to rebuild (e.g., long LLM queries, expensive APIs):
Enable at least AOF for durability.
👉 Many production teams use both RDB + AOF for safety.

Running Redis in Docker/K8s?
If you’re running Redis in Docker/Kubernetes, persistence depends on volumes:
If no volume is mounted → data disappears when container restarts.
Mount a persistent volume to /data in Redis container:
volumes:
  - redis-data:/data
That way, your dump.rdb and appendonly.aof survive container restarts.

Yes — you configure these settings once globally in Redis (redis.conf or Docker/K8s env vars). Your Python RedisCache client just connects; it doesn’t care about persistence details.

---------------------------------------------------
Llamaindex Caching 
------------------
https://markaicode.com/llamaindex-caching-strategies-document-retrieval/
https://github.com/run-llama/llama_index/issues/14995
https://stackoverflow.com/questions/79210759/does-llama-index-remember-the-last-query-why-is-response-time-faster-for-repe - Default Caching Behaviour 
https://blog.milvus.io/ai-quick-reference/how-can-i-optimize-the-performance-of-llamaindex-queries





