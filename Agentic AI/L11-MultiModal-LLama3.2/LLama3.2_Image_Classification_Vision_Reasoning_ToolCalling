1B model parameters to its 405B foundation model 
Llama 3.2 model - 
1.From image classification, vision reasoning to tool use
2.Learn the details of Llama 3.2 prompting, tokenization, built-in and custom tool calling
3.To learn all about the latest additions to the Llama models 3.1 and 3.2, from custom tool calling to multimodality and the new Llama stack


New vision capabilities that Llama 3.2
You’ll learn how to leverage this along with tool-calling, and **Llama Stack, which is an open-source orchestration layer for building on top of the Llama family of models.


Features 
1.Understand how to do multimodal prompting with Llama and work on advanced image reasoning use cases such as understanding errors on a car dashboard, 
2.Adding up the total of three restaurant receipts, grading written math homework, and many more.
3.Learn different roles—system, user, assistant, ipython—in the Llama 3.1 and 3.2 family and the prompt format that identifies those roles.
4.Understand how Llama uses the tiktoken tokenizer, and how it has expanded to a **128k vocabulary size that improves encoding efficiency and enables support for seven non-English languages.
5.Learn how to prompt Llama to call both built-in and custom tools with examples for web search and solving math equations.
6.Learn about ‘Llama Stack API’, which is a standardized interface for canonical toolchain components like fine-tuning or synthetic data generation to customize Llama models and build agentic applications.

Vision Architecture 
    Compositional Approach 
       1. Parallelize the development of vision and language model 

       Pre-Trained Image Encoder 
       Pre-Trained Text Model - AutoRegressive Decoder - Cross Attention Layer to get the input of image while inference 
       Speech-Encoder - Speech-to-Text , and giving the input tokens to the text model 
