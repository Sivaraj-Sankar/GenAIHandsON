
User Request â†’ State Event â†’ Node Processing â†’ State Update Event â†’ Next Node â†’ ... â†’ Final State
Design Pattern 
Strategy Pattern (for interchangeable agents)
Chain of Responsibility (for workflow stages)
Template Method (for base classes)
Builder Pattern (for workflow construction)
Observer Pattern (for state management)
Event-Driven Architecture (for asynchronous workflow execution)
--------------------------------------
1. Structural Design Pattern - Facade Pattern is a structural design pattern
   simplified interface to a complex subsystem
   Reduces coupling between client code and complex libraries/subsystems.
   Clients donâ€™t need to know about the multiple classes, their interactions, or dependencies
2. Builder + Template Method 
   Builder is a creational pattern.
   An Object is complex - Separate how an object is built from what the final object looks like.
   Template Method is a behavioral pattern.
   You have a fixed algorithm skeleton
   Some steps must be customized by subclasses
   Define the algorithm in a base class, let subclasses override specific steps.
Pattern	Responsibility
Template Method	Controls the workflow / order of steps
Builder	Controls what is produced at each step
3. Template Method + Strategy - 
Strategy defines:
A family of interchangeable behaviors
Behavior can be selected at runtime
do_task(strategy):
    strategy.execute()
Behavior can change, structure stays the same
Template Method controls the FLOW
Strategy controls the BEHAVIOR of a step
ğŸ“Œ Template Method = WHEN & ORDER
ğŸ“Œ Strategy = HOW
Because:
You want a stable workflow
But pluggable logic inside steps
This avoids:
Huge inheritance trees
if/else explosions
Hardcoded behaviors


4. Chain of Responsibility - behavioral design pattern - its main idea is to pass a request along a chain of handlers until one of the handlers processes it
  This decouples the sender of a request from its receivers.
  Think of it as a help desk system: you ask a question, it goes from the first level of support to the next level until someone can answer.
  Handlers: Objects that can handle a request.
  Next in Chain: Each handler knows the next handler to forward the request if it cannot process it.
  Decoupling: The client only sends the request, it doesnâ€™t care who handles it.
  Flexible: You can add or remove handlers without changing the client code.
Client
   |
   v
Handler1 ---> Handler2 ---> Handler3 ---> None
In AI/Agent systems, the CoR pattern is often used for workflow of agents: a query is passed through multiple agents 
(like extraction â†’ analysis â†’ summarization) until itâ€™s fully processed.

AI example mapping
Pattern	In your system
Template Method	Agent execution pipeline
Builder	Response / reasoning / entity builders
Facade	Wraps multiple agents
Chain of Responsibility	Agent routing
Builder + Template	Structured output generation

Topic 1
-------------------------------------
State transitions = events
Async processing with astream()
Nodes communicate via immutable state objects
Observable event streams for real-time updates

Topic 2
-------------------------------------------
Execution flow with decorators/lifecycle hooks
State transformation patterns
Event flow and async processing model

Topic 3 
------------------------------------------
# BaseNode with template method pattern
class QueryParsingNode(BaseNode[SubWorkflowState]):
    def process(self, state: SubWorkflowState, config: Dict) -> SubWorkflowState:
        agent = AgentRegistry.get_agent(AgentType.QUERY_PARSER.id)
        response = agent.perform_action(query=state.data["query"])
        state.data["query_components"] = response
        return state

# Workflow composition with builder pattern
query_workflow = (
    WorkflowBuilder(SubWorkflowState, "QueryWorkflow")
    .create_linear_workflow([
        ("query_parsing", QueryParsingNode()),
        ("entity_extraction", EntityExtractionNode())
    ])
    .compile()
)

# Event-driven execution
async for event_type, event_data in workflow.process_workflow(workflow_info):
    # Each state change is an event
    print(f"Event: {event_type}")
--------------------------------------------

base_react_agent - Template + Strategy - Wrapper for LangChain ReAct agents
query_workflow - Chain of Responsibility - Parse and understand user queries
base_node/query_parsing_node - Template Method - Template for workflow processing nodes
abstract_workflow/core_rag_workflow - Full chatbot workflow lifecycle management - Template Method + Facade - AbstractChatbotWorkflow (rag_workflow/core usage)
base_workflow - Builder + Template Method Foundation for building LangGraph state machines - base_workflow.py (Used by AbstractWorkflow)

--------------------------------------------
State Transitions as Events: Each node execution produces state changes that trigger subsequent nodes
Asynchronous Processing: Uses async/await for non-blocking operations
Message Passing: State objects act as events flowing through the workflow
Decoupled Components: Nodes communicate only through state
Observable Streams: Built-in event streaming via LangGraph


--------------------------------------------
Hierarchical Multi-Agent Architecture
  â”‚
  â”œâ”€â†’ Event-Driven (overall pattern)
  â”œâ”€â†’ Template Method (base classes)
  â”œâ”€â†’ Strategy (interchangeable agents)
  â”œâ”€â†’ Chain of Responsibility (workflow stages)
  â”œâ”€â†’ Builder (workflow construction)
  â”œâ”€â†’ Facade (AbstractChatbotWorkflow)
  â””â”€â†’ Composite (workflows contain sub-workflows)

What you'll see:

Workflow initialization
Node-by-node execution with logging
Agent processing
State transformations
Event streaming
Final results
Output includes:

Workflow construction logs
Node execution traces
Agent reasoning output
State updates
Event notifications
Architecture pattern summary

Questions
----------------------
BaseReactAgent: Consistent agent interface
BaseNode: Common node behavior (logging, timing, tracing)
BaseWorkflow: State machine construction
AbstractChatbotWorkflow: Full application lifecycle
WorkflowBuilder: Simplified workflow creation

Easy addition of new workflows (just inherit and compose)
Reusable components across multiple workflows
Type-safe state management
Built-in observability
Testable components in isolation

Is this over-engineered?
How do I add a new workflow?
Define your state class
Create nodes (inherit from BaseNode)
Build workflow using WorkflowBuilder
Optionally extend AbstractChatbotWorkflow for full features

state_persistence_service
conversation_history_service
Repository pattern

**
------------
Initial state is an event
Each node processes and emits updated state (new event)
State changes trigger next nodes
Events can be streamed to clients in real-time
All asynchronous with async/await

Abstraction	Real Implementation
BaseReactAgent	src/com/enverus/agents/base_react_agent.py
BaseNode	src/com/enverus/workflows/base/base_node.py
BaseWorkflow	src/com/enverus/workflows/base/base_workflow.py
AbstractChatbotWorkflow	src/com/enverus/workflows/abstract_workflow.py
Query Subworkflow	src/com/enverus/workflows/rag_workflow/query/
CoreRagWorkflow	src/com/enverus/workflows/rag_workflow/core/core_rag_workflow.py

-------------------------
User Request
    â†“
AbstractChatbotWorkflow (Initialize + Process)
    â†“
CoreRagWorkflow (Orchestrate)
    â†“
Sub-Workflows (Query, Retrieval, Generation, etc.)
    â†“
Nodes (BaseNode implementations)
    â†“
Agents (BaseReactAgent implementations)
    â†“
Response (Event stream to user)

----------------------------
Each layer is:

âœ… Independently testable
âœ… Loosely coupled
âœ… Type-safe
âœ… Observable
âœ… Reusable


--------------------------------------
1. Hierarchical Multi-Agent Pattern
Core Workflow orchestrates multiple Sub-Workflows
Each sub-workflow contains specialized Nodes (agents)
Nodes inherit from common base classes providing shared behavior
2. Template Method Pattern
Base classes define the workflow structure
Subclasses implement specific behavior
Examples: BaseNode, BaseWorkflow, AbstractChatbotWorkflow
3. Strategy Pattern
Interchangeable agents registered in AgentRegistry
Different agents for different tasks (query parsing, retrieval, response generation)
4. Chain of Responsibility
Request flows through multiple handler nodes
Each node processes and passes to the next
5. Builder Pattern
WorkflowBuilder provides fluent API for constructing workflows
Simplifies complex workflow creation


-----------------------
Provides a foundation for creating LangChain ReAct agents with tools.
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      BaseReactAgent                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  - llm: LanguageModel              â”‚
â”‚  - tools: List[Tool]               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  + validate_attributes()            â”‚
â”‚  + run(query): response            â”‚
â”‚  + arun(query): response           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â”‚ uses
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   create_react_agent()              â”‚
â”‚   (LangChain)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Key Characteristics
Abstraction Level: Agent wrapper
Responsibility: Execute ReAct reasoning loop
Dependencies: LangChain's create_react_agent
Customization: Override for custom agent behavior



Base class for all workflow nodes providing common execution patterns.
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          BaseNode<S>                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  - node_name: str                           â”‚
â”‚  - logger: Logger                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  + __call__(state: S, config): S           â”‚
â”‚  + process(state: S, config): S            â”‚  â† Must Override
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â”‚ provides
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Decorators Applied:                        â”‚
â”‚  - @time_it                                 â”‚
â”‚  - @log_node_exceptions                     â”‚
â”‚  - OpenTelemetry tracing                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Execution Flow
Request â†’ __call__() â†’ [Context Setup] â†’ [Tracing Start] 
                                      â†“
                              process() â† Implemented by subclass
                                      â†“
                              [Update State] â†’ [Tracing End] â†’ Response

Key Characteristics
Abstraction Level: Node/Task processor
Responsibility: Process state transformations
Pattern: Template Method
Generic Type: S represents state type
-------------------------------------------------------
BaseWorkflow
Foundation for creating LangGraph state machines with node management.
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              BaseWorkflow<S>                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  - state_class: Type[S]                               â”‚
â”‚  - name: str                                          â”‚
â”‚  - graph: StateGraph                                  â”‚
â”‚  - compiled_app: CompiledGraph                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  + add_node(name, node)                               â”‚
â”‚  + add_edge(start, end)                               â”‚
â”‚  + add_conditional_edges(node, router, routes)        â”‚
â”‚  + set_entry_point(node)                              â”‚
â”‚  + set_finish_point(node)                             â”‚
â”‚  + compile(): CompiledGraph                           â”‚
â”‚  + run(state, config): S                              â”‚
â”‚  + arun(state, config): S                             â”‚
â”‚  + astream(state, config): AsyncIterator[S]           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â”‚ builds
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         LangGraph StateGraph                          â”‚
â”‚         (State Machine)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

BaseWorkflow Creation
    â”‚
    â”œâ”€â†’ Add Nodes (processing units)
    â”œâ”€â†’ Add Edges (direct transitions)
    â”œâ”€â†’ Add Conditional Edges (routing logic)
    â”œâ”€â†’ Set Entry/Exit Points
    â”‚
    â–¼
Compile â†’ StateGraph â†’ Executable Workflow

Abstraction Level: Workflow orchestrator
Responsibility: Build and execute state machines
Pattern: Builder + Template Method
State Management: Generic type S for type safety

------------------------------------------
AbstractChatbotWorkflow
Specialized workflow base for chatbot applications with full lifecycle management.

1. initialize_workflow()
       â”‚
       â”œâ”€â†’ Create WorkflowState
       â”œâ”€â†’ Setup feature toggles
       â”œâ”€â†’ Configure checkpointer
       â”œâ”€â†’ Initialize conversation
       â”‚
       â–¼
2. process_workflow()
       â”‚
       â”œâ”€â†’ Stream events
       â”œâ”€â†’ Parse responses
       â”œâ”€â†’ Update conversation
       â”‚
       â–¼
3. Complete

Abstraction Level: Application workflow
Responsibility: Full chatbot lifecycle
Integration: Database, features, auth, history
Pattern: Facade + Template Method

RAG Workflow - Query Subworkflow
Specialized subworkflow for query processing and understanding.
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Query Workflow (Subworkflow)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚   â”‚ Query        â”‚â”€â”€â”€â”€â”€â†’â”‚ Entity           â”‚       â”‚
â”‚   â”‚ Parsing      â”‚      â”‚ Extraction       â”‚       â”‚
â”‚   â”‚ Node         â”‚      â”‚ Node             â”‚       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                           â”‚
        â”‚ State Transformation      â”‚
        â–¼                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            SubWorkflowState                          â”‚
â”‚  - query_components                                  â”‚
â”‚  - entities                                          â”‚
â”‚  - internal_query                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Input State (CoreWorkflowState)
    â”‚
    â–¼
enter_query_team_chain() â† Transforms core state to sub-state
    â”‚
    â–¼
QueryParsingNode.process()
    â”‚ Updates state with:
    â”‚ - query_components
    â”‚ - query_intent
    â”‚ - fragments
    â–¼
EntityExtractionNode.process()
    â”‚ Updates state with:
    â”‚ - extracted entities
    â”‚
    â–¼
Output State (SubWorkflowState) â†’ Merged back to Core


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        CoreRagWorkflow                                   â”‚
â”‚        extends AbstractChatbotWorkflow                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  Composition of Sub-Workflows:                          â”‚
â”‚                                                          â”‚
â”‚  1. Request Validation Team                             â”‚
â”‚  2. Caching Team                                        â”‚
â”‚  3. Query Team              â† Query Subworkflow         â”‚
â”‚  4. Pre-Retriever Team                                  â”‚
â”‚  5. Retriever Team                                      â”‚
â”‚  6. Response Generation Team                            â”‚
â”‚  7. Response Augmentation Team                          â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


BaseNode provides:
Automatic timing via @time_it
Exception handling via @log_node_exceptions
OpenTelemetry tracing
Context management
Aggregate tracking
Node Implementation provides:

Business logic in process()
State transformations
Agent orchestration





Facade Pattern Example 
# Complex subsystem classes
class DVDPlayer:
    def on(self): print("DVD Player on")
    def play(self, movie): print(f"Playing {movie}")

class Projector:
    def on(self): print("Projector on")
    def set_input(self, device): print(f"Projector input set to {device}")

class SoundSystem:
    def on(self): print("Sound system on")
    def set_volume(self, level): print(f"Volume set to {level}")

# Facade
class HomeTheaterFacade:
    def __init__(self, dvd, projector, sound):
        self.dvd = dvd
        self.projector = projector
        self.sound = sound

    def watch_movie(self, movie):
        print("Get ready to watch a movie...")
        self.dvd.on()
        self.dvd.play(movie)
        self.projector.on()
        self.projector.set_input("DVD")
        self.sound.on()
        self.sound.set_volume(20)

# Client code
dvd = DVDPlayer()
projector = Projector()
sound = SoundSystem()
home_theater = HomeTheaterFacade(dvd, projector, sound)

home_theater.watch_movie("Inception")


Chain of Responsibility - Design Pattern 
from abc import ABC, abstractmethod

# Abstract Handler
class Handler(ABC):
    def __init__(self, successor=None):
        self.successor = successor
    
    @abstractmethod
    def handle(self, request):
        pass

# Concrete Handlers
class LowLevelHandler(Handler):
    def handle(self, request):
        if request < 10:
            print(f"LowLevelHandler handled request: {request}")
        elif self.successor:
            self.successor.handle(request)

class MidLevelHandler(Handler):
    def handle(self, request):
        if 10 <= request < 20:
            print(f"MidLevelHandler handled request: {request}")
        elif self.successor:
            self.successor.handle(request)

class HighLevelHandler(Handler):
    def handle(self, request):
        if request >= 20:
            print(f"HighLevelHandler handled request: {request}")
        elif self.successor:
            self.successor.handle(request)

# Client code
handler_chain = LowLevelHandler(MidLevelHandler(HighLevelHandler()))

requests = [5, 14, 25]

for req in requests:
    handler_chain.handle(req)


Builder + Template Method 
from abc import ABC, abstractmethod

class ReportBuilder(ABC):
    @abstractmethod
    def add_header(self): pass

    @abstractmethod
    def add_body(self): pass

    @abstractmethod
    def add_footer(self): pass

    @abstractmethod
    def get_result(self): pass

class TextReportBuilder(ReportBuilder):
    def __init__(self):
        self.parts = []

    def add_header(self):
        self.parts.append("=== REPORT ===")

    def add_body(self):
        self.parts.append("This is the report body.")

    def add_footer(self):
        self.parts.append("=== END ===")

    def get_result(self):
        return "\n".join(self.parts)

class HtmlReportBuilder(ReportBuilder):
    def __init__(self):
        self.parts = []

    def add_header(self):
        self.parts.append("<h1>Report</h1>")

    def add_body(self):
        self.parts.append("<p>This is the report body.</p>")

    def add_footer(self):
        self.parts.append("<footer>End</footer>")

    def get_result(self):
        return "".join(self.parts)

class ReportGenerator:
    def generate(self, builder: ReportBuilder):
        # Template Method
        builder.add_header()
        builder.add_body()
        builder.add_footer()
        return builder.get_result()

generator = ReportGenerator()

text_report = generator.generate(TextReportBuilder())
html_report = generator.generate(HtmlReportBuilder())
print(text_report)
print(html_report)


Template + Strategy Pattern 
from abc import ABC, abstractmethod

class ScoringStrategy(ABC):
    @abstractmethod
    def score(self, data):
        pass

class SimpleScoring(ScoringStrategy):
    def score(self, data):
        return sum(data)

class WeightedScoring(ScoringStrategy):
    def score(self, data):
        return sum(v * 2 for v in data)

class DataProcessor:
    def __init__(self, strategy: ScoringStrategy):
        self.strategy = strategy

    def process(self, data):
        # Template Method
        cleaned = self.clean(data)
        result = self.calculate(cleaned)
        return self.format(result)

    def clean(self, data):
        return [d for d in data if d > 0]

    def calculate(self, data):
        # STRATEGY USED HERE
        return self.strategy.score(data)

    def format(self, result):
        return f"Final Score: {result}"

data = [1, -2, 3, 4]

processor1 = DataProcessor(SimpleScoring())
processor2 = DataProcessor(WeightedScoring())

print(processor1.process(data))
print(processor2.process(data))


Data Preprocessing 
------------------
DIP 
1) Workflows - View/Edit/Access 
2) Prompts & Governance 
3) G-C-S-E Prompt Design 
4) Testing & Metrics 

Enterprise Document Intelligence Platform 
Multimodal Extraction 
Field-level Validation 
Traceable Workflows 
HTTP Callback integration  - An event hook is a callback function that runs automatically when a specific event happens inside a library.
In botocore (the low-level library behind boto3), AWS request execution is broken into well-defined stages. Botocore emits events at each stage, and you can hook into those events to run your own code.

Layout + OCR + LLM Extraction 
- Retry & Timeout Handling 
- Error Categorization 
- Prompt Governance 
- DIP Console + Databricks orchestration 

Input Sources PDFS/Images/Emails -> DIP Extraction Enigne OCR + Layout + LLM -> Workflow Layer Validation + Retry -> Ouput/Callback JSON + API's 

Bayern FUT, 
Union FUT
MEAG CDS are the active workflows 

Future 
Bayern FUT and Union FUT focus on futures processing, showcasing DIP's ability to handle diverse futures instruments 
Credit Default SWAP Workflow - MEAG CDS manages credit default swaps, highlighting DIP's adaptability to complex credit instruments and documents 

Configuration and Automation 
Understanding workflow setups, prompts is essential for applying best practices in automation.


Application Setup 
Access and Workflow Setup 
1) DIP Portal - 
2) Verify Permissions - 
3) Create & Edit workflows via Workflow Configuration 
Document Ingestion Configuration 
1) Choose Between Manual Upload or SFTP for document ingestion, ensuring backup paths are defined correctly.
Indexing and Classification Setup 
1) Setup index fields with Tagged or Ordered formats and enable classification with confidence thresholds for accuracy. 
Testing and Deploymet 
1) Test Workflows by reviewing logs at multiple stages, then deploy and monitor performance to handle exceptions. 


Worklfow Overview 
Document Ingestion 
System Prompt 
Pydantic 
Output JSON & Storage 
Callback -> Downstream  IDPP will use the API to extract our output 

DIP Console Access 
1) View Inputs & Outputs
2) Inspect Extraction 
3) Review run History 
4) Playground Testing 
5) GT vs Output Comparison 

Databricks Access 
Job Logs & Artifacts 
1) Parameter inspection 
2) Model/gateway/parsing error diagnosis 
3) DWC-created workflows need admin 

Prompt Governance 
1) Centralized Prompt Hub 
2) Versioning & Tagging 
3) Change History 
4) Rollback Support 
5) Cross-Workflow reuse 

Draft Prompt -> Review Approval -> Prompt Hub Versioning -> Tag as `latest` / Rollback 

Stable Predictable Output 
- Avoid Silent Drift 
- Support Audits 
- Improve Collaboration 

----------------------------
Key Prompt Elements 
1) Effective Prompt include Goal, Context, Expected Output, Source Reference to guide extraction tasks clearly. 
2) Prompt Tuning and Enhancement - Iterative Prompt Refinement and use of system and user prompts improve model accuracy and task performance.
3) Review and version Control - Regular reviews and version control via Prompt Hub ensure consistent and traceable prompt updates. 


G-C-S-E Framework 
G - Goal      - Goal - Task Objective 
C - Context   - Context - Business / Document Context 
S - Source    - Source - Rules & References 
E - Expectations - Expectations - Schema, formats, examples 

Prompt Best Practices 
1) Explicit Schemas  - strict output structure (schema) so the model knows exactly what fields to return and in what format. 
2) Avoid ambiguity   - Remove vague or unclear instructions. Prompts should be precise, direct, and leave no room for multiple interpretations. 
3) Use positive examples  - Provide correct, representative examples that show the model what a good output looks like. Positive samples lead to more reliable extraction. 
4) Concise Deterministic prompts  - keep prompts short, focused, and deterministic - designed to produce consistent results every time without randomness. 
5) Avoid long Chain-of-Thought - Do not encourage the model to produce lengthy reasoning. For extraction tasks, shorter reasoning reduces errors and keeps output structured. 
6) No Hallucination - Ensure the model does not invent or guess values. It must extract only what explicitly exists in the document. 
7) Normalize formats - force standardized formats (eg: YYYYMMDD for dates, 2-decimal numbers, ISO currency codes) to ensure consistency across outputs. 
8) Fallback to null - if a field does not exist in the document. 
9) Table alignment rules - Specify how rows and columns should align when extracting data from tables. Define how to handle merged cells, blank rows, or inconsistent formatting. 


Tuning Methodology 
Single-Factor iteration 
 - Modify model OR examples OR rules. 
 - Track changes.
UAT A/B testing 
 - Compare accuracy.
 - Promote winner -> latest.

Testing & Metrics 
Field - Level Accuracy - (correct field predictions) / (total field predictions)
Completeness - (fields present) / (fields required)
Correctness 
Accuracy - (fully correct records) / (total records)
If one field is wrong â†’ record is wrong.
Record	Status
Record 1	âŒ
Record 2	âœ…
Record 3	âŒ

Case Pass Rate - Percentage of test cases that pass all acceptance criteria.
(passed test cases) / (total test cases)

Latency - end_time - start_time
Usually tracked as:
Average latency
P95 / P99 latency
Max latency

Accuracy Targets 
----------------
> 95% field Accuracy 
> 98% mandatory fields 
Use real UAT samples 

Accuracy Dashboard 
------------------
Field Accuracy - 95% 
Mandatory - 98%
Null Rate - 2% 
Case Pass - 98% 
Latency - 10s 


Demo Flow 
Open Workflow 
- Review Extraction 
- Run Playground Tests 

Databricks logs 
- Error Categorization 
- Prompt Versioning & Rollback 

Run Failure 
Check Logs 
Categorize: Model/Gateway/Parsing/Timeout 
Fix Prompt/ Retry/ Adjust Config 

Understand DIP 
   - Access Workflows 
   - Design & Tune Prompts 
   - Validate Accuracy 

Derivatives Doc Intelligence Project 

Prompt Support 
Testing Support 
Root Cause Analysis 




##Core Mission 
In JSON format extraction the values related to each field. 
Create a list object "Trades" for each "Financial Instrument ID". Each object in the JSON list should consist of the following fields extracted directly from the document content. 

##Document Context: 
{Context}

##Fundamental Extraction Rules 
## 1. Data Integrity 
- **Extract only explicit data**: Never invent, hallunicate, or guess missing information 
- **NULL for missing**: Return `null` for any required field not present in the text 
- **Type compliance**: Ensure extracted data matches expected shcema types 
- **Context awareness**: Use Headers, sections, and surrounding text for proper classification 

## 2. Data Standardization 
- **Currency codes**: Convert symbols to ISO 4217 codes ($ -> USD, ) 
- **Date format**: 
- **Number format**: 
Use regex to clean such cases if necessary. 

### 3. Quality Assurance 
- **Pattern Recognition**: 
- **Hierarchical respect**:
- **Metadata distinction**: 
- **Multi-page consolidation**:
- **Duplicate resolution**:
- **Character handling**:
- **Whitespace normalization**: 

## Extraction Schema 
**Output Structure**: Always return JSON with `"Trades": []` array. Each Trade is a separate array element. 


S*S*A*I tenant - Different tenant 

-----------------------------------------------
Collateral
D*I*P - K*T 

CSA Docs in D*I*P
Onboard New Doct Type - Without Extra Development 
Platform Architecture 
Databricks Demo 

Eligibilty 
Triparty 

Session 1 
Solution Overview 
IT Architecture for APIs and DataFlow 
LLM for Structured Output with JSON and JSON Schema 
RAI Gateway and SDK Client 

Session 2  with live demo on Databricks
Deep Dive of CSA Bilateral Prompt 
Testing the prompt with Ground Truth and Scripts on Databricks (Without self service capability) 
DIP Admin UI for onboarding new use-cases and updating prompts on existing workflows 


JSON Schema Design 
Azure Could and SSC SDLC 
API Integrations, RAI SDK, PDF Plumber, LangChain -
Databricks Workflow Automation (Scripts for accuracy testing)  - How to navigate notebooks, 
Prompt Optimization(Design and optimise prompts) 

Test Automation (accuracy scripts, workflow validation, Python) 
API Testing (integration points) - DIP Interacts with several systems, so making sure the data flow is correct. 
Data validation eligibility fields, schema checks) 

Collateral Management Domain Knowledge (asset eligibility, governance) 
Accuracy Reporting & Validation (interpret dashboards, metrics) 
Gen AI & Prompt Engineering 

Scope / Problem Statement 
To Reduce the manual effort by the collateral Onboarding team associated with Credit Support Annex Document & ISDA. This initiative would enable automation to reduce time spent 
extracting account data details from ISDAs by creating and saving an output file. 

ISDA Master Agreement (that governs the derivatives b/w two parties)- Standardized legal agreement for OTC derivatvies that defines the rights, obligations and risk management terms between counterparties. 
CSA (what types are allowed, eligibiltiy rule etc) - is an annex to ISDA that sets rules for collateral exchange to reduce credit risk in those derivative transactions. 

DIP is Document Intelligence Platform that automates the extraction and processing of critical data from legal agreements like ISDAs and CSAs, 
It leverages workflow automation, structured data output, and integration with existing collateral management systems to reduce manual effort, improve accuracy. 
and accelerate onboarding for the collateral team 

C*ollateral Plus or C*olline 


Manual Process 
Client Provides Legal Document Information in different forms (pdf documents, Excel, word etc) 
User to read and extract documentation/add into Excel/normalize data 
Review by 2nd user to ensure data accuracy 
Load data into CollateralPlus/Colline 
Approve Entries in CollateralPlus/Colline 
Active Agreements in CollateralPlus/Colline 


Data Flow 
DIP for mulimodal LLM extraction 
                                     RAI -> EAM [Enterprise API Mangement Gateway] 
Collateral+ EAM     Ingest - AFR/OCR - MultiModal LLM - BBox - HTTP Callback                 EAM + Collateral+ 

                  CSA Workflow   - Uses Dynamic workflow editor to create workflow (manage prompts, settings, model names etc) 


            Collateral Use Case Tenant     App RAI Credentials, data isolation, security via entitlements, 
  
                  DIP Platform - Shared core platform, with reusable features 


IT Architecture for APIs and Dataflow 
DIP AKS Endpoints via EAM will receive the request and trigger the databricks workflow Postgres and AKS are used for metadata and APIs. 
Databricks Executes the actual extraction workflow 
Callback API to C+ with the result is triggered from the databricks environment. 
C+ review the output and complete manual edits where AI is not accurate 
Reviewed results are submitted back to the GT Metrics API for accuracy and metrics dashboard calculations on DIP Platform.

                                          Extraction Fields Requirement/Business Inputs 

                                      CSA Fields       Eligibility Fields 
                                         Ref Fields            Business Discription 
 DIP Collateral Tenant + Workflow 
                 
                                       Prompt Json Schema 
                                             |
Collateral Plus --->   Ingest API --> MultiModal Extraction  -> AFR Layout 
                                                                   |
                                                                  Bounding Box Info Extraction      C+ Push API Call ---> Push    C+   Business Review 
                                                                    |                                |
                                                                   Validation ----->        Post Processing 


Feedback and Review will again ingest it back into the Collateral 
DIP Collateral Tenant + Review Result Capture 

View API -> Store to volume[Databrics Volume] -> Populate to metrics Dashboard - Dashboard features 

RAI Gateway 
RAI Credentials 
EAM Onboarding 


Apigee  - API Proxies 

Send the Business Review data back to the DIP team or Onboarded EAM Team 



APP Subnet                                                                           Data Subnet 
       Azure Kubernetes Service  
                   NameSpace                              
                       PodNOde 
                               Container DIP Service 
                                                                    Log Agent 
                                                                                     Data Subnet 



Postgres for metadata and configuration data wrt to DIP and then we used the Log Analytics for identifying all the f
Azure Blob Storage - Ingest Document, Extracted Document, Processed Document 
Aunthentication leverages the Azure AD
Azure Key valut 
Internal Communication we used the Event Driven Platform within Azure which is Event Hub 
ACR
JFrog - Artifacts 


DataBricks (DIP Core) 
      Compute Engine 
         Workflows 


Databricks and App is running Azure VM 


Workflow Components 
-------------------
Dynamic Workflow Components on DIP 
1) Basic Info: Use case Details, JOB Type Multimodal 
2) Ingestion: Select Manual Upload and disable SFTP Transfer
3) Classification: Disabled 
4) OCR: Select Azure Document Intelligence, Prebuilt Layout and Plain text 
5) Multimodal, visible only when selecting job type MultiModal), turn off image stitching and choose prompt and model.
6) Bounding Box: Enable and choose prompt and model.
7) HTTP Callback: Enable and provide endpoint details for result callback 
8) Review: Disable (Handled outside DIP Platform) 
9) Export: Choose JSON 

C*+ --> Collateral Plus --> Collateral Agreements and profiles are managed here. T*O*T app code 
DIP -> T*X*T is the app code hosting serveral reusable services including  D*I*P
EAM -> Enterprise API Management -> an API Gateway for inter application endpoint management. Provides authentication using Client ID
RAI - 
PDF Plumber - G*M*A*S -> approved python package for creating images from PDF Files for use with the multimodal LLMs. 
Databricks - A Cloud-Based data processing platform with compute clusters, access management, data connectors for data sources such as blobs and databases 
workflow mangement features and metrics features 
GPT 4.1 - Multimodal LLM model used for the extraction from openai 


https://a*r*i*e*l-uat-















DIP Core 
DR Collateral 























