Agent Flow
History
How RAG should work 
How Agentic RAG Should work - for all questions Boolean Question, Summary Question, Specific Question 
How Multi-Agent Chat History will be
    Chat history will be save for each agent or 
    Chat history will be save for Overall Agent 
How to Save the Chat History in Database - like once completed or for each transaction 
How to Load the Chat History in FrontEnd 
How to Manage State in FrontEnd for Each Page
Logging and Tool Error



Agent Flow 
----------
System Instruction
Human Message - Input 
FunctionMessage - agent_scratchpad - is a placeholder that expects a list of BaseMessage objects to be injected into the prompt at runtime under the name "agent_scratchpad"
    Tool Call
    Observation [Retrieval Context]
AI Mesasge - Final Answer 

---------------------------------------------------------------------------------------------
How RAG should work 
1st Question
Human 
FunctionMessage
AI Message 

2nd Question 
Human 
Chat_History



AIMessage, FunctionMessage, ToolMessage, HumanMessage
This allows the LLM to see prior reasoning and tool use before responding again

prompt.messages
prompt.input_variables
prompt.input_types
prompt.output_parser 
prompt.partial_variables

1. Parsing to Get the Tool Name and Tool Input
chain = prompt | model | OpenAIFunctionsAgentOutputParser()
2. After Getting Observation --> format to Message format 
from langchain.agents.format_scratchpad import format_to_openai_functions
format_to_openai_functions([(result1, observation), ])


History
-------
Message History of Reasoning Step 
Message History of Final AI Answer 

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are helpful but sassy assistant"),
    MessagesPlaceholder(variable_name="chat_history"),
    ("user", "{input}"),
    MessagesPlaceholder(variable_name="agent_scratchpad")
])

memory = memory = ConversationBufferMemory(return_messages=True,memory_key="chat_history")

'chat_memory': ChatMessageHistory(messages=[]),
 'output_key': None,
 'input_key': None,
 'return_messages': True,
 'human_prefix': 'Human',
 'ai_prefix': 'AI',
 'memory_key': 'chat_history'}

memory.chat_memory.add_user_message("Hi, who won the World Cup in 2018?")
memory.chat_memory.add_ai_message("France won the 2018 FIFA World Cup.")

memory.load_memory_variables({})


Logging and Tool Error
----------------------
Add logging to see intermediate outputs?
Convert this into a fully async agent?
Handle tool errors or retry logic?


Context Engineering
-------------------
1.The important process of tuning the instructions and relevant context that an LLM needs to perform its tasks effectively.
2.In blind prompting, you are just asking the system a question [short task description you use in an LLM like ChatGPT]
3.In prompt engineering, you have to think more carefully about the context and structure of your prompt
4.Context engineering is the next phase, where you architect the full context 
 more rigorous methods to obtain, enhance, and optimize knowledge for the system
5.Context engineering involves an iterative process to optimize instructions and the context you provide an LLM to achieve a desired result
6.This includes having formal processes (e.g., eval pipelines) to measure whether your tactics are working.

"the process of designing and optimizing instructions and relevant context for the LLMs and advanced AI models to perform their tasks effectively"
7.encompasses not only text-based LLMs but also optimizing context for multimodal models
8.Designing and managing prompt chains (when applicable)
  Tuning instructions/system prompts
  Managing dynamic elements of the prompt (e.g., user inputs, date/time, etc.)
  Searching and preparing relevant knowledge (i.e., RAG)
  Query augmentation
  Tool definitions and instructions (in the case of agentic systems)
  Preparing and optimizing few-shot demonstrations
  Structuring inputs and outputs (e.g., delimiters, JSON schema)
  Short-term memory (i.e., managing state/historical context) and long-term memory 
       (e.g., retrieving relevant knowledge from a vector store)
  And the many other tricks that are useful to optimize the LLM system prompt to achieve the desired tasks.
9.In other words, what you are trying to achieve in context engineering is optimizing the information you are providing in the context window of the LLM
10.This also means filtering out noisy information, which is a science on its own, as it requires systematically measuring the performance of the LLM.

Example: Context Engineering 
1.Search Plan Agent  - charge of generating a search plan based on the user query.
    Prompt for this Search Plan Agent is Big, have different division in the complete prompt 


Let’s break down the problem into core components that are key to effective context engineering.
Instructions - The instruction is the high-level instructions provided to the system to instruct it exactly what to do.
You are an expert research planner. Your task is to break down a complex research query (delimited by <user_query></user_query>) into specific search subtasks, each focusing on a different aspect or source type.
User Input - The user input wasn’t shown in the system prompt, but below is an example of how it would look.
<user_query> What's the latest dev news from OpenAI? </user_query>
Notice the use of the delimiters, which is about structuring the prompt better
   - This is important to avoid confusion and adds clarity about what the user input is and what things we want the system to generate. Sometimes, the type of information we are inputting is related to what we want the model to output (e.g., the query is the input, and subqueries are the outputs).


Structured Inputs and Outputs - In addition to the high-level instruction and the user input, you might have noticed that I spent a considerable amount of effort on the details related to the subtasks the planning agent needs to produce

                              - This is a really powerful approach, especially when your agent is getting inconsistent outputs that need to be passed in a special format to the next component in the workflow.

-------------
Why not search tool 
The only other tool that would make sense to add is a retrieval tool that retrieves relevant subtasks given a query. Let’s discuss this idea below
---------
RAG & Memory
This first version of the deep research application I have built doesn’t require the use of short-term memory, but we have built a version of it that caches subqueries for different user queries
This is useful to achieve some speed-ups/optimizations in the workflow
If a similar query was already used by a user before, it is possible to store those results in a vector store and search over them to avoid the need to create a new set of subqueries for a plan that we already generated and exists in the vector store
Remember, every time you call the LLM APIs, you are increasing latency and costs.
You can also get more creative about how you are maintaining that vector store and how you pull those existing subtasks into context. Creative and novel context engineering is the moat!
|
Moat

------
State & Historical Context 

In many cases, the agentic system might need to revise all or a subset of the queries, subtasks, and potentially the data it’s pulling from the web search APIs
This means that the system will take multiple shots at the problem and needs access to the previous states and potentially all the historical context of the system.

What does this mean in the context of our use case? In our example, it could be giving the agent access to the state of the subtasks
The revisions (if any), the past results from each agent in the workflow, and whatever other context is necessary to help in the revision phase. 
For this type of context, what we are passing would depend on what you are optimizing for. Lots of decision-making will happen here
Context engineering isn’t always straightforward, and I think you can start to imagine how many iterations this component will require

------------------------
WIP
such as context compression, context management techniques, context safety, and evaluating context effectiveness (i.e., measuring how effective that context is over time). We will be sharing more ideas about these topics in future articles.
Context can dilute or become inefficient (i.e., be filled with stale and irrelevant information), which requires special evaluation workflows to capture these issues.
I expect that context engineering continues to evolve as an important set of skills for AI developers/engineers
Beyond manual context engineering, there are also opportunities to build methods that automate the processing of effective context engineering. I’ve seen a few tools that have attempted this, but there needs to be more progress in this area.

--------------------------
Long Context won't be always good  [derailing the agentic reasoning]

While models like Gemini 2.5 and GPT-4.1 support million-token contexts, Breunig argues that longer isn’t always better; large contexts introduce new and severe failure modes that derail agentic reasoning.

He identifies four core failure types:

Context Poisoning - Once a hallucination enters the context, it can persist and mislead the model. Cited examples from Gemini 2.5 show agents repeating irrelevant strategies due to poisoned goals.
Context Distraction - As the context grows, especially past 100k tokens, agents increasingly rely on past actions instead of generating novel strategies, effectively overfitting to the prompt history. This issue appears earlier in smaller models, such as LLaMA 3.1, where failures emerge as early as 32k tokens.
Context Confusion - Feeding too many tool definitions or irrelevant instructions leads to degraded performance, even if the full context fits within the model's window. The Berkeley Function-Calling Leaderboard shows that adding more tools reduces success rates across models, with smaller models suffering more.
Context Clash - When multi-turn interactions or external inputs provide conflicting information, models tend to latch onto earlier, often incorrect, inferences. A study from Microsoft and Salesforce demonstrated a 39% accuracy drop when benchmark prompts were sharded across turns instead of given all at once.


Context Engineering Link 
https://docs.google.com/document/d/1JU8w-E4LlseFZm-ag22GSBU5A2rp2nb7iFGBNAbFL7k/edit?tab=t.0
https://www.promptingguide.ai/guides/context-engineering-guide







