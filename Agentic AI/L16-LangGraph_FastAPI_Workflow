LangGraph API Key - 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
LangGraph & FastAPI [https://github.com/wassim249/fastapi-langgraph-agent-production-ready-template?tab=readme-ov-file]
Langfuse for LLM observability and monitoring

1. Agentic AI Application with LangGraph and FastAPI
2. Structured logging with environment-specific formatting
3. Rate limiting with configurable rules
4. PostgreSQL for data persistence
5. Docker and Docker Compose support
6. Prometheus metrics and Grafana dashboards for monitoring


Security
JWT-based authentication
Session management
Input sanitization
CORS configuration
Rate limiting protection

Developer Experience
Environment-specific configuration
Comprehensive logging system
Clear project structure
Type hints throughout
Easy local development setup


Model Evaluation Framework
Automated metric-based evaluation of model outputs
Integration with Langfuse for trace analysis
Detailed JSON reports with success/failure metrics
Interactive command-line interface
Customizable evaluation metrics


Commands 
uv sync installs (or removes) the exact set of packages specified in your project's lockfile (uv.lock) into your environment
uv lock --upgrade updates your uv.lock file
cp .env.example .env.[development|staging|production] # e.g. .env.development
Create the Local Database or Supabase 
POSTGRES_URL="postgresql://:your-db-password@POSTGRES_HOST:POSTGRES_PORT/POSTGRES_DB"
Create a PostgreSQL database (e.g Supabase or local PostgreSQL)

ORM
You don't have to create the tables manually, the ORM will handle that for you.But if you faced any issues,please run the schemas.sql file to create the tables manually

GNU Make
make [dev|staging|production] # e.g. make dev
make looks in the Makefile in your project’s root directory.
If that Makefile has a target like:
dev:
    uv sync --inexact --env-file .env.development
    uv run python main.py

make docker-build-env ENV=development
docker-build-env:
    docker build --build-arg ENV=$(ENV) -t my-app:$(ENV) .
make docker-run-env ENV=[development|staging|production] # e.g. make docker-run-env ENV=development


Complete Docker Setup with Production Application
-------------------------------------------------
The Docker setup includes:
FastAPI application

PostgreSQL database
Prometheus for metrics collection
Grafana for metrics visualization
Pre-configured dashboards for:
API performance metrics
Rate limiting statistics
Database performance
System resource usage

Access Monitoring Tool 
# Prometheus metrics
http://localhost:9090

# Grafana dashboards
http://localhost:3000
Default credentials:
- Username: admin
- Password: admin

Model Evaluation 
----------------
The project includes a robust evaluation framework for measuring and tracking model performance over time. The evaluator automatically fetches traces from Langfuse, applies evaluation metrics, and generates detailed reports.

Running Evaluations
You can run evaluations with different options using the provided Makefile commands:

# Interactive mode with step-by-step prompts
make eval [ENV=development|staging|production]
# Quick mode with default settings (no prompts)
make eval-quick [ENV=development|staging|production]
# Evaluation without report generation
make eval-no-report [ENV=development|staging|production]

Evaluation Features
Interactive CLI: User-friendly interface with colored output and progress bars
Flexible Configuration: Set default values or customize at runtime
Detailed Reports: JSON reports with comprehensive metrics including:
Overall success rate
Metric-specific performance
Duration and timing information
Trace-level success/failure details

Customizing Metrics
Evaluation metrics are defined in evals/metrics/prompts/ as markdown files:
Create a new markdown file (e.g., my_metric.md) in the prompts directory
Define the evaluation criteria and scoring logic
The evaluator will automatically discover and apply your new metric

evals/reports/evaluation_report_YYYYMMDD_HHMMSS.json
Each report includes:

High-level statistics (total trace count, success rate, etc.)
Per-metric performance metrics
Detailed trace-level information for debugging


----------------------
.github
Issue Templates
Files in .github/ISSUE_TEMPLATE/ define pre-filled templates for bug reports, feature requests, etc.
This makes it easier for contributors to provide consistent and complete info when creating issues.
Pull Request Templates
A .github/pull_request_template.md file gives contributors a checklist or structure for PR descriptions.
Helps maintainers quickly understand the changes.
Workflows (GitHub Actions)
.github/workflows/ holds YAML files that define CI/CD pipelines.
For example, you can run tests automatically when someone pushes code or creates a PR.
Funding Information
.github/FUNDING.yml can list ways for people to sponsor the project (GitHub Sponsors, Patreon, etc.).
CODEOWNERS
.github/CODEOWNERS specifies which users or teams must review certain parts of the code.
GitHub will automatically request their review when relevant files change.
Community Health Files
Files like CONTRIBUTING.md, CODE_OF_CONDUCT.md, or SECURITY.md inside .github/ set contribution rules, behavior guidelines, and vulnerability reporting methods.
GitHub shows these in issue/PR creation screens.

GitHub provides cloud-hosted runners (virtual machines) where the workflow executes.
Step 1 — Workflow Trigger
Trigger: Runs on every push to main or any pull request.
Steps: Checkout your code → set up the environment → install dependencies → run your tests.
name: Run Tests
Step 2 — GitHub Spins Up a Runner
When you push code or open a PR:
Step 3 — Your Tests Run
If your workflow says run: npm test, GitHub executes that command in the VM.
The test framework you use (Jest, Pytest, JUnit, etc.) will run exactly like on a local machine — but inside the GitHub-hosted environment

Step 4 — Results in the GitHub UI
You see logs for each step directly in the Actions tab of your repo.
If a test fails, the workflow is marked ❌ failed, and you can click in to see the exact error.

on:
  push:
    branches: [ main ]
  pull_request:

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 18

      - name: Install dependencies
        run: npm install

      - name: Run tests
        run: npm test

Running Locally & Debugging 
https://docs.github.com/en/actions/concepts/runners/github-hosted-runners
https://github.com/nektos/act - a CLI tool that simulates GitHub Actions locally.

The logic for the tests lives in your repository (or in a dependency you install).
The workflow file in .github/workflows/ is just telling GitHub Actions when and how to run those tests.
If you don’t have tests, the workflow can still run other things (like linting, building, or deployments).


Job: build-and-push
Runs on an Ubuntu virtual machine provided by GitHub Actions.
Steps inside it:
Checkout code
Uses actions/checkout@v3 to clone your repository into the workflow runner.
Install utilities
Updates apt packages.
Installs make (needed for the build command).
Sanitize repository name
Converts your repo name to lowercase.
Replaces non-alphanumeric characters with hyphens.
Stores it in the REPO_NAME environment variable (so Docker tags are valid).
Build Docker image
Runs make docker-build-env ENV=production (this likely builds your app’s production Docker image).
Tags the image with:
<docker_username>/<sanitized_repo_name>:production
Log in to Docker Hub

Uses secrets DOCKER_USERNAME and DOCKER_PASSWORD stored in your repo’s GitHub settings to authenticate.
Push image to Docker Hub
Pushes the built and tagged Docker image to your Docker Hub repository.

Finding dotenv
--------------
1️⃣ A framework-specific override
Some frameworks and tools wrap python-dotenv and add their own rules.
For example:
Django with django-environ
Flask with flask run in debug mode
FastAPI/Uvicorn if configured with python-dotenv hooks
Poetry / Invoke / custom CLI that calls load_dotenv(".env.development") internally
They often check environment variables like ENV, APP_ENV, DJANGO_SETTINGS_MODULE, etc., and automatically pick .env.development if ENV=development.

from dotenv import load_dotenv, find_dotenv
load_dotenv(find_dotenv())
find_dotenv() is told to look for .env.development (via usecwd=True or a custom search), it will pick that up.

import os
from dotenv import load_dotenv

env = os.getenv("APP_ENV", "development").lower()
load_dotenv(f".env.{env}")

----------------------------------------------------------------
A) what happens when the Gradio app launches, 
(B) what happens when you click Generate Essay, 
(C) what happens when you click Continue Essay, and 
(D) the exact stopping / thread‑creation behavior + a few recommended fixes.

1.writer_gui.__init__ runs:
stores graph, share, sets up bookkeeping fields: partial_message = "", response = {}, max_iterations = 10, iterations = [], threads = [], thread_id = -1, thread = {"configurable": {"thread_id": str(self.thread_id)}}.
calls self.create_interface() which builds the Gradio UI and registers all event handlers. Nothing has invoked the graph yet — you’ve only created UI components and attached callbacks.
2.create_interface() builds Gradio components (textboxes, buttons, dropdowns, CheckboxGroup stop_after, etc.) and wires their .click() / .input() / .then(...) chains. Those handlers are registered but not executed until user interaction.

B — Click Generate Essay (the exact call/handler chain)
gen_btn.click(vary_btn, <...>).then(
    fn=self.run_agent, inputs=[gr.Number(True, ...), topic_bx, stop_after], outputs=[live], show_progress=True
).then(fn=updt_disp, inputs=None, outputs=sdisps).then(...)

Sequence when user clicks Generate Essay:
-----------------------------------------
1.vary_btn is called first (the immediate .click(...) target). In your code it just returns a gr.update(variant=stat) to change button styling. (So the button variant may change to a “secondary” style to signal work.)
2.run_agent is invoked as a generator with inputs:
start=True (so we create a new thread),
topic from topic_bx,
stop_after from the CheckboxGroup.
Because show_progress=True and run_agent is a generator (it yields), Gradio will stream the generator’s yielded values to the components listed in outputs as the generator yields.

3.Inside run_agent(start=True, topic, stop_after) (step-by-step):
self.iterations.append(0) — allocate a per-thread iteration counter (index aligns with new thread id).
config = {...} is created (seed task payload).
self.thread_id += 1 — new thread id created (first click: -1 -> 0).
self.threads.append(self.thread_id) — remember this thread id.
self.thread = {"configurable": {"thread_id": str(self.thread_id)}} — the thread descriptor used for graph calls.
Enter while self.iterations[self.thread_id] < self.max_iterations: loop.
Each loop iteration

   self.response = self.graph.invoke(config, self.thread) — this is the important call: it runs the graph/agent one step. On the first iteration config is the seed; after the first loop iteration the code sets config = None so subsequent invoke calls continue the thread using the stored thread state.
   self.iterations[self.thread_id] += 1
   self.partial_message += str(self.response) + divider
   lnode, nnode, _, rev, acount = self.get_disp_state() (pulls lnode, next, revision_number, count from the graph state for the current thread)
   yield self.partial_message, lnode, nnode, self.thread_id, rev, acount

   stop checks after yield:
      if not nnode: return (graph indicates there is no next node — end)
      if lnode in stop_after: return (user selected this node as a stopping point)
      otherwise loop continues (with config=None after first iteration)

4.Gradio receives yields: each yield is sent to the outputs you listed when you registered the click.
Important note about your current code: run_agent yields 6 things (partial_message, lnode, nnode, thread_id, rev, acount) but your .then(..., outputs=[live]) lists only one output (live). Gradio will map the first yielded value to live. The other yielded values are not automatically wired to the other UI fields — you rely on the subsequent updt_disp .then(...) call to refresh those fields after the generator finishes. So in current code: live textbox gets streaming updates, but the other UI fields (lnode box, count, revision, etc.) are updated only when updt_disp runs after the generator returns.
5.After run_agent returns (because of one of the stop conditions or reaching max_iterations) the chained .then(fn=updt_disp, ...) executes and updates the UI boxes in sdisps to reflect the final state of the thread.


C — Click Continue Essay
cont_btn.click(vary_btn, ...).then(
    fn=self.run_agent, inputs=[gr.Number(False, visible=False), topic_bx, stop_after], outputs=[live]
).then(fn=updt_disp, ...)

Key differences and behaviour:
start=False → you DO NOT create a new thread. run_agent will not increment self.thread_id or append to self.threads. Instead it sets config = None and sets self.thread to the current self.thread_id.
run_agent uses self.iterations[self.thread_id] as the starting iteration counter (so it resumes from where that thread left off).
The loop calls self.graph.invoke(config, self.thread) repeatedly, continuing the thread’s progress until one of the same stop conditions occurs (no next node, lnode in stop_after, or hitting max_iterations).
So Continue resumes the last-created thread (or whichever self.thread_id you switched to via switch_thread) and continues its stepwise execution.

D — Stopping conditions, thread creation summary, and important gotchas

Thread creation
Only run_agent with start=True (i.e., when you click Generate Essay) increments self.thread_id and creates a new thread id. So each Generate click == one new thread.
Continue does not create new threads.
copy_state and modify_state update states inside the current thread — they do not create a new thread id.

When run_agent stops
It returns (stops) when any of:
self.iterations[self.thread_id] >= self.max_iterations (default 10), OR
nnode is falsy (if not nnode:) — graph says “no next node”, OR
lnode in stop_after — user-specified list of nodes to interrupt on.


Important gotchas in your code (things that cause surprising behavior)
stop_after default is set to checks (you pass all node keys as the default). That means lnode in stop_after will almost always be true on the first iteration, so the generator will usually stop immediately after the first invoke. If you don’t want immediate stopping, set the default stop_after to [] or to a smaller set.
run_agent yields six values but the outputs array for the .then(fn=self.run_agent, ...) call lists only live. So only partial_message (the first yielded value) is streamed. The other UI components are updated only at the end by updt_disp. If you want live updates of lnode, nnode, etc. on each yield, make the outputs list include those target components (and wire them in the .then(...) call).
self.partial_message is never reset when you create a new thread: if you click Generate twice, the new thread’s output will be appended to the existing partial_message unless you explicitly reset it at the start of a new run.
self.iterations is a list and you append a 0 for a new thread; ensure your code never sets thread_id to a number for which iterations has no entry — that will raise an index error if you attempt to read iterations[self.thread_id].
update_thread_pd() references threads (likely a global), while elsewhere you use self.threads. That looks like a bug — prefer self.threads consistently.

E — Quick recommended fixes (small code snippets)
Reset partial_message on new run:
if start:
    self.partial_message = ""        # <--- reset per new thread
    self.iterations.append(0)
    ...
Avoid immediate stop due to default stop_after:
# when creating the CheckboxGroup
stop_after = gr.CheckboxGroup(checks, label="Interrupt After State", value=[], ...)

Stream other fields during generator yields — expose all yields to Gradio:
change .then(fn=self.run_agent, outputs=[live]) to list all outputs matching the yielded tuple:

outputs=[live, lnode_bx, nnode_bx, threadid_bx, revision_bx, count_bx]
and ensure the yield matches that order.

Make thread bookkeeping robust:
# safer append:
if start:
    self.thread_id += 1
    self.threads.append(self.thread_id)
    if len(self.iterations) <= self.thread_id:
        self.iterations.append(0)


F — TL;DR (very short)
On launch: UI created, handlers registered, no graph calls yet.
On Generate: a new thread_id is created (self.thread_id += 1), graph.invoke(config, thread) is called repeatedly by the run_agent generator; results are yielded to Gradio until max_iterations, no next node, or lnode in stop_after stops the loop.
On Continue: no new thread is created; run_agent resumes invoking the same thread (using config=None) until the same stop conditions happen.
Each Generate click creates one thread id; Continue reuses the current thread id.


If you want, I can:
produce a sequence trace for a concrete example (I can mock graph.invoke responses so you can see exact yields / UI updates), or
provide a patched version of your code with the fixes above (resetting partial_message, fixing stop_after default, wiring the yielded values to the proper outputs), or
point out exact lines to change to get live updates for lnode / count during streaming.
Which would you like next?



---------------
gr.Number

That gr.Number(True, ...) in your code isn’t creating a visible number input for the user — it’s basically being used as a hidden constant input to the event handler.
inputs=[gr.Number(True, visible=False), topic_bx, stop_after]

gr.Number(...) normally creates a numeric input box in Gradio.
But here, instead of giving it a number like 42 or 0, they gave it True
Gradio will try to coerce True into a number (True → 1) if it’s actually used, but here it’s mostly being used as a flag.
visible=False means the user never sees it in the UI — it’s just a hidden, pre-filled input value.
That hidden value is passed as the first argument to run_agent.

-----------------------------------------------------------
**Very Very Important about the get_state 

In __init__ they set:
self.thread_id = -1
self.thread = {"configurable": {"thread_id": str(self.thread_id)}}

So the very first real thread id is 0.
The only time self.thread is accessed before the first Generate is if you call UI functions like updt_disp() or get_state() without starting a thread — but those would likely error if the graph has no thread state for -1.

In practice, self.thread with thread_id = -1 is never actually used to call graph.invoke or graph.get_state — because those calls only happen inside run_agent, and when you run with start=True (Generate button), they immediately increment:
This means that before you click Generate Essay, self.thread refers to a non-existent “thread -1”.




