https://cookbook.openai.com/examples/evaluation/getting_started_with_openai_evals
An eval is a task used to measure the quality of the output of an LLM or LLM system.
Given an input prompt, an output is generated. We evaluate this output with a set of ideal answers and find the quality of the LLM system.

How different model versions and prompts might affect your use case.
Developing a suite of evals customized to your objectives will help you quickly and effectively understand how new models may perform for your use cases.
You can also make evals a part of your CI/CD pipeline to make sure you achieve the desired accuracy before deploying.

Matching Eval 
JSON Eval 
Model Grading - LLM as a Judge 
     Model grading works best with the latest, most powerful models like GPT-4 and if we give them the ability to reason before making a judgment.
     Model grading will have an error rate, so it is important to validate the performance with human evaluation before running the evals at scale.

Human evaluation before running the evals at scale  - ?

OpenAI Evals
we have discovered several "templates" that accommodate many different benchmarks.
Basic Eval Templates: These contain deterministic functions to compare the output to the ideal_answers. In cases where the desired model response has very little variation, such as answering multiple choice questions or simple questions with a straightforward answer, we have found this following templates to be useful. 
Model-Graded Templates: These contain functions where an LLM compares the output to the ideal_answers and attempts to judge the accuracy. In cases where the desired model response can contain significant variation, such as answering an open-ended question, we have found that using the model to grade itself is a viable strategy for automated evaluation.


The test dataset in the jsonl format.

Model Name: The model you want to evaluate (e.g., gpt-4, gpt-3.5-turbo, or a custom model via API).
Eval Name: A benchmark/test name defined in a YAML file.
YAML Files: Located in evals/registry/evals, they define input datasets, metrics, and settings.
Eval Implementations: Python code in evals/elsuite/ contains the logic for how evaluations are run (like multiple-choice grading, QA grading, etc.).

Two CLI Tools
oaieval ‚Äî runs a single evaluation.
oaievalset ‚Äî runs a batch of evaluations defined in a YAML file.

export OPENAI_API_KEY=your-api-key
evals set-api-key your-api-key

oaieval gpt-4 mathqa
The name of the eval (defined in evals/registry/evals/mathqa.yaml)

evals:
  - id: mathqa
    class: evals.elsuite.basic.match:Match  # Implementation
    args:
      samples_jsonl: evals/registry/data/mathqa_samples.jsonl

Creating Your Own Eval
oaievalset evals/registry/eval_sets/my_eval_set.yaml
Write your samples in .jsonl (e.g., question/answer format).
Add a YAML file under evals/registry/evals/.
Use a built-in class (like Match, CompletionFnEval) or implement a custom one in evals/elsuite/.
Run with oaieval.

Accuracy - Percentage of correct outputs (model prediction exactly matches the target/expected output).
Used in: Match, MCQA, BasicEval, MultipleChoice

Exact Match (EM)
Used in: QA-type tasks (e.g. SQuAD-style)
Definition: The model's response must exactly match the expected answer (after normalization like removing punctuation, case, etc.).

Pass@k (in coding tasks)
Used in: HumanEval or other code generation evals
Definition: Measures the probability that at least one of the top-k model generations passes all test cases.
For instance, pass@1 means the first generation was fully correct.

BLEU / ROUGE / F1 (less common in default suite)
Used in: Free-text generation tasks
Definition: These compare n-gram overlaps (ROUGE for recall, BLEU for precision, F1 for balance) between generated and reference texts.

5. Custom Metrics
You can define your own metrics inside custom evaluation classes.

Eval Class	Metric(s) Used
Match	Accuracy
MCQA / MultipleChoice	Accuracy
CodeEval / HumanEval	Pass@1, Pass@k
QA	Exact Match, optionally F1
Critique	Match, or custom judgment
Custom Class	Anything you define

Pass@K
It's widely used in code generation benchmarks (like HumanEval) because:
Pass@k ‚âà (# of problems where any of the top k solutions passed all tests) / (total problems)
If only one solution is correct, and you sample without replacement, OpenAI uses a statistical estimation for pass@k:

Pass@k
=
1
‚àí
(
ùëõ
‚àí
ùëê
ùëò
)
(
ùëõ
ùëò
)
Pass@k=1‚àí 
( 
k
n
‚Äã
 )
( 
k
n‚àíc
‚Äã
 )
‚Äã
n = number of generations

c = number of correct generations

k = how many generations you allow (top-k)

Use Case	Measures how well models can solve tasks with multiple tries


To use the Pass@k metric, your dataset must be structured for code generation tasks ‚Äî meaning each sample should include
A code prompt (like a docstring or problem description).
A set of unit tests or an expected function signature.
(Optional) A reference solution for comparison.

This format allows the model to generate code, which can then be executed and tested for correctness.


{
  "task_id": "reverse_string",
  "prompt": "\"\"\"\nWrite a function to reverse a string.\n\"\"\"\ndef reverse_string(s):",
  "tests": [
    {"input": "\"hello\"", "output": "\"olleh\""},
    {"input": "\"OpenAI\"", "output": "\"IAnepO\""}
  ]
}

for more advanced evals (like HumanEval), the prompt itself includes the test code.

Example from HumanEval (used by OpenAI)
Each item contains a docstring prompt, and the evaluation framework runs hidden test cases against the generated code.
{
  "task_id": "HumanEval/0",
  "prompt": "def add(a, b):\n    \"\"\"Return the sum of a and b.\"\"\"",
  "entry_point": "add",
  "test": "assert add(2, 3) == 5"
}

The evaluation script:
Appends the model's code completion to the prompt.
Runs the resulting function.
Executes hidden or public test cases.
Checks whether the generated code passes all tests.

üß† Key Fields for Pass@k Dataset
Field	Description
prompt	The code generation prompt (often includes docstring + function signature).
entry_point	The function name to test (e.g., reverse_string).
test	Code that tests the function (e.g., assert reverse_string("abc") == "cba").
task_id	(Optional) Unique identifier for the task.

What the Evals Framework Does
For Pass@k metrics, the framework:
Generates k completions from the model for each prompt.
Appends each completion to the prompt to form a complete function.
Executes the function in a sandboxed environment (e.g., Docker or subprocess).
Runs test cases against the function.
Checks if any of the completions pass all tests.
Aggregates how many tasks had at least one successful generation.

{"task_id": "sum_two", "prompt": "def sum_two(a, b):\n    \"\"\"Return sum of a and b\"\"\"", "entry_point": "sum_two", "test": "assert sum_two(3, 4) == 7"}
{"task_id": "is_even", "prompt": "def is_even(n):\n    \"\"\"Return True if n is even.\"\"\"", "entry_point": "is_even", "test": "assert is_even(2) == True"}

evals:
  - id: my_code_eval
    class: evals.elsuite.modelgraded.code_eval:CodeEval
    args:
      samples_jsonl: evals/registry/data/my_code_tasks.jsonl


Create a small working code-gen dataset
Write a YAML config
Run it with oaieval
See Pass@k results


-------------
You don‚Äôt manually add the model-generated code output. The evals system automatically generates code from the model based on your prompt and evaluates it.
Full Flow of Code Evaluation (e.g., Pass@k)

Step 1: You provide the dataset
{
  "task_id": "reverse_string",
  "prompt": "def reverse_string(s):\n    \"\"\"Reverses the input string.\"\"\"",
  "entry_point": "reverse_string",
  "test": "assert reverse_string('hello') == 'olleh'"
}

Step 2: You run the command
oaieval gpt-4 my_code_eval

evals:
  - id: my_code_eval
    class: evals.elsuite.modelgraded.code_eval:CodeEval
    args:
      samples_jsonl: evals/registry/data/my_code_eval.jsonl

Step 3: evals asks the model to complete the prompt
The framework automatically:
Sends the prompt (e.g., def reverse_string(s): """Reverses the input string.""") to the model.
Gets back a code completion, such as:

   return s[::-1]

This is not saved manually by you ‚Äî the framework handles this during the run.


def reverse_string(s):
    """Reverses the input string."""
    return s[::-1]


Step 4: The generated code is tested
The system combines the model‚Äôs output with your test (from the dataset), like:
def reverse_string(s):
    """Reverses the input string."""
    return s[::-1]

assert reverse_string('hello') == 'olleh'


This is executed in a safe environment.
If the code passes all tests, it's a pass.
For Pass@k, this is repeated k times ‚Äî if any one generation passes, it counts.


The model-generated code is created at runtime by oaieval.
It's not added to the dataset file.
The results (pass/fail for each task, and optionally the generated code) are saved in the evals/runs/ directory after you run the eval.


Want to Save the Model Outputs?
If you want to see or analyze the generated code completions:
evals/runs/{your_run_id}/samples.jsonl
Each entry will include the prompt, completion, and test results.

MMLU: https://github.com/hendrycks/test
BBH (Big-Bench Hard): https://github.com/suzgunmirac/BIG-Bench-Hard
ARC: https://github.com/allenai/ai2-arc
GSM8K: https://github.com/openai/grade-school-math
HellaSwag: https://github.com/rowanz/hellaswag

Benchmark	Task Type	Primary Metric(s)
MMLU	Multiple-choice QA	Accuracy
HellaSwag	Commonsense reasoning	Accuracy
ARC	MC science questions	Accuracy (Challenge set is more difficult)
GSM8K	Grade-school math	Accuracy (exact match of final answer)
BBH	Diverse hard tasks	Mostly Accuracy, sometimes F1 or BLEU (task-dependent)

Some newer evaluations (e.g., for BBH) use chain-of-thought prompting, so you might also see "CoT@1" or "Majority@k" metrics.
For GSM8K, programs might be evaluated for correct numerical answer, not just textual output.
Evaluation Tools
To evaluate models on these datasets:
Use the Hugging Face evaluate library
Or tools like lm-evaluation-harness by EleutherAI, which supports:

python main.py --model hf --model_args pretrained=gpt2 --tasks arc,hellaswag,mmlu --device cuda



RAG Evaluation 
Evaluating Generation - Groundedness/Faithfulness
Evaluating Hallucination - 
Attribution/Grounding Tests - 

Answer Relevance - Correctly answered for Query 
Context Relevance  - Retreived Correct Context for the Query 
Groundedness  - Context Retrieved and Answer Generated are faithful or not, or hallunicated 

Pipeline for RAG - Evaluation 
  {"query":
  "retrieved_docs":
  "generated_answer":
  gold_answer":
}

ROUGE/BLEU  - Token overlap (for summaries/QA) 


Metric	What It Measures	Score Range	Type
Faithfulness	How factually accurate the generated answer is w.r.t the retrieved context	0 to 1	NLI-based / QA - Uses an NLI model (e.g., DeBERTa or a fine-tuned LLM) to check entailment
Answer Relevance	How relevant the generated answer is to the original question	0 to 1	Semantic Similarity - Uses sentence embeddings to measure semantic similarity (e.g., SBERT or OpenAI)
Context Precision	How much of the retrieved context is actually useful or relevant to the answer	0 to 1	Information Precision - Uses embeddings + overlap analysis with generated answer
Context Recall	How much of the answer is supported by the retrieved context	0 to 1	Information Recall Measures how much of the answer is grounded in the retrieved context
Context Relevance	How relevant the retrieved documents are to the question	0 to 1	Semantic Similarity  Also embedding-based similarity between context and question
Answer Correctness (optional)	How correct the answer is (if a reference or gold answer is provided)	0 to 1	QA metric (e.g., EM/F1) - If enabled, uses QA metrics like Exact Match (EM) or F1 ‚Äî optionally via LLM

Type of Model	Used For
LLMs	Answer evaluation, faithfulness, correctness
NLI models	Checking entailment (faithfulness)
Embedding models	Semantic similarity (context & answer relevance)
Rule-based logic	Combining scores and thresholds

NLP Metric Definition 
-----------------------------
F1 Score (in NLP, usually token-based)
Used in QA, NER, and classification.
Precision = \frac{\text{# of overlapping tokens}}{\text{# of tokens in predicted answer}}
Recall = \frac{\text{# of overlapping tokens}}{\text{# of tokens in reference answer}}
ROUGE (Recall-Oriented Understudy for Gisting Evaluation)
Measures n-gram overlap between reference and generated text. Focuses more on recall.
Used for machine translation, measures precision of n-grams up to n=4.

Metric	Focus	Formula Type	Notes
F1	Balance of P/R	Harmonic mean of Precision and Recall	Common in QA, NER
ROUGE	Recall	n-gram overlap, LCS	Good for summarization, QA
BLEU	Precision	Geometric mean of n-gram precisions √ó BP	Common in translation








