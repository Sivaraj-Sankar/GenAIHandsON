1) How to Evaluate with OpenAIEval 
2) How to Evalute the Code by HumanEval 
3) Pass@k
4) Exact Match 
5) BLEU - Summarization, Translation
6) ROUGE - Recall - For QA, 
7) F1 - Recall - overlap tokens/token in grounded answer 
        Precision - overalap tokens/token in generated answer 
8) EM/F1 - Answer Correctness - Groundedness 
9) Top Benchmark - hellaswag - common sense reasoning , Grade School Math, BB, ARC - Science Question , MMLU - Multi Choice Answer 
10) Common NLP Metrics 
11) RAGAS Evaluation - metrics , models used 


https://cookbook.openai.com/examples/evaluation/getting_started_with_openai_evals
An eval is a task used to measure the quality of the output of an LLM or LLM system.
Given an input prompt, an output is generated. We evaluate this output with a set of ideal answers and find the quality of the LLM system.

How different model versions and prompts might affect your use case.
Developing a suite of evals customized to your objectives will help you quickly and effectively understand how new models may perform for your use cases.
You can also make evals a part of your CI/CD pipeline to make sure you achieve the desired accuracy before deploying.

Matching Eval 
JSON Eval 
Model Grading - LLM as a Judge 
     Model grading works best with the latest, most powerful models like GPT-4 and if we give them the ability to reason before making a judgment.
     Model grading will have an error rate, so it is important to validate the performance with human evaluation before running the evals at scale.

Human evaluation before running the evals at scale  - ?

OpenAI Evals
we have discovered several "templates" that accommodate many different benchmarks.
Basic Eval Templates: These contain deterministic functions to compare the output to the ideal_answers. In cases where the desired model response has very little variation, such as answering multiple choice questions or simple questions with a straightforward answer, we have found this following templates to be useful. 
Model-Graded Templates: These contain functions where an LLM compares the output to the ideal_answers and attempts to judge the accuracy. In cases where the desired model response can contain significant variation, such as answering an open-ended question, we have found that using the model to grade itself is a viable strategy for automated evaluation.


The test dataset in the jsonl format.

Model Name: The model you want to evaluate (e.g., gpt-4, gpt-3.5-turbo, or a custom model via API).
Eval Name: A benchmark/test name defined in a YAML file.
YAML Files: Located in evals/registry/evals, they define input datasets, metrics, and settings.
Eval Implementations: Python code in evals/elsuite/ contains the logic for how evaluations are run (like multiple-choice grading, QA grading, etc.).

Two CLI Tools
oaieval ‚Äî runs a single evaluation.
oaievalset ‚Äî runs a batch of evaluations defined in a YAML file.

export OPENAI_API_KEY=your-api-key
evals set-api-key your-api-key

oaieval gpt-4 mathqa
The name of the eval (defined in evals/registry/evals/mathqa.yaml)

evals:
  - id: mathqa
    class: evals.elsuite.basic.match:Match  # Implementation
    args:
      samples_jsonl: evals/registry/data/mathqa_samples.jsonl

Creating Your Own Eval
oaievalset evals/registry/eval_sets/my_eval_set.yaml
Write your samples in .jsonl (e.g., question/answer format).
Add a YAML file under evals/registry/evals/.
Use a built-in class (like Match, CompletionFnEval) or implement a custom one in evals/elsuite/.
Run with oaieval.

Accuracy - Percentage of correct outputs (model prediction exactly matches the target/expected output).
Used in: Match, MCQA, BasicEval, MultipleChoice

Exact Match (EM)
Used in: QA-type tasks (e.g. SQuAD-style)
Definition: The model's response must exactly match the expected answer (after normalization like removing punctuation, case, etc.).

Pass@k (in coding tasks)
Used in: HumanEval or other code generation evals
Definition: Measures the probability that at least one of the top-k model generations passes all test cases.
For instance, pass@1 means the first generation was fully correct.

BLEU / ROUGE / F1 (less common in default suite)
Used in: Free-text generation tasks
Definition: These compare n-gram overlaps (ROUGE for recall, BLEU for precision, F1 for balance) between generated and reference texts.

5. Custom Metrics
You can define your own metrics inside custom evaluation classes.

Eval Class	Metric(s) Used
Match	Accuracy
MCQA / MultipleChoice	Accuracy
CodeEval / HumanEval	Pass@1, Pass@k
QA	Exact Match, optionally F1
Critique	Match, or custom judgment
Custom Class	Anything you define

Pass@K
It's widely used in code generation benchmarks (like HumanEval) because:
Pass@k ‚âà (# of problems where any of the top k solutions passed all tests) / (total problems)
If only one solution is correct, and you sample without replacement, OpenAI uses a statistical estimation for pass@k:

Pass@k
=
1
‚àí
(
ùëõ
‚àí
ùëê
ùëò
)
(
ùëõ
ùëò
)
Pass@k=1‚àí 
( 
k
n
‚Äã
 )
( 
k
n‚àíc
‚Äã
 )
‚Äã
n = number of generations

c = number of correct generations

k = how many generations you allow (top-k)

Use Case	Measures how well models can solve tasks with multiple tries


To use the Pass@k metric, your dataset must be structured for code generation tasks ‚Äî meaning each sample should include
A code prompt (like a docstring or problem description).
A set of unit tests or an expected function signature.
(Optional) A reference solution for comparison.

This format allows the model to generate code, which can then be executed and tested for correctness.


{
  "task_id": "reverse_string",
  "prompt": "\"\"\"\nWrite a function to reverse a string.\n\"\"\"\ndef reverse_string(s):",
  "tests": [
    {"input": "\"hello\"", "output": "\"olleh\""},
    {"input": "\"OpenAI\"", "output": "\"IAnepO\""}
  ]
}

for more advanced evals (like HumanEval), the prompt itself includes the test code.

Example from HumanEval (used by OpenAI)
Each item contains a docstring prompt, and the evaluation framework runs hidden test cases against the generated code.
{
  "task_id": "HumanEval/0",
  "prompt": "def add(a, b):\n    \"\"\"Return the sum of a and b.\"\"\"",
  "entry_point": "add",
  "test": "assert add(2, 3) == 5"
}

The evaluation script:
Appends the model's code completion to the prompt.
Runs the resulting function.
Executes hidden or public test cases.
Checks whether the generated code passes all tests.

üß† Key Fields for Pass@k Dataset
Field	Description
prompt	The code generation prompt (often includes docstring + function signature).
entry_point	The function name to test (e.g., reverse_string).
test	Code that tests the function (e.g., assert reverse_string("abc") == "cba").
task_id	(Optional) Unique identifier for the task.

What the Evals Framework Does
For Pass@k metrics, the framework:
Generates k completions from the model for each prompt.
Appends each completion to the prompt to form a complete function.
Executes the function in a sandboxed environment (e.g., Docker or subprocess).
Runs test cases against the function.
Checks if any of the completions pass all tests.
Aggregates how many tasks had at least one successful generation.

{"task_id": "sum_two", "prompt": "def sum_two(a, b):\n    \"\"\"Return sum of a and b\"\"\"", "entry_point": "sum_two", "test": "assert sum_two(3, 4) == 7"}
{"task_id": "is_even", "prompt": "def is_even(n):\n    \"\"\"Return True if n is even.\"\"\"", "entry_point": "is_even", "test": "assert is_even(2) == True"}

evals:
  - id: my_code_eval
    class: evals.elsuite.modelgraded.code_eval:CodeEval
    args:
      samples_jsonl: evals/registry/data/my_code_tasks.jsonl


Create a small working code-gen dataset
Write a YAML config
Run it with oaieval
See Pass@k results


-------------
You don‚Äôt manually add the model-generated code output. The evals system automatically generates code from the model based on your prompt and evaluates it.
Full Flow of Code Evaluation (e.g., Pass@k)

Step 1: You provide the dataset
{
  "task_id": "reverse_string",
  "prompt": "def reverse_string(s):\n    \"\"\"Reverses the input string.\"\"\"",
  "entry_point": "reverse_string",
  "test": "assert reverse_string('hello') == 'olleh'"
}

Step 2: You run the command
oaieval gpt-4 my_code_eval

evals:
  - id: my_code_eval
    class: evals.elsuite.modelgraded.code_eval:CodeEval
    args:
      samples_jsonl: evals/registry/data/my_code_eval.jsonl

Step 3: evals asks the model to complete the prompt
The framework automatically:
Sends the prompt (e.g., def reverse_string(s): """Reverses the input string.""") to the model.
Gets back a code completion, such as:

   return s[::-1]

This is not saved manually by you ‚Äî the framework handles this during the run.


def reverse_string(s):
    """Reverses the input string."""
    return s[::-1]


Step 4: The generated code is tested
The system combines the model‚Äôs output with your test (from the dataset), like:
def reverse_string(s):
    """Reverses the input string."""
    return s[::-1]

assert reverse_string('hello') == 'olleh'


This is executed in a safe environment.
If the code passes all tests, it's a pass.
For Pass@k, this is repeated k times ‚Äî if any one generation passes, it counts.


The model-generated code is created at runtime by oaieval.
It's not added to the dataset file.
The results (pass/fail for each task, and optionally the generated code) are saved in the evals/runs/ directory after you run the eval.


Want to Save the Model Outputs?
If you want to see or analyze the generated code completions:
evals/runs/{your_run_id}/samples.jsonl
Each entry will include the prompt, completion, and test results.

MMLU: https://github.com/hendrycks/test
BBH (Big-Bench Hard): https://github.com/suzgunmirac/BIG-Bench-Hard
ARC: https://github.com/allenai/ai2-arc
GSM8K: https://github.com/openai/grade-school-math
HellaSwag: https://github.com/rowanz/hellaswag

Benchmark	Task Type	Primary Metric(s)
MMLU	Multiple-choice QA	Accuracy
HellaSwag	Commonsense reasoning	Accuracy
ARC	MC science questions	Accuracy (Challenge set is more difficult)
GSM8K	Grade-school math	Accuracy (exact match of final answer)
BBH	Diverse hard tasks	Mostly Accuracy, sometimes F1 or BLEU (task-dependent)

Some newer evaluations (e.g., for BBH) use chain-of-thought prompting, so you might also see "CoT@1" or "Majority@k" metrics.
For GSM8K, programs might be evaluated for correct numerical answer, not just textual output.
Evaluation Tools
To evaluate models on these datasets:
Use the Hugging Face evaluate library
Or tools like lm-evaluation-harness by EleutherAI, which supports:

python main.py --model hf --model_args pretrained=gpt2 --tasks arc,hellaswag,mmlu --device cuda



RAG Evaluation 
Evaluating Generation - Groundedness/Faithfulness
Evaluating Hallucination - 
Attribution/Grounding Tests - 

Answer Relevance - Correctly answered for Query 
Context Relevance  - Retreived Correct Context for the Query 
Groundedness  - Context Retrieved and Answer Generated are faithful or not, or hallunicated 

Pipeline for RAG - Evaluation 
  {"query":
  "retrieved_docs":
  "generated_answer":
  gold_answer":
}

ROUGE/BLEU  - Token overlap (for summaries/QA) 


Metric	What It Measures	Score Range	Type
Faithfulness	How factually accurate the generated answer is w.r.t the retrieved context	0 to 1	NLI-based / QA - Uses an NLI model (e.g., DeBERTa or a fine-tuned LLM) to check entailment
Answer Relevance	How relevant the generated answer is to the original question	0 to 1	Semantic Similarity - Uses sentence embeddings to measure semantic similarity (e.g., SBERT or OpenAI)
Context Precision	How much of the retrieved context is actually useful or relevant to the answer	0 to 1	Information Precision - Uses embeddings + overlap analysis with generated answer
Context Recall	How much of the answer is supported by the retrieved context	0 to 1	Information Recall Measures how much of the answer is grounded in the retrieved context
Context Relevance	How relevant the retrieved documents are to the question	0 to 1	Semantic Similarity  Also embedding-based similarity between context and question
Answer Correctness (optional)	How correct the answer is (if a reference or gold answer is provided)	0 to 1	QA metric (e.g., EM/F1) - If enabled, uses QA metrics like Exact Match (EM) or F1 ‚Äî optionally via LLM

Type of Model	Used For
LLMs	Answer evaluation, faithfulness, correctness
NLI models	Checking entailment (faithfulness)
Embedding models	Semantic similarity (context & answer relevance)
Rule-based logic	Combining scores and thresholds

What is HumanEval?
HumanEval is:
A set of 164 hand-written Python programming problems
Each problem includes:
A function signature
A docstring describing the task
A set of unit tests to check correctness
The idea is to prompt the model to generate the body of the function, and then automatically test its output.


NLP Metric Definition 
-----------------------------
F1 Score (in NLP, usually token-based)
Used in QA, NER, and classification.
Precision = \frac{\text{# of overlapping tokens}}{\text{# of tokens in predicted answer}}
Recall = \frac{\text{# of overlapping tokens}}{\text{# of tokens in reference answer}}
ROUGE (Recall-Oriented Understudy for Gisting Evaluation)
Measures n-gram overlap between reference and generated text. Focuses more on recall.
Used for machine translation, measures precision of n-grams up to n=4.

Metric	Focus	Formula Type	Notes
F1	Balance of P/R	Harmonic mean of Precision and Recall	Common in QA, NER
ROUGE	Recall	n-gram overlap, LCS	Good for summarization, QA
BLEU	Precision	Geometric mean of n-gram precisions √ó BP	Common in translation


AI Agents Metrics
-----------------
Agentic AI's Web Search and Reasoning capabilities involves a multi-dimensional approach. 
- Compile a dataset of queries that cover
  Typical Searches 
  Edge Cases - vague, ambiguous or trick questions
  Multi-Step Tasks - requiring search + synthesis 

Agent Capabilities Evaluation - Planing and Multi-Step Reasoning  - AQUA-RAT
                                Function Calling - Tool Use 
                                Self-Reflection 
                                Memory 
                          
Application-Specific Agent Evaluation - 
Generalist Agents Evaluation 
Frameworks for Agent Evaluation 

--------------------------------------------------------------------------------------------------------
** Evaluating Agentic AI is different from evaluating a static model. Why?
Static LLMs	Agentic AI Systems
One-off input/output	Multi-step behavior over time
Measured via accuracy	Measured via task success, safety, coherence, etc.
Often benchmarked with QA sets	Often benchmarked with simulations, environments, or real-world tasks

Task Completion (Success Rate)
Does the agent achieve the end goal?
Used in benchmarks like:
WebArena (agents navigating websites)
ALFWorld (in virtual homes)
Toolformer (tool usage)
‚úÖ Metric: Success Rate, Exact Match, Goal Achievement


Planning & Reasoning Quality
Can the agent plan logically and break down the task?
Metrics:
Coherence of plans
Accuracy of intermediate reasoning steps
Validity of tool calls

Tools like ReAct, AutoGPT log intermediate thoughts and actions for analysis.


Safety and Alignment
Does the agent act safely and within constraints?

‚úÖ Metrics:
Adherence to rules
Avoiding unsafe actions
Human-in-the-loop assessment
E.g., red-teaming with adversarial prompts

Tool Use Effectiveness
Can the agent use APIs, calculators, or memory correctly?
‚úÖ Metrics:
Accuracy of tool call arguments
Efficiency: # of tool calls to goal
Tool misuse rate

Interactivity and Adaptability
Can the agent adapt when goals or environments change?
‚úÖ Metrics:
Response to user clarification
Adaptation in unfamiliar situations
Robustness to ambiguous input


Evaluation Frameworks for Agents
Some emerging tools and frameworks:

Framework	Description
LangChain Benchmarks	For LLM-based agents using tools
OpenAI Evals	Can test multi-turn or tool-using agents
WebArena	Browsing-based environment for evaluating agents
AutoEval (AutoGPT)	Test workflows with custom goals
LEGOEval	Structured, stepwise benchmarks for agent plans


Summary: Key Evaluation Dimensions
Category	Metrics / Methods
Task success	Success rate, accuracy, completion
Reasoning	Step correctness, plan coherence
Tool use	Tool call accuracy, efficiency, success/fail
Safety	Rule violations, harmful outputs
Adaptability	Robustness to ambiguity or new goals
Efficiency	Steps to completion, unnecessary actions


How to Test 

Define the Agent Domain and Capabilities 
What kind of agent are you testing?
E.g., Task automation? Web navigation? Code writing? Tool use?
What tools can it use?
E.g., calculator, database, web browser, APIs
Example: Agent that answers questions by searching documents and using a calculator.

Create Testset Schema
{
  "test_id": "task-001",
  "description": "Answer a math question using calculator tool.",
  "input": "What is 125.4 * (2.3 + 0.6)?",
  "expected_behavior": [
    "Use calculator tool",
    "Compute correct expression",
    "Return 125.4 * 2.9 = 363.66"
  ],
  "expected_output": "363.66",
  "tools_available": ["calculator"],
  "constraints": ["must use tool", "no hardcoded answer"]
}


Define Evaluation Criteria per Test
Each test should define:

Criterion	Description
Task Success	Did the agent complete the goal?
Correctness	Is the final output correct?
Steps Taken	Were the reasoning/tool steps valid?
Tool Use	Were tools used correctly and efficiently?
Safety	Did the agent follow constraints/rules?
Efficiency	Was the task done with minimal steps?


Automate Evaluation (Optional)
Use code or frameworks to auto-grade:
Correctness via string match or custom logic
Tool usage via agent logs
Safety via rule checks
Step-by-step trace comparison (if agent logs intermediate thoughts)
You can implement in Python, or use frameworks like:
üîß LangChain Benchmarks
üß™ OpenAI Evals
üß† AgentBench
üï∏Ô∏è WebArena (for web-based tasks)





Example Test Case for Tool-Using Agent (Simple Calculator)
{
  "test_id": "calc-002",
  "description": "Agent should use the calculator to compute a compound expression.",
  "input": "What is (5 + 10) * (4 / 2)?",
  "expected_output": "30.0",
  "tools_available": ["calculator"],
  "evaluation_criteria": {
    "used_tool": true,
    "correct_answer": true,
    "steps_logged": ["call calculator with 5 + 10", "call calculator with 4 / 2", "multiply results"]
  }
}


Checklist for Building a Good Agent Test Set
Checklist Item	Description
üìå Goal defined	Clear and specific outcome expected
‚öôÔ∏è Tools specified	What tools are available and expected to be used
üß™ Evaluation plan	Criteria to mark pass/fail
üóÇÔ∏è Coverage	Tasks vary in complexity, domain, tool usage
üîÑ Repeatability	Test runs can be logged and re-evaluated consistently
üßº No data leakage	Tasks not copied from training data if testing generalization

---------------------------------
Standardized Leaderboards & Real‚ÄëWorld Frameworks
HAL (Holistic Agent Leaderboard)
An open platform from Princeton that offers standardized, cost‚Äëaware, and third‚Äëparty evaluations across 13 benchmarks and 122 agents. It allows reproducible agent testing with failure analysis and detailed logging.
https://hal.cs.princeton.edu/?utm_source=chatgpt.com
Agent Leaderboard (Galileo / Hugging Face)
Evaluates agents across multi-domain tasks (tool use, reasoning, web, game, embodied). Highlights top-performing models like Gemini‚Äë2.0‚Äëflash (0.938 overall) and GPT‚Äë4o (0.900), with detailed breakdowns on tool calling, irrelevance detection, and parallel execution. 
https://huggingface.co/blog/pratikbhavsar/agent-leaderboard?utm_source=chatgpt.com

Specialized & Domain‚ÄëDriven Benchmarks
MAPS (Multilingual Agent Performance & Security)
A recent benchmark suite evaluating agents across translated tasks in 10 languages (8,855 total instances), based on GAIA, SWE‚Äëbench, MATH, and Security benchmarks. Shows consistent performance and safety degradation in non‚ÄëEnglish languages

MLR‚ÄëBench (Machine Learning Research Benchmark)
Contains 201 ML research tasks from recent ICLR, NeurIPS, and ICML workshops. Tests agents on generating ideas, experimentation, and writing, assessed via an automated judge. Agents struggle especially with invalid or fabricated results.

ML Research Benchmark (MLRB)
Focuses on competition-level machine learning challenges, including training efficiency and model compressions. Tested on agents like Claude‚Äë3.5 and GPT‚Äë4o; results show wide performance variation and limitations.


TimeSeriesGym
Designed for evaluating ML engineering agents on time-series data challenges: data handling, code generation, model submission, and evaluation via numeric and LLM metrics

ML‚ÄëDev‚ÄëBench
Open‚Äësource benchmark with 30 realistic end‚Äëto‚Äëend ML development tasks‚Äîincluding preprocessing, debugging, implementation tweaks, fine‚Äëtuning. Framework named Calipers used across agent evaluations (ReAct, OpenHands, AIDE), revealing modest success rates, especially in open‚Äëended or optimization tasks.


Emerging & Critical Domain Benchmarks
AgentBoard
Multi-domain benchmark covering embodied simulations, games, web navigation, and function/tool usage. Uses fine-grained progress-based evaluation rather than simple final scores
https://www.generativemodels.ai/blog/llm-agent-benchmarks/?utm_source=chatgpt.com
œÑ‚ÄëBench (Sierra)
Designed to simulate real-world retail/customer service tasks, measuring agent reliability across repeated runs (pass^k metrics). Highlights brittleness in even top agents like GPT‚Äë4o failing at long-horizon consistency tasks
https://sierra.ai/blog/benchmarking-ai-agents?utm_source=chatgpt.com
CollaborativeAgentBench (ColBench)
Developed by Meta/UC Berkeley using SWEET‚ÄëRL. Evaluates collaborative human‚Äëagent interaction in tasks like frontend design and backend programming. Agents must iteratively refine outputs with limited turns (~10). 
https://www.reddit.com/r/machinelearningnews/comments/1jhmui7/?utm_source=chatgpt.com
https://www.reddit.com/r/machinelearningnews/comments/1jhmui7/meta_ai_researchers_introduced_sweetrl_and/?utm_source=chatgpt.com
Benchmark Integrity & Reliability Concerns
A recent analysis argues that many existing benchmarks are flawed, finding that 7 out of 10 popular agent benchmarks suffer major validity issues (e.g. broken simulators, no gold labels, overoptimistic scoring). A proposed 43-item Benchmark Checklist aims to ensure rigorous task validity, outcome validity, and reporting transparency

Benchmark	Domain / Focus	Highlights
HAL	Broad agent evaluation framework	Standardized, reproducible, cost-aware
Agent Leaderboard	Multi‚Äëdomain tool & reasoning performance	High-ranking: Gemini‚Äë2.0, GPT‚Äë4o
MAPS	Multilingual performance & security	Performance drops outside English
https://arxiv.org/abs/2505.15935?utm_source=chatgpt.com
MLR‚ÄëBench	Open-ended ML research tasks	Strong reasoning; weak experiments
https://arxiv.org/abs/2505.19955?utm_source=chatgpt.com
MLRB (ML Research Benchmark)	ML engineering competition tasks	Mixed performance; model reliability issues
https://arxiv.org/abs/2410.22553?utm_source=chatgpt.com
TimeSeriesGym	Time-series engineering tasks	Scalable ML engineering evaluation
https://arxiv.org/abs/2505.13291?utm_source=chatgpt.com
ML‚ÄëDev‚ÄëBench	Real-world ML workflows	Low success rates in practical tasks
https://www.reddit.com/r/AI_Agents/comments/1itv4qf/?utm_source=chatgpt.com
AgentBoard	Embodied / web / game / tool use	Fine-grained progress tracking
œÑ‚ÄëBench	Customer service & retail simulation	Highlights consistency failure
ColBench	Human‚Äëagent collaborative tasks	Iterative refinement under limited turns










