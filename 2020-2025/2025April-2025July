Plain JavaScript 
async function sendMessage(content, documentId) {
  const response = await fetch("/chat/send", {
    method: "POST",
    headers: {
      "Content-Type": "application/json"
    },
    body: JSON.stringify({ content, documentId })
  });

  if (!response.ok) {
    console.error("Error sending message");
    return;
  }

  const reader = response.body.getReader();
  const decoder = new TextDecoder();
  let fullResponse = "";

  while (true) {
    const { value, done } = await reader.read();
    if (done) break;
    const chunk = decoder.decode(value, { stream: true });
    fullResponse += chunk;

    // üîÑ Optionally: update UI with chunk
    document.getElementById("response-box").textContent += chunk;
  }

  console.log("Final response:", fullResponse);
}


UseEffect in React
import { useState } from "react";

function ChatComponent({ documentId }) {
  const [responseText, setResponseText] = useState("");

  const sendMessage = async (content) => {
    const res = await fetch("/chat/send", {
      method: "POST",
      headers: {
        "Content-Type": "application/json"
      },
      body: JSON.stringify({ content, documentId })
    });

    if (!res.body) return;

    const reader = res.body.getReader();
    const decoder = new TextDecoder();

    let full = "";
    while (true) {
      const { done, value } = await reader.read();
      if (done) break;
      const chunk = decoder.decode(value, { stream: true });
      full += chunk;
      setResponseText((prev) => prev + chunk);
    }
  };

  return (
    <div>
      <button onClick={() => sendMessage("What is this document about?")}>
        Ask AI
      </button>
      <div>{responseText}</div>
    </div>
  );
}



1)stream JSON
2)Session Management: 
You use the same session for both user and AI messages, which is fine as long as the session is valid for the async generator's lifetime
3)If your frontend expects a JSON response, you should not use streaming, or you should stream JSON chunks.

Streaming by API request , from JSON 
import requests

headers = {
    "Authorization": f"Bearer YOUR_API_KEY",
    "Content-Type": "application/json",
}

json_data = {
    "model": "gpt-4",
    "messages": [{"role": "user", "content": "Hello!"}],
    "stream": True
}

response = requests.post(
    "https://api.openai.com/v1/chat/completions",
    headers=headers,
    json=json_data,
    stream=True
)

for line in response.iter_lines():
    if line:
        print(line.decode('utf-8'))

With Normal Client, with stream = True 
response = client.responses.create(
  model="o4-mini",
  input="This is a test",
  stream=True
)

for event in response:
    if event.type == 'response.output_text.delta':
       print(event.delta, end='')


What I have learned Today 
1) Document Object received and using for loop adding to response model in sqlmodel 
2) Encoded Embedding saving the cosmosdb container 
3) polling is the simplest way to check the status from the Frontend - if i use this can I go with request 
4) UI have to change according to the API Endpoint, so in Frontend have to handle accordingly 
5) Streaming Tips 
The endpoint streams the AI response as plain text. If you want to stream JSON, you can buffer and yield JSON objects at the end.
The AI message is saved to the database after the full response is streamed.
If you want to send both the user and AI message metadata at the end, you can yield a final JSON object after the stream.
If you want to stream a final JSON response at the end (with the full ChatSendResponse)
If you want the endpoint to stream in a specific format (e.g., Server-Sent Events, JSON lines, etc.)

5.1)You need an async generator that yields tokens/chunks as they are generated by the AI model:

6)SQLModel/SQLAlchemy isn‚Äôt always async-safe. Consider using AsyncSession if needed



If you're using OpenAI's Chat Completions API (/v1/chat/completions) with stream=True, you do not need the endpoint itself to be "async", but you must call it using an async HTTP client, like httpx.AsyncClient, for compatibility with FastAPI's async model.


async def generate_ai_response_stream(prompt: str, document_id: str) -> AsyncGenerator[Dict[str, Any], None]:
    # Call OpenAI's API with stream=True

OpenAI Streaming API Support
OpenAI supports streaming with stream=True. The response will be streamed chunk-by-chunk over an HTTP connection.

This is available for:

gpt-4, gpt-3.5-turbo with stream=True in the request body.

Response is an event stream (data: lines that end with \n\n).


Async-Compatible HTTP Client: use httpx.AsyncClient
If you're using FastAPI with async views and generators, your call to OpenAI must be done asynchronously, or it will block the event loop.

Here‚Äôs a correct example using httpx.AsyncClient:
7)import httpx
import asyncio
from typing import AsyncGenerator, Dict, Any

OPENAI_API_KEY = "your-api-key"

async def generate_ai_response_stream(prompt: str, document_id: str) -> AsyncGenerator[Dict[str, Any], None]:
    headers = {
        "Authorization": f"Bearer {OPENAI_API_KEY}",
        "Content-Type": "application/json"
    }

    payload = {
        "model": "gpt-4",
        "stream": True,
        "messages": [
            {"role": "user", "content": prompt}
        ]
    }

    async with httpx.AsyncClient(timeout=60.0) as client:
        async with client.stream("POST", "https://api.openai.com/v1/chat/completions", headers=headers, json=payload) as response:
            async for line in response.aiter_lines():
                if line.startswith("data: "):
                    data = line[len("data: "):]
                    if data.strip() == "[DONE]":
                        break
                    try:
                        delta = json.loads(data)["choices"][0]["delta"]
                        output_text = delta.get("content", "")
                        yield {"output_text": output_text}
                    except Exception as e:
                        # You could log or yield an error
                        continue

8)‚ùå Using requests (sync client) inside async generator ‚Äî will block the event loop!
9)‚ùå Not setting stream=True ‚Äî you won‚Äôt get partial responses.
10)Aspect	Requirement
OpenAI API supports streaming?	‚úÖ Yes, with stream=True
Client supports async?	‚úÖ Use httpx.AsyncClient, not requests
Server-side async safe?	‚úÖ Yes, because you're using FastAPI with async def
Use aiter_lines()?	‚úÖ Yes, to process each line of the streamed response
----------------
11) Alembic Issue 
alembic upgrade head ‚Äî What it Actually Does
It applies existing migrations to your database up to the latest one (i.e., the "head"
It does not create new migrations
It also does not affect the migration history or structure (it just applies what's already there)

Multiple heads are created only when you run this
alembic revision --autogenerate -m "Some change"
Initial revision
  ‚îú‚îÄ‚îÄ revision_a.py  ‚Üê head
  ‚îî‚îÄ‚îÄ revision_b.py  ‚Üê head

Solution Merge 
alembic merge -m "Merge revisions" <revision_a> <revision_b>
alembic heads      # shows current heads
alembic history    # shows full migration history


----------------
12) document: Optional["Document"] = Relationship(
    back_populates="report",
    sa_relationship_kwargs={"foreign_keys": "[Report.document_id]"}
)
report: Optional["Report"] = Relationship(
    back_populates="document",
    sa_relationship_kwargs={"foreign_keys": "[Document.report_id]"}
)






Routing Tips
------------
These are frontend routes, likely handled by a Single Page Application (SPA) using a framework like React, Vue, or Next.js
There's probably only one backend API route serving the entire /docs app
The page changes when you visit /docs/models or /docs/pricing, but it's handled entirely by the frontend router ‚Äî not hitting the server again for a full page reload
So the same SPA loads
<Route path="/docs/models" element={<ModelsPage />} />
<Route path="/docs/pricing" element={<PricingPage />} />
-------------------------
Your backend just serves the main HTML (e.g., index.html or Next.js SSR page
and then the frontend router takes over.
----------------------
/docs/models	User directly navigates to a specific section
/docs only	Could load all content and use tabs/anchors
--------------------
You can also do:
/docs#models
/docs#pricing
These don‚Äôt go to the server ‚Äî the browser jumps to that section of the page. Works well for static documentation.
-------------
SEO, shareable URLs	Use clean routes like /docs/models
Simple tab-based navigation, no reload	Use tabs or #anchors
Backend API per page (rare)	Only if SSR/SSG needed
---------------------
Clean, RESTful and intuitive
Easy to deep link to a specific application report
Lets you fetch data based on the URL directly (no need to keep state elsewhere)
<Route path="/app/:application_id/report" element={<ReportPage />} />
const { application_id } = useParams();
// use application_id to call backend API
-------------------------
Option 2: Use a flat or centralized URL like /report
/report
And you pass the application_id via:
state (e.g. React Router's navigate(..., { state }))
query param (e.g. /report?application_id=xyz)
global state (like Redux or React Context)

Cleaner URL for simple apps
Useful if application_id is sensitive and you don't want it in the URL

But watch out:
You must store or pass the application_id manually to the page (via state, query, or context)
If the user refreshes the page, and application_id isn‚Äôt in the URL or persisted elsewhere, you‚Äôll lose it
---------------------
Use Case	Best Option
Multi-app system (each app has its own report)	‚úÖ /app/:id/report
Simple app with only one app context	‚úÖ /report (with state or global context)
You want shareable URLs	‚úÖ Include application_id in the URL
You want minimalistic route names	/report, but handle context carefully
-----------
if (!application_id) {
  navigate("/app-selection"); // or redirect to pick an application
}
If you use /report, consider adding fallback logic:



Front End Tips 
--------------
Save application_id in route or React state
Use React Query or Axios to handle GET/POST cleanly
Show loading state while generating
Disable ‚ÄúView Report‚Äù button until report exists

Backend API Tips 
----------------
All routes related to credit applications should be grouped under this base path.
RESTful vs RPC-style discussion.
Use Case	Method	Where to put application_id
RESTful / resource-focused	GET, POST	In path: /credit-applications/{id}/report
Action-focused or abstracted	POST	In body: { "application_id": "..." }
Simple fetch	GET	In query param: ?application_id=...

Learning the Project Structure 
------------------------------
Storing the application_id
The frontend stores this application_id in its state management system (such as React state, Redux, Vuex, etc.).
It may also store it in local storage or session storage if the user might refresh or navigate away and return.

User (1) ‚îÄ‚îÄ‚îÄ< Application >‚îÄ‚îÄ‚îÄ (1) Borrower
                          |
                          ‚îú‚îÄ‚îÄ‚îÄ< Loan (many)
                          ‚îú‚îÄ‚îÄ‚îÄ< Benchmark (many)
                          ‚îî‚îÄ‚îÄ‚îÄ< Document (many)

-------------------------------------

Message History 
---------------
How to Store the Chat History - Multiple Options, 
  1. Store Human & AI Messages Separately --> Separate Table 
       Cons - Requires joins or merging when reconstructing conversation history
              More complex to maintain conversational flow
  2. Store Combined per Session
       Cons - Hard to query individual messages
       Difficult to scale for long conversations 
       Not ideal for real-time or incremental updates
  3. Store Combined per Turn
       One table with each turn (a human input + AI response) as one record
       Pros
          Easier to audit, replay, or fine-tune models from turn-level data
          Easy to implement pagination and indexing
       Cons 
          Not ideal if you want full flexibility (e.g., multi-turn responses, interruptions)
          Less granular than storing individual messages

Recommendations
Use a single Messages table, storing each message as one row, with metadata:
session_id
message_id
sender (human/ai/system)
text
timestamp
turn_number (optional)
parent_message_id (optional, for threading)

When should you use parent_message_id?
Use it when:
Users ask multiple questions at once
You support branching conversations (like revisiting an earlier question)
You want to display the conversation in a threaded or nested UI
You're using the data to train LLMs and need clearer context for responses


When feeding context to an LLM:
You can reconstruct the thread to include relevant past messages only.
Avoids bloating the prompt with unrelated or outdated turns.


How to start from the session after resuming - , do we need load it in the Lang chain memory before starting the conversation 
How to Display the Messages to the UI 
How to Give the Context to the ChatBot - Constructing Initially, and Storing Vector DB and Constructing on Each Turn



Project Industrial Analysis Adding
1. Model for Document & Report - Done
+----------------+        1               *+----------------+
|  Application   |------------------------>|    Document    |
|----------------|                        |----------------|
| id             |                        | id             |
| application_id |                        | filename       |
| user_id        |                        | application_id |
+----------------+                        +----------------+
         |                                          |
         |                                          | 1
         |                                          | 
         |                                          v
         |                                +----------------+
         |                                |    Report      |
         |                                |----------------|
         |                                | id             |
         |                                | document_id    |
         |                                +----------------+
         |
         | 1                                      
         |------------------------------->+----------------+
                                         | ChatMessage     |
                                         |-----------------|
                                         | id              |
                                         | application_id  |
                                         | document_id     |
                                         +-----------------+




1) MongoDB for storing the metadata   - Skip  
2) Document Uploading - Done
3) Document View  - Done
4) Report View - Work on 

5) Background task for report generation - Done
6) ChatBot WebSocket  - Work On 
7) 


Steps to Follow 
1) Document Upload - MongoDB  - Model 
2) Report Generation - MongoDB - Model 
3) 

from transformers import AutoTokenizer, AutoModelForCausalLM, TextGenerationPipeline
tokenizer = AutoTokenizer.from_pretrained(...)
model = AutoModelForCausalLM.from_pretrained(...)
pipe = TextGenerationPipeline(model=model, tokenizer=tokenizer)

All that is wrapped up inside the pipeline(...) factory.

Why Use Factory Functions?
Simplify complex initialization:
If creating an object involves several steps, a factory function hides that complexity.
Return different types conditionally:
You can have logic inside that decides what kind of object to return based on input.
Abstract library internals:
Users don't need to know which class is behind the object ‚Äî they just use the returned result.

class Dog:
    def speak(self):
        return "Woof!"

class Cat:
    def speak(self):
        return "Meow!"


def animal_factory(animal_type: str):
    if animal_type == "dog":
        return Dog()
    elif animal_type == "cat":
        return Cat()
    else:
        raise ValueError("Unknown animal type")


# Usage:
animal = animal_factory("dog")
print(animal.speak())  # Output: Woof!

animal2 = animal_factory("cat")
print(animal2.speak())  # Output: Meow!


Today's Work 
1) FastAPI System Design 
2) Session and State Management 
3) Agentic AI - Anything Progress 
4) Programming Practice 
5) Memory Conversation U Tube and course 
6) UI learning 
7) Deploying the LLM Models and Inferencing 
8) 


Doubt 
-----
1) Is this will be correct - Using One Instance for all users by adding user id , and different chat session using different thread id 
2) Using different instance as per the user - FAST api automatically know, because once server shutdown for only one user , one user instance will close 
3) OpenAI responses memory Vs own Memory handle 

2 Doubts, 
1) If I storing the response id in the database, will that response id valid for long time to call again, --> because it will know the file id and context window memory, and the memory chat 
2) If it is valid for long time, how I can set up the response id to get it to resume again, after logout , login, loading the chat history 

memory/context persistence across follow-up turns
All context (questions, answers, file reference) is preserved automatically. No manual messages list needed.
Includes built-in tools like file search (for PDFs), web search, and others in a single endpoint.
The Responses API can be stateful‚Äîretain context across turns by linking previous_response_id.
curl https://api.openai.com/v1/responses \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4.1",
    "input": [
      {"type": "input_file", "file_id": "'"$FILE_ID"'"},
      {"type": "input_text", "text": "Give me an overview of the document."}
    ]
}'

# Follow-up
curl https://api.openai.com/v1/responses \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4.1",
    "previous_response_id": "resp_XXXXXXXX",
    "input": "What did it say about X topic?"
}'


Start memory chain	Use file_id + input_text in first call
Follow-up questions	Use previous_response_id only
Access long-term memory	Coming via Assistant API, not responses yet
Parallel sessions	Use separate chains (track each response.id)

You can do dozens or more turns, but responses are still subject to context window limits (128k tokens in GPT-4o).
Each previous_response_id forms a contextual chain ‚Äî like a memory thread.
You can cache each response to resume conversations later.



Caching OpenAI Responses API results for resuming conversations later is essential for building persistent, multi-session memory (like user history). Here's a breakdown of how to cache and resume chats using response_id.

‚úÖ Goal
Enable a user to return later and continue the conversation from where they left off, using the built-in memory of the Responses API.

üîß What You Need
A cache or database (e.g. Redis, PostgreSQL, SQLite, or even a flat JSON file)

Store the last_response_id per user/session

Use that response_id as previous_response_id in future API calls

üí° Basic Python Example Using JSON File Cache
1. Create a simple JSON file cache for sessions
python
Copy
Edit
import json
import os

CACHE_FILE = "session_cache.json"

# Load cache
def load_cache():
    if os.path.exists(CACHE_FILE):
        with open(CACHE_FILE, "r") as f:
            return json.load(f)
    return {}

# Save cache
def save_cache(cache):
    with open(CACHE_FILE, "w") as f:
        json.dump(cache, f)

# Update session's last response_id
def update_session(user_id, response_id):
    cache = load_cache()
    cache[user_id] = response_id
    save_cache(cache)

# Get last response_id
def get_last_response(user_id):
    return load_cache().get(user_id, None)
2. Use it in conversation
python
Copy
Edit
from openai import OpenAI
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

user_id = "user123"  # Could be a session token or user ID from auth system

# Initial chat (no prior memory)
last_response_id = get_last_response(user_id)

if last_response_id is None:
    # Upload file once if needed
    file = client.files.create(file=open("document.pdf", "rb"), purpose="user_data")

    response = client.responses.create(
        model="gpt-4.1",
        input=[
            {"role": "user", "content": [
                {"type": "input_file", "file_id": file.id},
                {"type": "input_text", "text": "Can you summarize the document?"}
            ]}
        ]
    )
else:
    # Continue from previous memory
    response = client.responses.create(
        model="gpt-4.1",
        previous_response_id=last_response_id,
        input="What are the key takeaways from section 2?"
    )

# Cache new response_id
update_session(user_id, response.id)

print(response.output_text)
üß† Summary
Action	Implementation
Store memory across sessions	Save response.id
Resume chat later	Pass previous_response_id
Per-user session persistence	Use user_id as cache key
Store in DB (prod)	SQLite, Redis, DynamoDB, Firestore
üõ† Production Suggestions
Redis: Fast in-memory cache with expiration
PostgreSQL: For long-term memory per user
Vector DBs: For combining memory with RAG (e.g. using file_search or embeddings)
Would you like me to provide a Redis or PostgreSQL-based caching setup for this?

------------------------------------------------------------------------------------------------------------------------------

Software Design Pattern 
-----------------------
1.onion/layered software design pattern
2.Being an ASGI framework means that it can leverage concurrency to process web requests
3.Advanced use cases like resource-intensive AI workloads may require specialized web frameworks or solutions
4.Serialization is required when using HTTP for data transmissions, as only text or binary data can be transferred via HTTP.
5.When building services, your service must be able to handle multiple requests by several users to increase usage efficiency as demand scales
6.If you define a route with async def, FastAPI will run it on the main event loop using asyncio, without creating a new thread. This is suitable for non-blocking, asynchronous operations like making HTTP requests or database calls using async drivers.
7.If you define a route with regular def (i.e., synchronous), FastAPI will run it in a separate thread from a thread pool using starlette.concurrency.run_in_threadpool. This is done so that blocking operations (like reading a file or using a synchronous database client) don't block the main event loop
Async: Uses the main thread's event loop (non-blocking, cooperative multitasking).
Sync: Uses a thread from the threadpool (blocking, preemptive multitasking).
8.You can build routes capable of handling long-running tasks (e.g., sending emails) without the need of external libraries (e.g., celery)
9.FastAPI includes a background tasks feature4 for working with systems that need time to process data but you don‚Äôt want them to delay returning the responses to requests
10.**You can hand off the long-running operation to a background task running on a separate thread, after you respond to the client.
11.**As a response, you can then let clients know that your service has accepted and queued their request to process it in the background.
12.**As an example, in GenAI services you can use background tasks to process large uploaded documents into a vector database without blocking the server
13. CQRS (Command Query Responsibility Segregation)
CQRS splits the system into two distinct parts:
Commands: Perform actions (Create/Update/Delete)
Changes system state
Might have validations, side effects, domain logic
Queries:
Retrieve data (Read)
Pure reads, no state change
Optimized for performance, caching, projections, etc

CQRS
Separate complex write logic (business rules) from read logic.
Can optimize query model for performance (e.g., denormalized views)
Pairs well with Event Sourcing and Domain-Driven Design (DDD).
14.CORS (Cross-Origin Resource Sharing) is a security feature implemented by browsers to control how web pages from one origin (domain) can make requests to another origin.
How CORS Works
Preflight Request (OPTIONS): For some requests, the browser sends a request asking
Server Response (CORS headers):
Access-Control-Allow-Origin
Access-Control-Allow-Methods
Access-Control-Allow-Headers
If the headers are correct, the browser allows the actual request
15.CSRF Protection - Prevent unwanted actions from authenticated browsers - Applies within the same origin
16.API Key / OAuth2 / JWT	Control who can access the API	Server	Authenticated access to APIs	CORS doesn‚Äôt replace this ‚Äî it just governs where requests can come from
17.Reverse Proxy (e.g., Nginx)	Route or restrict requests	Server	Internal API protection	Can bypass browser limits entirely
18.SameSite Cookies	Control when cookies are sent cross-site	Browser	Login sessions, third-party cookies	Works alongside CORS
19.CSRF (Cross-Site Request Forgery) is a type of web security vulnerability that tricks a logged-in user into unknowingly submitting a malicious request to a website where they're authenticated
20.A CSRF attack forces a user‚Äôs browser to perform actions on a site they‚Äôre logged into ‚Äî without their consent.
Because you're still logged in to bank.com, your browser includes the session cookie, and the bank thinks you initiated the transfer
Use CSRF Tokens (Server-Side Rendered apps)
Server sends a unique token in forms or headers.
Server checks this token before accepting a request.
The token is tied to the user session and can‚Äôt be guessed
Use SameSite Cookies - Set-Cookie: sessionid=abc123; SameSite=Strict
 This tells the browser not to send the cookie on cross-site requests
Blocks most CSRF attacks by default

21.Enforced By	Server (checks tokens, cookies)	Browser (blocks or allows requests)
22.Token-based authentication (like JWT)
No cookies or browser sessions
The attacker can't access your tokens (they‚Äôre stored in JS, not automatically sent by the browser like cookies).
You're using Authorization headers, not cookies

Cookie-based sessions (especially in browser-based apps)
Server-rendered pages with forms
23.Main Components of an API Request
Component	Description	Example
HTTP Method	What kind of operation the request wants to perform	GET, POST, PUT, DELETE, PATCH, etc.
URL / Endpoint	The specific path/resource being accessed	/api/items/123, /users, /login
Headers	Metadata about the request (e.g. auth, content type)	Authorization: Bearer <token>
Content-Type: application/json
Query Params	Extra data in the URL, often for filtering/searching	/items?limit=10&category=books
Path Params	Dynamic parts of the URL path	/items/{item_id} ‚Üí /items/123
Request Body	The actual data sent to the server (usually with POST/PUT)	JSON, form data, file uploads
Cookies	Key-value pairs stored in the browser, often for sessions	session_id=abc123
Authentication	Often sent via headers or cookies, for securing the request	Authorization: Bearer <token>

POST /items/?discount=true HTTP/1.1
Host: api.example.com
Authorization: Bearer <jwt-token>
Content-Type: application/json
Cookie: session_id=xyz456

{
  "name": "Laptop",
  "price": 1200,
  "description": "Gaming laptop"
}

Part	Example
Method	POST
Endpoint	/items/
Query Params	?discount=true
Headers	Authorization, Content-Type
Body	JSON object
Cookies	session_id=xyz456
Path parameters (e.g. /users/{user_id})
Query parameters (e.g. ?search=book)

from fastapi import FastAPI, Path, Query, Header, Cookie, Body
from pydantic import BaseModel
from typing import Optional

app = FastAPI()

class Item(BaseModel):
    name: str
    price: float
    description: Optional[str] = None

@app.post("/items/{item_id}")
async def create_item(
    item_id: int = Path(..., description="The ID of the item"),
    q: Optional[str] = Query(None, description="Search query"),
    item: Item = Body(...),
    token: Optional[str] = Header(None),
    session_id: Optional[str] = Cookie(None)
):
    return {
        "item_id": item_id,
        "query": q,
        "item": item.dict(),
        "token": token,
        "session_id": session_id
    }
24. Powerful component of FastAPI is its dependency injection system based on a development pattern called inversion of control
25. Specify common query parameters across API routes (e.g., for pagination and filtering)
26. FastAPI will also automatically expose parameters within dependencies on your endpoint
27. Inject these dependencies into other functions by passing them as parameters to Depends() for FastAPI to evaluate and cache your function outputs.
28. Here you define a utility function for creating a database session and then use it as a dependency of the get_current_user_messages function to inject the created database session
27. FastAPI‚Äôs lifespan events are excellent for handling initialization and cleanup of your service when you need to set up resources that can be shared between requests
28. During server startup, you can create database connection pools or load GenAI models into memory for reuse across requests. Afterward, before server shutdown, you can clean up by unloading AI models, closing connection pools, deleting temporary artifacts, and logging events
29. By using lifespan events, your FastAPI service performs long-running operations like model loading at the start, before serving requests, and keeps it loaded for reuse among requests. During server shutdown, you can then gracefully finish all remaining and queued requests before running any cleanup operations.
30. If you‚Äôre building a chat application, you may also need real-time client-server communication or longer-duration connections where data is streamed in a direction. WebSocket (WS) and server-sent events (SSE) endpoints can help you stream generative model outputs to the clients
31. cookie-cutter templates for starting FastAPI projects
32. Some even recommend following a structure popularized by the Netflix Dispatch FastAPI project for larger API applications that has inspired other templates.
33. There are a few project structures you can adopt: flat, nested, and modular.
34. Project Grows - global Python modules into packages of their own using the nested structure.
35. You can achieve modularity by designing components of your system with re-usability and disposability in mind
36. About a software design pattern that helps you manage the complexity of your AI services. This is called the onion, or layered, application design pattern, which we will talk about next.
37. Dependency Inversion Principle (DIP), one of the SOLID principles
         - In the context of FastAPI, the dependency injection system can be used to apply this principle cleanly. Let's break this down, then walk through an example
38. It refers to keeping an open, long-lived connection between the frontend (client/browser) and backend (server), where both sides can send data to each other at any time without reestablishing the connection.
Common Use Cases
Chat applications
Real-time dashboards
Live notifications
Collaborative editing (e.g., Google Docs)

Type	Client ‚û°Ô∏è Server	Server ‚û°Ô∏è Client	Connection
HTTP (REST)	‚úÖ	‚ùå (only on response)	Short-lived
Polling	‚úÖ	‚úÖ (simulated via repeated requests)	Repeated short-lived
WebSockets	‚úÖ	‚úÖ	Persistent & bidirectional
Server-Sent Events (SSE)	‚úÖ	‚úÖ (server ‚û°Ô∏è client only)	Persistent (1-way)

1. WebSockets (Most Common)
Full-duplex communication
Ideal for chat, games, live updates
Supported in FastAPI (via websockets)

from fastapi import FastAPI, WebSocket

app = FastAPI()

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    while True:
        data = await websocket.receive_text()
        await websocket.send_text(f"Echo: {data}")

const socket = new WebSocket("ws://localhost:8000/ws");

socket.onmessage = (event) => {
    console.log("Received:", event.data);
};

socket.onopen = () => {
    socket.send("Hello from frontend!");
};


2. Server-Sent Events (SSE)
One-way (server ‚û°Ô∏è client)
Simpler than WebSockets
Great for notifications or logs


3. Socket.IO
Abstraction on top of WebSockets
Adds fallbacks and extra features
Used in both Python (python-socketio) and JS

Term	Meaning
Persistent	The connection stays open over time
Bidirectional	Both frontend and backend can send/receive data
Maintaining	Keeping the connection alive across events or actions

If the WebSocket disconnects?
It must be reconnected manually or via retry logic
Some libraries (like Socket.IO) help with automatic reconnection

Frontend                         Backend
    |                                |
    | ----------- connect ---------->|
    |                                |  (connection stays open)
    |                                |
    | <------ message from server ---|
    | --- send message to server --->|
    |                                |
    |  (connection still alive...)   |


‚úÖ Real-World Example
A collaborative whiteboard app:
Users draw on canvas in real time.
Browser sends drawing events to backend via WebSocket.
Backend broadcasts updates to all other connected clients

Both sides constantly send and receive ‚Äî persistent + bidirectional.
39. This makes Django an excellent choice for monolith progressive web applications (PWA) that deploy as a single backend with a frontend
40. ASGI-based frameworks can process multiple requests by running concurrent asynchronous operations on the main event loop.
    allowing it to handle a higher volume of requests at scale.
41. It can also use a thread pool (i.e., a pool of thread workers) to perform synchronous tasks concurrently without blocking the main server thread
42. Once tasks are finished, these threads return control to the main web server thread and share their results
43.When a thread raises an error, the web server gathers information from the worker thread and sends an error response to the client instead.
44.Flask, relying on a WSGI server, will process each request synchronously, Vs
FastAPI uses an event loop for concurrent workloads.
45.Therefore, FastAPI is going to be much faster with input/output (I/O) heavy tasks‚Äîfor instance, when communicating with an external API or data store, which would block an entire worker process in Flask.
46.BentoML is also built on top of Starlette and is designed with FastAPI patterns in mind but specifically for machine learning. Its architecture allows for scaling web requests separately from model inference, providing flexibility in computing distributions.
47.The role of FastAPI lifecycle system in model serving
48. Finally, you will be introduced to the FastAPI background tasks system to offload long-running inference operations.


-----------------------------------------------------------------------------------------------------------------------
Database connection 
1. Client -> Request --> db connected -- used for one or two queries --> 
   Client -> second Request -> db connected - used for one or two queries 
MongoDB Database Index 
------------------------------------------------------------------------------------------------------------------------

Scalable FASTAPI
1) Scale efficiently using the microservice pattern. 
2) Contextual data from a variety of sources such as databases, the web, external systems, and files uploaded by users.
3) Explore how to handle long-running tasks such as model inference
4) Authentication concepts, security considerations, performance optimization, testing, and deployment of production-ready generative AI services.
5) Finally, to run models on a CUDA-enabled NVIDIA GPU, you will also need to install the torch package compiled for CUDA.
6) FastAPI Recent AI model-serving support via lifecycle events may be contributing to this.
7) Integrating the AI service with existing systems, such as internal databases, web interfaces, and external APIs, can pose a challenge. This integration can be difficult due to compatibility issues, the need for technical expertise, potential disruption of existing processes, malicious attempts on these systems, and similar concerns about data security and privacy
8) Data Privacy and Security issues‚Äîcan be solved with software engineering best practices
-------------------------------------------------------------------------------
Industry Overview 
Key Companies and Performance 
Emerging Trends 
Investment Potential - 
-------------------------------------------------------------------------------
Doubt
1.How the Frontend call the fastapi for chat for each request - object will create each time 
----------------------------------------------------------------------------------------------------

Today's Task 
1) Demo Ready 
2) Memory Management for Each Chat
3) Using Response API 
Clarify 
1) Memory Management 
2) Thread per chat per user /Session per user 
3) Editing the Portion of the Section 
---------------------------------------------------------------------------------------------

https://langchain-ai.github.io/langgraph/concepts/memory/
https://langchain-ai.github.io/langgraph/agents/evals/
https://python.langchain.com/docs/introduction/
https://news.microsoft.com/source/features/ai/workers-in-all-kinds-of-roles-and-industries-count-on-copilot-to-do-more-in-less-time/?OCID=lock2
https://medium.com/@amanatulla1606/llm-web-scraping-with-scrapegraphai-a-breakthrough-in-data-extraction-d6596b282b4d
https://tcscomprod-my.sharepoint.com/:p:/r/personal/2026133_tcs_com/_layouts/15/Doc.aspx?sourcedoc=%7B448A6947-D6B6-4922-9CD1-07FA60E32568%7D&file=CBD%20PoC_UI%20Screens.pptx&action=edit&mobileredirect=true
https://tcscomprod-my.sharepoint.com/:p:/r/personal/2026133_tcs_com/_layouts/15/Doc.aspx?sourcedoc=%7B748E2E36-A275-4BB9-9EE1-2DB500F156AB%7D&file=CBD%20PoC%20Demo.pptx&wdOrigin=TEAMS-MAGLEV.p2p_ns.rwc&action=edit&mobileredirect=true

https://modelcontextprotocol.io/llms-full.txt

SWOT & PESTEL 
------------------------------------------------------------------------------


Project Work 
-----------
1) Citation Work 
2) Memory Work 
3) Chat History 
4) Session Management Work 
5) State Management Work 


Best Practice 
-------------






Doubts
1) Document Object - Pydantic - ORM - Done
2) Creating the Object in __init__ is fine 
3) How to Write the DB queries with ORM - MongoDB
4) Request and Response Model Validation
5) Returning proper output from the function, if pass & fail

Industrial Analysis Improvement
‚úÖ Optional Improvements
Save generated .docx in a persistent directory and set the fileUrl in Report.
Store the generated .py and .docx with UUID names to avoid overwrite.
Use tempfile.NamedTemporaryFile instead of hardcoded generated_docx_script.py.




Today's Work 
------------ 
1) Best Practice for Backend 
2) 
3) 

One Page Report 
---------------
Industry Overview - what are factors influenced the growth 
Key Companies
Emerging Trends 
Competitive Positioning 
Investment Potential 


IBISWorld - A global market research company known for publishing industry reports and business intelligence.



Features 
1) Generate Report Per Document 
2) Giving Context to the ChatBot - Per Document 
3) Preview the Report 
4) Versioning the Report Generation - Base on the User's Intent 

Hackathon
---------
Group Innovation Management Systems (‚ÄúGIMS‚Äù) (groupinnovationmanagement.com)
Tata InnoVista (DIYRR) (tatadiyrnr.com)
Tata InnoVista website (tatainnovista.com)
Tata eHack
Tata Ideas Plus

1. Improved Products / services
Focuses on enhancing existing offerings to deliver better customer value. The proposed solutions could include ways to make current products smarter, more sustainable, or more user-friendly using design, technology, or process tweaks. Improvements could result in reducing service time, enhancing digital interfaces, or increasing product durability. The goal is to elevate customer satisfaction while optimizing internal efficiency.
2. New Products / services
Focuses on creation of innovative offerings that address unmet or ignored needs of current customers or open new markets. The proposed solutions may involve identifying emerging trends or customer pain points and designing solutions that are scalable and impactful. They may span digital platforms, eco-friendly products, or inclusive services for underserved segments. The focus will be on originality, feasibility, and value creation.
6. Finance & Accounts
Focuses on financial system workflows such as budgeting, reporting, and compliance. The proposed solutions might include automation of routine tasks, predictive analytics, or tools that enhance transparency and control. The solutions may result in reducing manual effort, improving accuracy, or enabling faster decision-making. This theme also welcomes ideas in fraud detection, audit readiness, and financial planning including cashflows.
8. Use of data & analytics for people related process
Focuses on leveraging data & emerging technologies to improve HR functions like hiring, retention, capability building, internal or group movement, engagement & satisfaction.
9. Reimagining use of new/emerging technologies in areas not covered under above themes
This open theme focuses on bold ideas that use emerging technologies in areas not covered under any of the above themes. The goal of this theme is to provide equal opportunity to all young employees to push boundaries and explore the untapped potential of emerging tech in all areas of business.

------------------------------------------------
1.client.chat.completions.create(
2./v1/chat/completions ‚Äì Chat Completions API
3.Use function-calling with "strict": true in your function definitions.
4.Or enable response_format / json_schema mode for models like gpt-4o-2024-08-06.
5.Works with GPT‚Äë4o, GPT‚Äë4.1, GPT‚Äë3.5 Turbo ‚â•‚ÄØ0613, etc


response = openai.ChatCompletion.create(
    model="gpt-4o-2024-08-06",
    messages=[
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user",   "content": "Give me a joke in structured JSON."}
    ],
    functions=[
      {
        "name": "get_joke",
        "description": "Fetch a joke",
        "strict": True,
        "parameters": {
          "type": "object",
          "properties": {
            "setup": {"type": "string"},
            "punchline": {"type": "string"}
          },
          "required": ["setup", "punchline"]
        }
      }
    ],
    function_call={"name": "get_joke"}
)
joke = response["choices"][0]["message"]["function_call"]["arguments"]


from pydantic import BaseModel, Field
from langchain_openai import ChatOpenAI

class Joke(BaseModel):
    setup: str = Field(description="Setup of the joke")
    punchline: str = Field(description="The punchline")

llm = ChatOpenAI(model="gpt‚Äê4o‚Äêmini", temperature=0)
structured_llm = llm.with_structured_output(Joke, method="json_schema", strict=True)
result = structured_llm.invoke("Tell me a joke about cats")
print(result.setup, result.punchline)
This ensures the output is a valid Joke object 
üìå Two main modes:
Function-calling (default):

mode="openai-functions" ‚Äì binds a Pydantic schema as a pseudo-function.

Use strict=True to enforce schema compliance.

JSON-schema mode:

mode="openai-json", also method="json_schema" in newer versions.

Enforces schema adherence at model level 

------
.with_structured_output() on chat models
Wraps OpenAI (or other provider) chat models to enforce structure.
Supports modes:
"openai-functions" (function-calling schema)
"openai-json" or "json_mode" (JSON-schema mode)
--------

/v1/files, /v1/fine-tunes, /v1/usage, /v1/edits, /v1/completions/streaming, etc.
These support tasks like uploading training files, managing fine-tune jobs, streaming long completions, generating text edits, and monitoring API usage

128,000 tokens, equivalent to more than 300 pages of text.
100 Tokens = 75 words 
4 chars of text --> 1 token --> translates 3/4 of a word
https://magic.dev/blog/100m-token-context-windows -- 100 Million Token
This equals ~10 million lines of code of ~750 novels.
TinyLlama can‚Äôt generate more than a few sentences at a time. You will also need around 3 GB of disk space and RAM to load this model onto memory for inference.
gpt-4o-mini
gpt-4.1-nano
gpt-4.1-mini
gpt-4 November 2023- A faster and more cost-effective version of GPT‚Äë4
gpt-4o - Multimodal (text,image,audio) - May 13, 2024 ‚Äì Multimodal (text, image, audio), with larger context window
gpt-4o-mini - July 18, 2024 ‚Äì Compact, cheaper multimodal variant 
o4-mini
o3-mini
o3
o1-preview/o1-mini - first reasoning model - September 12, 2024 ‚Äì First ‚Äúreasoning‚Äù model and its smaller variant
o1 - o1 ‚Äì December 5, 2024 ‚Äì Full reasoning-capable model 
gpt-4.5 - February 27, 2025 ‚Äì Codenamed Orion, improved accuracy and conversational quality - Orion - improved accuracy and conversational quality 
o3-mini - January 31, 2025 ‚Äì Next-gen reasoning small model
o3 - April 16, 2025 ‚Äì Stronger reasoning and logic model 
o4‚Äëmini ‚Äì April 16, 2025 ‚Äì Lighter, efficient reasoning model that succeeds o3‚Äëmini
o1‚Äëpro ‚Äì March 2025 (announced) / March 19, 2025 ‚Äì Premium reasoning-capable model with high compute 
en.wikipedia.org
o3‚Äëpro ‚Äì June 10, 2025 ‚Äì OpenAI's most advanced reasoning model to date
GPT‚Äë4.1, GPT‚Äë4.1 mini, GPT‚Äë4.1 nano ‚Äì April 14, 2025 ‚Äì New multimodal series outperforming GPT‚Äë4o in coding, long‚Äëcontext and cost

GPT‚Äë4 ‚Üí GPT‚Äë4 Turbo ‚Üí GPT‚Äë4o ‚Üí GPT‚Äë4o mini ‚Üí o1-preview/mini ‚Üí o1 ‚Üí GPT‚Äë4.5 ‚Üí o3-mini ‚Üí o3 & o4-mini ‚Üí o1-pro ‚Üí o3-pro ‚Üí GPT‚Äë4.1 / mini / nano

gpt-4o
gpt-4-turbo
gpt-4-5-turbo
gpt-4-1-turbo

o1
o1-mini
o1-mini
o1-pro
o1-preview
o1-pro
o3
o3-mini
o3-pro
o4-mini

Only gpt-4o currently supports image, video (via Sora), and audio input/output directly via OpenAI's platform

Long Context 
Image Input and Image Output 
Reasoning 


(                                                                                ‚îÇ                   ‚îÇ
‚îÇ ‚îÇ                      ‚îÇ   ‚îÇ   'GPT-4.1-nano - Ultra-lightweight model for basic operations',               ‚îÇ                   ‚îÇ
‚îÇ ‚îÇ                      ‚îÇ   ‚îÇ   'gpt-4.1-nano'                                                               ‚îÇ                   ‚îÇ
‚îÇ ‚îÇ                      ‚îÇ   ),                                                                               ‚îÇ                   ‚îÇ
‚îÇ ‚îÇ                      ‚îÇ   ('GPT-4.1-mini - Compact model with good performance', 'gpt-4.1-mini'),          ‚îÇ                   ‚îÇ
‚îÇ ‚îÇ                      ‚îÇ   ('GPT-4o - Standard model with solid capabilities', 'gpt-4o'),                   ‚îÇ                   ‚îÇ
‚îÇ ‚îÇ                      ‚îÇ   ('o4-mini - Specialized reasoning model (compact)', 'o4-mini'),                  ‚îÇ                   ‚îÇ
‚îÇ ‚îÇ                      ‚îÇ   ('o3-mini - Advanced reasoning model (lightweight)', 'o3-mini'),                 ‚îÇ                   ‚îÇ
‚îÇ ‚îÇ                      ‚îÇ   ('o3 - Full advanced reasoning model', 'o3'),                                    ‚îÇ                   ‚îÇ
‚îÇ ‚îÇ                      ‚îÇ   ('o1 - Premier reasoning and problem-solving model', 'o1')     


{"role": "system", "content": "Think step by step before answering."},
{"role": "user", "content": "Why do objects fall to the ground?"}
{
            "role": "function",
            "name": "get_current_weather",
            "content": observation,
        }

-----------------------------------------------------------------------------------------------

Index:
Understanding the All the Backend Concepts:
Document Classification
Mongo DB 
Common Doubts
Deep Research
Tavily Search
Fine Tuning
DSA Algorithm 
Fast API
MCP
UV Installation
Data Engineering Pipeline 
CAG 
Preparing for Interview
Stateful AI Agent
Azure AI Search 
Cosmos DB - Integrated Vector DB
Introduction to Microsoft 365 Copilot 
API Integration
Azure Logic Apps 
Azure App Service Functions 
Work Explanation   - assess ESG risks 
A2A Protocol 
IBM Agentic Core Platform 
Meta - Academic Project - OAA  - Open Agent Architecture 
Model Context Protocol 
AI Agents vs Agentic AI
Azure AI Models
Agentic AI Memory Research Paper
AI Agent - Meta, Yale, Standford, DeepMind, Microsoft
Azure Cloud Native Solution
DSA Algorithm Continue 
Project Work
Goldman Sachs Interview Preparation


-------
Award Wining RAG 
-------
Currently working & Delivering the GenAI Project in BFSI domain - Automated Credit Application using GenAI - BFSI UK ME - Client CBD 
As a Data Engineer I have worked in Building the pipeline to preprocess  the several complex PDF report like Annual Report, Management Report, Security Document 
preprocessing the unstructured complex multimodal data to the normalized into standard LLM ready structured data Markdown Format and extracted required data from the documents to add as metadata for each chunk to process & enhance the capabilities in RAG search 
I have used SOTA preprocessing concept using the Vision Transformer & OCR Capabilities 

As a AI Engineer, provided advance AI strategies to build the RAG Pipeline to use & leverage GenAI Capabilities efficiently with Responsible AI principles 
Created a Advance RAG Pipeline to do different analysis with the preprocessed document like Financial Analysis, Management Analysis, SWOT analysis etc 
I have faced more challenges in the using the best chunking strategies & retrieval strategies to create vector store for RAG. 


I have worked in several POV projects using GenAI and Agentic AI 
1) Insurance Underwriting Guidelines Automation using the GenAI 
2) Creating the Application for HR To filter the right Candidate for the project 


Hands-on approach to AI for real-world applications (for TCS)
Certification Id: 23230
Date: 2024 - November 

I have several Trainings in GenAI & AI & Data Engineering so far - 4 times 
1) Pune TD Organized - Azure AI Engineer , Azure Data Engineer - 4 Hours Certification Training 
2) Pune TD Organized - GenAI Engineer [Mathematical & Practical] - 2 Hours E1 Competency Training 
3) Bangalore TD Organized - GenAI Engineer [Mathematical & Practical] - 2 Hours E1 Competency Training 


I have Developed AI Agentic System using GenAI Capabilities to do the Financial Market Research to utilize in Share Market Trading & Analysis without patent 


Have created POC in Blog Post Writing using the Agentic AI 
I have Created the Several AI Agents to do research on the Trending topic and write the Blog Post 
1) Planning Agent 
2) Research Agent 
3) Drafting over the Research documents
4) Critique Agent 

These Multi Agents will perform collaboratively and create the Blog post on the Trending Technical Topics provided by the user 
Please find the source for POC Project 






Understanding the All the Backend Concepts:
-------------------------------------------
Async 
Where Session Created 
Authentication 
Swagger UI 
How Request Sending to the Backend 
How Request 



Things to Done
---------------
1) To Add Metadata - what are the Metadata we need, Page Number: 
                                                    Section Heading 
                                                    Little Summary 
                                                    File Name 
                                                    Entity 
                                                    Questions 

                                                    

2) To Use the Hybrid Search - and Separately using that metadata
3) Setting up the Pipeline for each one separately 
4) https://medium.com/@chetanpp30/advance-rag-using-llama-parse-metadata-linking-ddab8193b016


Metadata Extracted
How to Use the Metadata in 
   1) Embedding Metadata & Text 
   2) Vectorizing Metadata & Text 
   3) Using Metadata as Filter - so in Vector Index we have metadata fields as it is 
   4) Passing the Metadata into the LLM Calls Why ? 

1)EntityExtractor =- extracts entities (i.e. names of places, people, things) mentioned in the content of each Node
2)TitleExtractor =- extracts a title over the context of each Node
3)QuestionsAnsweredExtractor =- extracts a set of questions that each Node can answer
4)SummaryExtractor =- automatically extracts a summary over a set of Nodes


API Backend Project 
-------------------
Django will create the Deployment version - API Documentation 
Swagger UI
OpenAPI Specification 



Need to Know About
------------------
Session Management DJRF
Caching - Redis Cache - for User Question 
Backend Authentication and Authorization 
CDN - CDN (Content Delivery Network) is a network of servers distributed geographically to deliver web content (like images, videos, stylesheets, scripts, and even full web pages) more efficiently to users.
Semantic vs Relevant 
Batch CDC Stream
Memory Management of Chat Per Session
Memory Management of Entire Chat of per User - This is Called the AI Agent - Letta Comes
Memory Management of Chat per session in summarizing and storing in VectorDB - This is also context window 





Project Work 
-----------
Approach Going to Follow 
1) Query Expansion with Prompt Specific Template - giving full metadata and searching through hybrid search - Getting the Relevant Context 
2) No adding the retrieved context to the message - for the follow up question, using the same approach 
        - First Getting the Standlone Question 
        - Based on the Prompt Specific Getting the Query Expansion for that Follow up question - giving full metadata and searching through hybrid search - Getting the Relevant Context 



Agentic Setup 
-------------
1) Parent Document Retrieval - after retrieving the chunks from the document 




Document Preprocessing Steps 
The user submits a question  ---> Need to Query Expansion with follow question necessary 
Vector search identifies relevant text chunks
Parent Document Retrieval expands these chunks to include surrounding context
(Optional) LLM reranking further refines the context
The expanded context is used for answer generation with chain-of-thought reasoning
 

LLamaParse 
Simple Document 
{'id_': 'd5ab23c4-ad19-46ae-bce0-87b8c9f6edb2',
 'embedding': None,
 'metadata': {'page_label': 'COV1',
  'file_name': 'Emmar_Annual_Report_2024_2025.pdf',
  'file_path': 'Emmar_Annual_Report_2024_2025.pdf',
  'file_type': 'application/pdf',
  'file_size': 20704160,
  'creation_date': '2025-03-27',
  'last_modified_date': '2025-03-26'},
 'excluded_embed_metadata_keys': ['file_name',
  'file_type',
  'file_size',
  'creation_date',
  'last_modified_date',
  'last_accessed_date'],
 'excluded_llm_metadata_keys': ['file_name',
  'file_type',
  'file_size',
  'creation_date',
  'last_modified_date',
  'last_accessed_date'],
 'relationships': {},
 'metadata_template': '{key}: {value}',
 'metadata_separator': '\n',
 'text_resource': MediaResource(embeddings=None, data=None, text='EMAAR PROPERTIES PJSC\nINTEGRATED ANNUAL REPORT 2024', path=None, url=None, mimetype=None),
 'image_resource': None,
 'audio_resource': None,
 'video_resource': None,
 'text_template': '{metadata_str}\n\n{content}'}
Document - list object 
documents[0].__dict__
{'id_': 'cbc4d0b3-b05e-462f-bc77-67c53ec872e1',
 'embedding': None,
 'metadata': {},
 'excluded_embed_metadata_keys': [],
 'excluded_llm_metadata_keys': [],
 'relationships': {},
 'metadata_template': '{key}: {value}',
 'metadata_separator': '\n',
 'text_resource': MediaResource(embeddings=None, data=None, text='\\# EMAAR # EMAAR PROPERTIES PJSC # INTEGRATED ANNUAL REPORT 2024', path=None, url=None, mimetype=None),
 'image_resource': None,
 'audio_resource': None,
 'video_resource': None,
 'text_template': '{metadata_str}\n\n{content}'}
MarkdownElementNodeParser
A parser specifically designed to understand the structure of Markdown documents (e.g., headings, subheadings, paragraphs, etc.). It's LLM-enhanced to better determine how to segment or chunk the content for downstream tasks (like indexing, retrieval, or question answering).
llm=llm:
The LLM is optionally used to enhance parsing, especially in ambiguous or complex cases where structure isn't clear just from syntax.
nodes[0].__dict__
{'id_': '2f35ae8d-e7f5-47d9-afc9-a866a8ca7a18',
 'embedding': None,
 'metadata': {},
 'excluded_embed_metadata_keys': [],
 'excluded_llm_metadata_keys': [],
 'relationships': {<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='cbc4d0b3-b05e-462f-bc77-67c53ec872e1', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='a0f7f25855108bc6a4cce0fa5c067aac424c9d6fdbb4a37a9375c089e6ed23cb')},
 'metadata_template': '{key}: {value}',
 'metadata_separator': '\n',
 'text': '\\# EMAAR # EMAAR PROPERTIES PJSC # INTEGRATED ANNUAL REPORT 2024',
 'mimetype': 'text/plain',
 'start_char_idx': 0,
 'end_char_idx': 64,
 'metadata_seperator': '\n',
 'text_template': '{metadata_str}\n\n{content}'}

Index Node 
{'id_': '4d1e36ef-2467-4cf0-94cb-f96c234fd623',
 'embedding': None,
 'metadata': {'col_schema': 'Column: AED 35.5 Bn\nType: string\nSummary: None\n\nColumn: Revenue\nType: string\nSummary: None\n\nColumn: AED 19.3 Bn\nType: string\nSummary: None\n\nColumn: EBITDA\nType: string\nSummary: None\n\nColumn: AED 110+ Bn\nType: string\nSummary: None\n\nColumn: Revenue backlog\nType: string\nSummary: None\n\nColumn: AED 17.5 Bn\nType: string\nSummary: None\n\nColumn: Net profit2\nType: string\nSummary: None'},
 'excluded_embed_metadata_keys': ['col_schema'],
 'excluded_llm_metadata_keys': [],
 'relationships': {<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='ef9e660e-927c-485f-b956-55ae205dece2', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='54a3da93e1917b017818271bdfcc3eca92ab5bc041f3ea342913a13d79d87efc'),
  <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='f5e63d56-738b-4394-8fa3-0ba17a36db1e', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='3c0be448e17398f8056e8ff94bf6cebc296d688f3a0e349931dd8c9978139d19'),
  <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='b93791e6-08a9-47ac-a5e8-e68fba39a5f4', node_type=<ObjectType.TEXT: '1'>, metadata={'table_df': "{' AED 35.5 Bn ': {0: ' AED 19.3 Bn ', 1: ' AED 110+ Bn ', 2: ' AED 17.5 Bn '}, ' Revenue         ': {0: ' EBITDA          ', 1: ' Revenue backlog ', 2: ' Net profit2     '}}", 'table_summary': 'The table provides financial metrics including revenue, EBITDA, revenue backlog, and net profit, expressed in billions of AED.,\nwith the following columns:\n- AED 35.5 Bn: None\n- Revenue: None\n- AED 19.3 Bn: None\n- EBITDA: None\n- AED 110+ Bn: None\n- Revenue backlog: None\n- AED 17.5 Bn: None\n- Net profit2: None\n'}, hash='34e4c5380b289ea1d092fb7e6048bed5c95e940c1f2f5ae41d29148a2fffdf05')},
 'metadata_template': '{key}: {value}',
 'metadata_separator': '\n',
 'text': 'The table provides financial metrics including revenue, EBITDA, revenue backlog, and net profit, expressed in billions of AED.,\nwith the following columns:\n- AED 35.5 Bn: None\n- Revenue: None\n- AED 19.3 Bn: None\n- EBITDA: None\n- AED 110+ Bn: None\n- Revenue backlog: None\n- AED 17.5 Bn: None\n- Net profit2: None\n',
 'mimetype': 'text/plain',
 'start_char_idx': 440,
 'end_char_idx': 609,
 'metadata_seperator': '\n',
 'text_template': '{metadata_str}\n\n{content}',
 'index_id': 'b93791e6-08a9-47ac-a5e8-e68fba39a5f4',
 'obj': None}

IndexNode(id_='4d1e36ef-2467-4cf0-94cb-f96c234fd623', embedding=None, metadata={'col_schema': 'Column: AED 35.5 Bn\nType: string\nSummary: None\n\nColumn: Revenue\nType: string\nSummary: None\n\nColumn: AED 19.3 Bn\nType: string\nSummary: None\n\nColumn: EBITDA\nType: string\nSummary: None\n\nColumn: AED 110+ Bn\nType: string\nSummary: None\n\nColumn: Revenue backlog\nType: string\nSummary: None\n\nColumn: AED 17.5 Bn\nType: string\nSummary: None\n\nColumn: Net profit2\nType: string\nSummary: None'}, excluded_embed_metadata_keys=['col_schema'], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='ef9e660e-927c-485f-b956-55ae205dece2', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='54a3da93e1917b017818271bdfcc3eca92ab5bc041f3ea342913a13d79d87efc'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='f5e63d56-738b-4394-8fa3-0ba17a36db1e', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='3c0be448e17398f8056e8ff94bf6cebc296d688f3a0e349931dd8c9978139d19'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='b93791e6-08a9-47ac-a5e8-e68fba39a5f4', node_type=<ObjectType.TEXT: '1'>, metadata={'table_df': "{' AED 35.5 Bn ': {0: ' AED 19.3 Bn ', 1: ' AED 110+ Bn ', 2: ' AED 17.5 Bn '}, ' Revenue         ': {0: ' EBITDA          ', 1: ' Revenue backlog ', 2: ' Net profit2     '}}", 'table_summary': 'The table provides financial metrics including revenue, EBITDA, revenue backlog, and net profit, expressed in billions of AED.,\nwith the following columns:\n- AED 35.5 Bn: None\n- Revenue: None\n- AED 19.3 Bn: None\n- EBITDA: None\n- AED 110+ Bn: None\n- Revenue backlog: None\n- AED 17.5 Bn: None\n- Net profit2: None\n'}, hash='34e4c5380b289ea1d092fb7e6048bed5c95e940c1f2f5ae41d29148a2fffdf05')}, metadata_template='{key}: {value}', metadata_separator='\n', text='The table provides financial metrics including revenue, EBITDA, revenue backlog, and net profit, expressed in billions of AED.,\nwith the following columns:\n- AED 35.5 Bn: None\n- Revenue: None\n- AED 19.3 Bn: None\n- EBITDA: None\n- AED 110+ Bn: None\n- Revenue backlog: None\n- AED 17.5 Bn: None\n- Net profit2: None\n', mimetype='text/plain', start_char_idx=440, end_char_idx=609, metadata_seperator='\n', text_template='{metadata_str}\n\n{content}', index_id='b93791e6-08a9-47ac-a5e8-e68fba39a5f4', obj=None),


LLamaParse Points [I need to add the metadata parameter in the Object and how to see the structure] 
-----------------
1) Document - list objects 
2) MarkdownElementNodeParser - Based on Sections & Headings it will split the text 
3) Nodes - Often structured with hierarchy based on document structure (e.g., heading levels)
4) TextNodes - A TextNode is a basic unit of text. It‚Äôs just a chunk of content with optional metadata. Most of your parsed content will end up as TextNode objects
5) IndexNodes[Check detailly what details it have] - An IndexNode is a special node that points to other nodes, kind of like a summary or table of contents node.
It does not have raw text itself, but instead references other nodes.
You‚Äôll usually get an IndexNode when the Markdown parser detects hierarchical structure, such as: Table or Markdown Structure 
6) To the nodes type and how its printing- 
 for i in nodes:
    print(i)
    print(type(i))
    #print(isinstance(i, IndexNode))
7)base_nodes, objects = node_parser.get_nodes_and_objects(nodes)
the method get_nodes_and_objects() is part of LlamaIndex's enhanced node parsing interface, especially when you're working with structured documents (like Markdown, HTML, Notebooks, etc.) and want to build rich hierarchical object representations.
Building a document navigator
Tree-structured answers (e.g., outline view)
Querying by section or topic (e.g., ‚ÄúGive me insights under ‚ÄòProduct A‚Äô only‚Äù)


Hugging Face 
c:\Users\sivaraj\Desktop\Project\cbdenv\Lib\site-packages\huggingface_hub\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\Users\sivaraj\.cache\huggingface\hub\models--BAAI--bge-reranker-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.
To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development
  warnings.warn(message)


Cosmos DB 
OperationFailure: Error in specification { "name" : "default_vector_search_index", "key" : { "content_vector" : "cosmosSearch" }, "cosmosSearchOptions" : { "kind" : "vector-hnsw", "m" : 2, "efConstruction" : 64, "similarity" : "COS", "dimensions" : 1536 } } :: caused by :: hnsw index is not supported for this cluster tier, full error: {'ok': 0.0, 'errmsg': 'Error in specification { "name" : "default_vector_search_index", "key" : { "content_vector" : "cosmosSearch" }, "cosmosSearchOptions" : { "kind" : "vector-hnsw", "m" : 2, "efConstruction" : 64, "similarity" : "COS", "dimensions" : 1536 } } :: caused by :: hnsw index is not supported for this cluster tier', 'code': 115, 'codeName': 'CommandNotSupported'}


----------------------------------------------------------------------------------------------------------------------------
LLamaParse Options 
1. Presets and Parsing Modes

fast_mode: For simple, text-only documents. Skips OCR, layout, and image extraction.
balanced_mode (default): For documents with mixed content (text, tables, images). Performs OCR, image extraction, and structure detection.
premium_mode: For complex, highly formatted documents. Adds LaTeX and diagram extraction.
preset: Can be set to use-case-specific options like "complexTables" for complex table extraction.
parse_mode: Fine-grained control, e.g., "parse_page_without_llm", "parse_page_with_llm", "parse_page_with_lvm", "parse_page_with_agent", "parse_page_with_layout_agent", "parse_document_with_llm", "parse_document_with_agent".
2. Output and Formatting

result_type: Choose "text", "markdown", or "json" output.
structured_output: Boolean, enables direct JSON extraction using a schema.
structured_output_json_schema / structured_output_json_schema_name: Define or select a schema for structured output.
3. Advanced Options

disable_ocr: Boolean, disables OCR.
disable_image_extraction: Boolean, disables image extraction.
skip_diagonal_text: Boolean, skips diagonal text.
do_not_unroll_columns: Boolean, prevents column unrolling.
target_pages: String, specifies which pages to parse.
page_separator, page_prefix, page_suffix: Customize how pages are separated or annotated in output.
bounding_box, bbox_top, bbox_right, bbox_bottom, bbox_left: Restrict parsing to a specific area of a page.
use_vendor_multimodal_model, vendor_multimodal_model_name, vendor_multimodal_api_key: Enable and configure multimodal parsing with external models.
language: Specify the language for parsing.
invalidate_cache, do_not_cache: Control caching behavior.
take_screenshot: Boolean, enables page screenshots.
webhook_url: URL for job completion callbacks.
4. Prompt Customization

user_prompt, system_prompt, system_prompt_append: Add custom instructions to guide parsing.
5. Miscellaneous

num_workers: Number of parallel workers for batch jobs.
input_s3_path, output_s3_path_prefix: For S3 integration.
max_pages: Limit the number of pages parsed.



Directly Chatting the PDF with Vision Transformer  

To Achieve the SOTA[State of the Art] Credit Application
1) Structured Output 
2) Agentic RAG - for using the VectorDB as a Tool
3) Memory Management Efficiently 
4) Storing that Memory Management in Vector Store - for Justification 
5) Maintaining the Prompt Template - to Act as Professional Person for particular Section
6) Guardrails



1.Retriever Should go with only User Query 
2.Finally LLM Should have the Prompt Query Content 


Document Classification 
-----------------------
1) User Will Upload the Documents --> without selecting the Document Type 
2) Backend We Need to Classify the Documents - and provide the JSON Result , include - document and document name classified 

Financial Annual Report Document 
Risk Report Document 
Security Document 

1) Ingesting the Document 
2) Parsing it and getting the Markdown format of Document 
3) Classifying the Document using LLM - passing the First 5 Pages along with Index Page 
3) Storing the Original PDF in Azure Blob Storage along with Document Category  
4) Creating the Embedding & vector store 

Data Ingestion Pipeline
-----------------------
BackendEnd have(Logic to Ingest + Parsing(Llamaparse)[Parallel Parsing of Multiple Documents] + Classify & Store(Azure Blob Storage) + Create Embedding & Vector Store(Azure Cosmos DB or Azure AI Search)
   in Between - Using Azure Service[Azure Functions or Azure Logic Apps]  to Decouple the process

      




Mongo DB
--------
No-SQL Relational Database 
Mongo DB - stores data in flexible, JSON-like documents called BSON - Binary JSON)
Key Features of MongoDB
--
Flexible Data Model 
Document-Oriented: key-value pairs
Schema-less - Documents in the same collection have different fields
Horizontal Scaling - Built for scalability via sharding(splitting data across servers)
Rich Query Language - Filtering, Sorting, indexing, aggregation, and full-text search 
Strong Ecosystem - Tools like MongoDB Atlas (cloud DB) 

Documents 
Basic Unit of Data - stored in BSON
Collections - Groups of Documents equivalent to a table in SQL 
Database - Logical Container of Collections 
Instance - Single MongoDB server (that can host multiple database)
Replica Set - Group of MongoDB maintain the same data set 
            - Primary for writes, Secondaries for reads/failover)
Sharding(Scalability) - Splits large datasets across multiple machines - Uses a Shard Key to distribute data 
**Mongos (Router) - A routing service used in Shared clusters to direct queries to the correct shard 

Azure Cosmos DB Fully managed NoSQL database service 
---
That supports multiple API 
One of the API is MongoDB API  - RU-Based Model 
MongoDB commands and tools on Cosmos DB 

---
Cosmos DB instance with MongoDB API - Cosmos DB backend that understands MongoDB wire protocol
**you can connect the Cosmos DB using MongoDB drivers, Compass, Mongoose etc just like native MongoDB database 

you use the MongoDB driver or tools like Compass or Mongo Shell 
you point them to a Cosmos DB Connection string 

----
Cosmos DB does not run MongoDB itself
Instead, it runs a protocol-compatible engine that understand and interprets requests made using the MongoDB wire protocol(
This allows MongoDB drivers and tools to interact as if they are talking to a real MongoDB server 


----
MongoDB Vcore Based Model 
- Newer Model with MongoDB compatibility using vCore architecture (compute + storage like traditional VMs) 
- It is Cosmos Db for MongoDB using vCore architecture, similar to how SQL databases are hosted with Dedicated Compute and Storage 
  It's different from the RU/s model 

key Features 
---
MongoDB wire protocol compatible 
vcore pricing - pay per virtual core (CPU), storage and IOPS
Better MongoDB Parity - Supports more Mongo DB native features than RU-Based version 
Ideal for lift-shit from on-prem MongoDB to cloud 

AzureCosmosDBMongoDBvCoreDemo
--------
how to use Azure Cosmosdb Mongodb vCore to perform vector searches in LlamaIndex

Azure Cosmos DB
Highest Fidelity with Azure Services 
Built-in Vector Search 
   Native Support for MongoDB vCore and PostgreSQL API's 
   Integrated with Azure Cognitive Search for Core NoSQL API 



Vector Embeddings & Vector Search
Vector Indexes - IVF & HNSW 
IVF - Inverted File Index - Partitions vectors into clusters and assign each vector to one cluster 
                            Building the index is fast and memory-efficient 
                            Requires a separate clustering step before indexing (slow)
                            Tuning parameters is important
HNSW - Hierarchical Navigable Small World - Graph Based Indexing Algorithm - (Each layer , network of nodes)
     - It involves creating a layered Graph Structure 
     - Builds a Multi-Layer graph with long and short connections between the vectors 
     - Robust and accurate at scale 
     - No-Preprocessing step 
     - Can support many inserts/deletes efficiently 
     - Large memory footprint 
     - It also has many parameters (such as the number of layers and neighbors) that need to be tuned carefully 

Azure Cosmos DB for MongoDB vCore
     - Cluster Form factor that allows to like scale up and scale outwards 

Vcore AI Ready [Built into the engine - Data, embeddings your vectors, and vector indexing is built into the database]
--------------
Native Vector Search, Including HNSW 
Plugins - LangChain, Semantic kernel and LlamaIndex
Integration with Azure OpenAI Studio 


OperationFailure: Error in specification { "name" : "default_vector_search_index", "key" : { "content_vector" : "cosmosSearch" }, "cosmosSearchOptions" : { "kind" : "vector-hnsw", "m" : 2, "efConstruction" : 64, "similarity" : "COS", "dimensions" : 1536 } } :: caused by :: hnsw index is not supported for this cluster tier, full error: {'ok': 0.0, 'errmsg': 'Error in specification { "name" : "default_vector_search_index", "key" : { "content_vector" : "cosmosSearch" }, "cosmosSearchOptions" : { "kind" : "vector-hnsw", "m" : 2, "efConstruction" : 64, "similarity" : "COS", "dimensions" : 1536 } } :: caused by :: hnsw index is not supported for this cluster tier', 'code': 115, 'codeName': 'CommandNotSupported'}
Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...


Cosmos DB  Feature 
Advanced Vector Index Algorithm 
Native Approach 
Integration with Llamaindex, Langchain
Hybrid Search - Limitations in Hybrid Search 
Full Text Search 





Tavily Search - 
-------------
The accidents and incidents related to a particular airline.
The past history, if the user is asking for that, my agent will do Internet search.
And the results are passed to the user and if a user wants to do any further research on any of the links.
LangChain tool is used to extract that web page and some pydantic model kind of information is extracted from that page and finally pass to the user.





Deep Research
-------------
Considering Snowboard Options
Examining Technical Specifications
Outlining a Comprehensive Report 

Fine-Tuning[Change Behaivour & Adding New Knowledge] - Lamini  
--------------------
Eleuther AI - The Pile - Dataset 

Fine-Tuning for Generative Task[Discriminative Task] is not well-defined - because we updating the weight of the entire model
More Advanced way to How much you need to update the Model 

Extraction More Text-in - Less-Text-Out  ---> Reading 
  Eg: keyword, Topic, routing, agents(planning, reasoning, self-critic, tool use)
Expansion tex in , more text out         ---> Writing 
  Eg: Writing, chat, write emails, write code 

Instruction Fine-Tuning[Instruction-tune or Instruction following] - Turning all the models into the ChatGPT Style 
-----------------------------------------------------------------------
task can be do in Instruction is - reasoning, routing, copilot, chat, agents 
-> Dialogue Dataset - Instruction Response Dataset 
-> Alpaca[Standford] - Uses ChatGPT to do this 
-> Non-Q/A data can be also be converted to Q/A - Using a prompt template 
                                                  Using another LLM 
-> For Instruction fine-tuning and other different types of fine-tuning 
   Data prep is the really where you have difference ,change your data, tailor the data to the specific task of fine-tuning that you're doing 

Data Preparation for Fine-Tuning 
--------------------------------
1) Adding Prompt Template 
2) Tokenize - pad, Truncate 
3) batch of inputs - Everything in the batch should follow the same length 
4) Truncation to met the maximum token,
5) Sliding window on left or right 
6) Adding the Tokenize Input Id's 

Training
--------
1) Learning Rate
2) Optimizer Hyperparameter
3) 

Token Count
-----------
1000 Word ~ 1500 Tokens 



Fine - Tuning
-------------
-> Alpaca is the perfect dataset for fine-tuning your language models to better understand and follow instructions
-> Alpaca is a language model fine-tuned using supervised learning from a LLaMA 7B
-> Alpaca dataset from Stanford, cleaned and curated
  - https://github.com/gururise/AlpacaDataCleaned


DSA Algorithm 
--------------

Bubble Sorting - adjacently swap the elements, fix from left to right [ascending]
--------------
def sorting(nums):
   n = len(nums)
   for i in range(n):
      for j in range(n - i - 1):
         if nums[j] > nums[j+1]:
             nums[j+1],nums[j] = nums[j], nums[j+1]
    return nums

Selection Sorting - pick the smallest index in each iteration of 2nd for loop and fix the first element iteratively
-----------------
def sorting(nums):
   n = len(nums) 
   for i in range(n):
      min_index = i  
      for j in range(i+1,n):
         if nums[j] < nums[min_index]:
              min_index = j 
      nums[min_index], nums[i] = nums[i], nums[min_index]
    return nums


Insertion Sorting - Find the Key leaving the 0th index, and then 2nd for loop for changing the j index --> fixing the first value 
-----------------
def sorting(nums):
   n = len(nums):
   for i in range(1,n):
       key = nums[i] 
       j = i - 1 
       while j >= 0 and key < nums[j]:
           nums[j+1] = nums[j] 
           j-=1 
       nums[j+1] = key  
   return nums

Quick Sorting
-------------

Merge Sorting
-------------


map(function, iterable)
map(lambda,iterable)

Linked List 
-----------
Linear Data Structure Where each element (called a node) points to the next element in the sequence. 
Unlike arrays, linked lists do not store elements in contiguous memory locations.

Structure of a Node 
Data[Stores] Next[A Reference (or Pointer) to the next node in the list.]

Singly Linked List  - Each node points to the next node. One Directional 
Doubly Linked List  - Each node has a next and a prev pointer. Can move in both directions. 
Circular Linked List - The last node points back to the head node. 
                       Can be singly or doubly circular. 

Operations in Linked List 
Traversal   - Visiting all the nodes.
Insertion   - Adding a node at the beginning, end or middle.
Deletion    - Removing a node from the list.
Search      - Finding a node with a given value.

Advantages of Linked List 
Dynamic Size: Grows/Shrinks at runtime 
Efficient Insert/Delete - Easy Insertions/Deletions (no Shifting like arrays) 

Disadvantages 
-------------
Extra Memory: For Storing pointers 
Sequential access only: Cannot access an element directly like arr[5] in arrays 


Comparison with Arrays 
----------------------
Memory Allocation    Contiguous    Non-Contiguous 
Access Time          O(1)          O(n)
Insert/Delete(middle)  O(n)        O(1) (if node given)


class Node:
    def __init__(self,value):
        self.value = value
        self.next  = None 
        
class LinkedList:
    def __init__(self):
        self.head = None 
    
    #Reverse the list in memory(modifies the list)
    #Traverse the list, changing each node's .next pointer to point to th previous node 
    #At the End, update the head 
    def reverse(self):
        Prev = None
        current = self.head 
        while current:
            next_node  = current.next 
            current.next = Prev 
            Prev = current
            current = next_node 
        self.head = Prev 
    def insert_at_begin(self,data):
        new_node = Node(data)
        new_node.next = self.head
        self.head = new_node 
        
    def insert_at_end(self,data):
        new_node = Node(data)
        if not self.head: #If list is empty 
            self.head = new_node
            return 
        current = self.head 
        while current.next:
            current = current.next 
        current.next = new_node 
    
    def insert_at_given_position(self,position,data):
        if position < 0:
            print("Invalid LinkedList Length")
            return 
        new_node = Node(data)
        if position == 0:
            new_node.next = self.head 
            self.head = new_node 
            return 
        
        current = self.head 
        count = 0  
        
        while current and count < position - 1: 
            print("Here")
            current = current.next 
            count +=1 
            
        if not current: 
            print("Position out of bounds")
            return 
        print(current.value)
        print(current.next.value)
        new_node.next = current.next 
        current.next = new_node
        print(current.next.value)
        
        
    def display(self):
        current = self.head 
        while current:
            print(current.value,end=" -> ")
            current = current.next 
        print("None")
ll = LinkedList()
print(ll.__dict__)
print(ll.insert_at_end(10))
print(ll.__dict__)
print(ll.head.next)
print(ll.insert_at_end(20))
print(ll.__dict__)
print(ll.head.next)
ll.insert_at_begin(30)
print("-----------------------------")
ll.insert_at_given_position(1,20)
print(ll.display())
ll.insert_at_given_position(2,50)
print(ll.display())
ll.reverse()
print(ll.display())
----------------------------------------

FAST API 
--------
app/
‚îú‚îÄ‚îÄ main.py                 # FastAPI app entrypoint
‚îú‚îÄ‚îÄ models/                 # SQLModel ORM models
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py         # Sub-Package
‚îÇ   ‚îú‚îÄ‚îÄ application.py
‚îÇ   ‚îú‚îÄ‚îÄ borrower.py
‚îÇ   ‚îú‚îÄ‚îÄ document.py
‚îú‚îÄ‚îÄ helpers/                # Helpers
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py         # Sub-Package
‚îÇ   ‚îú‚îÄ‚îÄ aztranslation.py
‚îÇ   ‚îú‚îÄ‚îÄ embeddings.py
‚îÇ   ‚îú‚îÄ‚îÄ scraper.py
‚îú‚îÄ‚îÄ schemas/                # Pydantic request/response schemas
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py         # Sub-Package
‚îÇ   ‚îú‚îÄ‚îÄ application.py
‚îÇ   ‚îú‚îÄ‚îÄ document.py
‚îú‚îÄ‚îÄ routes/                 # Route definitions
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py         # Sub-Package
‚îÇ   ‚îú‚îÄ‚îÄ application.py
‚îÇ   ‚îú‚îÄ‚îÄ document.py
‚îú‚îÄ‚îÄ dependencies.py         # Settings configuration
‚îú‚îÄ‚îÄ auth.py                 # Authentication configuration
‚îú‚îÄ‚îÄ database.py             # Session / DB engine configuration
‚îú‚îÄ‚îÄ generated/              # Uploaded files and 
‚îú‚îÄ‚îÄ alembic/                # Alembic migration files
‚îú‚îÄ‚îÄ constants/              # Constants
alembic.ini                 # Alembic config file
requirements.txt
README.md







Complete the DSA Algorithm
--------------------------
Bubble Sorting
Selection Sorting
Insertion Sorting
BFS Search 
DFS Search
A* Search   - f(n) - g(n) - Heuristic - cost of going to that index
Greedy Best First Search - GBFS - f(n) - Heuristic - cost of path
Uninformed Search  - BFS, DFS
Informed Search  - GBFS, A*
Adversarial Search - Minmax Algorithm - recursive algo
Adversarial Search - Minmax Algorithm with - Optimization 
Adversarial Search - Minmax Algorithm with - Evaluation - Example Chess Game
Quick Sorting - Divide and Conquer - Dividing the Problem with Middle value, {left,[MIDDLE],right} recursive for ever
Merge Sorting  - Dividing and Dividing with middle left to right recursively and then merge logic i = j = 0, with while 

MCP
---
1) How to Run the MCP server - sse, stdio
2) How to Run the Production Use Case, using own infrastructure 
3) How to Run the MCP Server - using uv, python mcp cli tool etc 
4) How to Run the MCP Inspector 
Microsoft MCP Server 
-------------------
Azure MCP Server - 


Integration to AI Agent 
-----------------------

Windsurf - IDE 
Cursor   - IDE 

Connection & Specific Implementation & Custom Works - will take care by MCP 

Agent [Claude Desktop, Windsurf, Cursor]
 | [protocol - to talk to MCP server[custom specification reside in the MCP server]
MCP Server 
 | 
Slack Gmail DB 

In your Client Application or IDE 
Slack Providing the MCP Server - connect MCP Server for Slack

MCP Connects Clients, servers, LLMs
    Hosts are LLM Application(Claude Desktop or IDEs) that initiate connections
    Clients maintain 1:1 connections with servers, inside the host application
    Servers provide context, tools, prompts to clients 


After Initialization, the following patterns are supported
After initialization, the following patterns are supported:
-----------------------------------------------------------
Request-Response: Client or server sends requests, the other responds
Notifications: Either party sends one-way messages
Either party can terminate the connection:
------------------------------------------
Clean shutdown via close()
Transport disconnection
Error conditions


Host - Cursor IDE - Application that implements agents 
MCP client - That's going to communicate with Server
Server - Contains the Implementation that we want to access 
         Slack MCP Server, 
         Gmail MCP Server 
         Custom MCP Server - Implement some python Functionality 


Tools - Implement Calculator, Weather Service - Expose those tools from the MCP server 
 
Host - Connects to the MCP server and immediately list all of the tools available 

Share Prompts - LLM or Agents can use those prompts to do stuff, or you can share resources
1.Create reusable prompt templates and workflows
2.Prompts enable servers to define reusable prompt templates and workflows that clients can easily surface to users and LLMs
3.They provide a powerful way to standardize and share common LLM interactions.
    ****Prompts are designed to be user-controlled, meaning they are exposed from servers to clients with the intention of the user being able to explicitly select them for use.*****

Prompts in MCP are predefined templates that can:
Accept dynamic arguments
Include context from resources
Chain multiple interactions
Guide specific workflows
Surface as UI elements (like slash commands)

Share Resources[Expose data and content from your servers to LLMs]
   - Agents knows how to call the API with these data 
1.Resources are core primitive in the Model Context Protocol (MCP) that allow servers to expose data and content that can be read by clients and used as context for LLM interactions.




 
                  How to call the 

------
Even Though You adding MCP server's from Different Companies, 
Everyone MCP Server is going to be independent - 
      MCP Server for Slack 
      MCP Server for Gmail 
      MCP Server for Custom Database 
      MCP Server Custom Things - these above 3 have to do by prompts, instead, can be do by the MCP custom server - 
---
Anthropic has a directory - have list of MCP Server's 
                 250+ MCP Server's 

uv installation 
---------------



Data Engineer Pipeline 
----------------------



CAG
---
Cache-Augmented Generation - 
Caching Relevant Information enabling faster, more efficient responses while enhancing scalability, accuracy and reliability 


CAG 
---
Implementation
Strategies
Analyze 

CAG is a approach that enhances language models by preloading relevant knowledge into their context window
    --- > Eliminating the need of real-time retrieval 
CAG optimizes knowledge-intensive tasks by leveraging precomputed key-value (KV) caches. - enabling faster and more efficient responses.
    --- > 

Preloading Knowledge - before inference, the relevant information is preprocessed and stored with an extended context or a dedicated cache. 
Key-Value Caching    - CAG utilizes precomputed inference states. These states act as a reference, allowing the model to access cached knowledge instantly. 
Optimized inference  - Model checks the cache for pre-existing knowledge embeddings. If a match if found. The model directly utilizes the stored context to generate a response. 
            This dramatically reduces inference time while ensuring coherence and fluency in generated outputs 




Query    Knowledge                            LLM 
           Source[Offline Preloading] -----> [Knowledge Cache] q r





Preparing for Interview
-----------------------
Gen AI - Assumptions, Sampling, Predictions, Inference Parameter
Different LLM Models - Context Length,  
Prompt Engineering - Prompt Structure 
Azure Prompt Flow 
RAG - Agentic RAG - How to Fit the Context, How to Retrieve the Results for Query 
How to Use Multiple Turn User Query + Maintaining the Memory + Retrieving the Documents
AI Agents 
    Back Story
    Prompt
    Connecting to Different Agent 
    Tool Usage 
    Message History 
Two Projects 
 Insurance Underwriting 
 Credit Loan Application 
Docker & Kubernetes - Done
Django  - Done

Hands-On Project 


-------------------------
Stateful AI Agent - Letta 
Dedicated Agent for Maintaining the Memory - Like Subconscious - Like Shadow of other agents 

1) Cog - Long Term Memory - recall or archival 
2) Letta Bringing those databases to interaction or developers responsible 
3) Human Semantic Episodic - you bring that add that as a plugin 
4) But letta - Hierarchy of the tokens that are in cache or like in context , and tokens that are out of like that cache 
5) If we want some more Psychological learning concepts - like from koala 
6) As tools that either like populate the core memory or like sit outside have to drawn in 

7) Very Transactional Agents so like in this particular example 
         like CRM so probably not salesforce, 
              CRM that has a object for people like a contact that I preserve in that longterm 
8) Memory blocks they're all backed by an API and you can access them directly 
   We can modify blocks by handle so you can in your app layer 

9) General way of designing the Memory Management is very much leaning on the thesis that like you just want to remove as much as human design of memory management as possible 

Reason AI Model 
1st - Reasoning 
2nd Turn - Avoid the Thought of first and ask the question and think 

This is the issue with Reasoners and ReAct Style Agents 


10)Compiling for at atleast what I've seen so far is the distillation of the agent like into some kind of summary so Do you control like the perspective 
   of like how a memory is bind how synthesis is actually like happening When it's creating memories 

  Ans: If you have agents that are generating and synthesizing the memories then you can always like tune their systems prompts or their personas to like adjust the way the memories are written 


   Evolving towards agentic workloads where like microservices takes a back state and a workload 

   Microservices like basically starting to become designed more for like agents maybe i think that's like one thesis that a lot of people have  
   
     new wave of API's that designed to call for agents 
---
11)There is a problem if we have a system where there's ***lot of information outside the context with the LLM , where you can't know what you don't know right***

12) View the information about the agent's context window, which includes metadata about the data available in external memory (eg: archival memory, recall memory)


13)External Memory: We can list the message history and also the archival memory of the agent - both which are also accessible to the agent via tools

14.1) Messages are in the memory buffer 
14)Letta and other Framework - very aggressive managing of this buffer 

       Much more intelligent version of like a recursive summarization mechanism 


Section 2: 
1) Understanding the core memory 
 and how this is memory blocks influence agent behavior because I think if you're developing agents the main thing you're going to be doing is like tuning in context memory 

Core Memory is memory that is stored in-context - so every LLM call, core memory is included. what's unique about Letta, is that this core memory is editable via tools by the agent itself. 
How can agent adapt its memory to new information 
2) Human block of core memory is used to remember information about the human in the conversation. As the agent learns new information about the human, it can update this part of memory to improve personalization. 

3) The Agent also records information about itself and how it behaves in the persona section of memory, This is important for ensuring a consistent persona over time. 
eg. not making the inconsistent claims, such as liking ice cream one day and hating it another) 

4) Unlike the system_prompt, the persona is editable, this means that it can be used to incorporate feedback to learn and improve its persona over time.

   In Letta agents will kind of like chain indefinitely , you can always set the limit in the API 


5) the Way they Chain is basically through heartbeat requests, 
       Notice that when core_memory_replace is alled, it has an extra argument, request_heartbeat, 

      In Letta, Agent Can control the execution loop, (e.g Whether the LLM is invoked again) by specifying the request_heartbeat argument, which is injected into all tools, If request request_heartbeat==True, The LLM will be called again (so the agent runs another step), if request_heartbeat=False, the agent will stop execution


ReAct - Agent Have to Say I'm Done, 
Letta - Agent Have to Say Keep Going - I think this is actually generally more practical because it like much less likely the agent will derail, if it has to explicitly say I want to keep going so we call those heartbeat request 


Section 3: 
  Understanding the Archival Memory 
 
Letta Agents store long term memories in archival memory, which persists data into an external databases. This allows agents additional space to write information outside of its context window, (eg: with core memory), Which is limited in size 

Basic Memory Editing tool via tool, giving the agent the ability to edit its own memory inside the context window 


1. Commit a bunch of my preferences to memory and then reset my conversation history and expected it to just pick up these previously remembered 
 
 If we edited anything in archival memory , like adding new messages, - Engineering Design problem, 
          -- then it will have to fetch and pull from archival first to see it so it's like you know it doesn't know what it doesn't know problem 
    this is actually fixable via like tool rules, so you can enforce behavior you could make a tool rule that says every time you enter the entry point of the LLM 
the step of the agent the agent has to call archival memory search , So this is kind of like building Graphs onto letta 
   

LangGraph - Decision Tree 

Letta - inverse - everythin starts off as a fully connected graph - the agent could do anything and but then you can kind of like start to enforce restrictions by peeling away edges - when you start you actually can't do everything , you can only do this but then once you do this you can do everything 


2. How can we identify what memory can store in what memory 
      everything got stored in core_memory 

    one thing is by default - which is core memory has limits and when a limit is hit that actually the way that works is let's say 
    agent is just being lazy it's like saving everything into core-memory 
       eg: TimeStamp, dates, etc 

     at certain point in time it will run out of space, 

    we need to cap the limit of those memory blocks because at the end of the day there is context window limit too 
    When it attempts to write it to core memory to a block that is at its limit the agent will actually get an error and actually prompt engineering around that eror,
it usually suggest the agent should clear by  
evict to the archival memory so it kind of is like a OS style flush concept 

say you running out of space, but you should consider like summarizing and pushing stuff out of archival memory 


-----------------------------------------------------
Letta - The Real Multi Agent Framework 

TCS SharePoint
https://tcscomprod-my.sharepoint.com/

Simulate and Evaluate the AI Agent 
https://www.getmaxim.ai/

Applied LLMs Evaluation Monitoring 
https://applied-llms.org/#evaluation-monitoring
https://hamel.dev/blog/posts/evals/#level-1-unit-tests
https://hamel.dev/blog/posts/evals/#level-1-unit-tests


Task weaver - Code First Agent - Microsoft 
Real Time SQL Engine
https://www.google.com/search?q=timeplus+real-time+SQL+engine&sca_esv=ea7911cd1c52328e&rlz=1C1GCEA_enIN1050IN1050&ei=CvvXZ4e1POqbnesP3eamiQs&ved=0ahUKEwiH4vHS9ZCMAxXqTWcHHV2zKbEQ4dUDCBA&uact=5&oq=timeplus+real-time+SQL+engine&gs_lp=Egxnd3Mtd2l6LXNlcnAiHXRpbWVwbHVzIHJlYWwtdGltZSBTUUwgZW5naW5lMgcQIRigARgKMgcQIRigARgKSLA_UABY6z1wAHgAkAECmAGyAqAB2iiqAQgwLjIyLjUuMbgBA8gBAPgBAZgCGqAC2yXCAgsQABiABBiRAhiKBcICCxAAGIAEGLEDGIMBwgIREC4YgAQYsQMY0QMYgwEYxwHCAgUQABiABMICCBAAGIAEGLEDwgIOEC4YgAQYsQMYgwEYigXCAgoQABiABBhDGIoFwgIREC4YgAQYkQIY0QMYxwEYigXCAg0QABiABBixAxhDGIoFwgINEC4YgAQYQxjlBBiKBcICEBAAGIAEGLEDGEMYgwEYigXCAggQLhiABBixA8ICBRAuGIAEwgImEC4YgAQYsQMYlwUY3AQY3gQY4AQY9AMY8QMY9QMY9gMY9wPYAQHCAgcQLhiABBgKwgIHEAAYgAQYCsICJRAuGIAEGAoYlwUY3AQY3gQY4AQY9AMY8QMY9QMY9gMY9wPYAQHCAgcQABiABBgNwgIGEAAYFhgewgIIEAAYgAQYogTCAgUQABjvBcICBRAhGKABwgIFECEYnwWYAwC6BgYIARABGBSSBwYwLjIwLjagB5qeAQ&sclient=gws-wiz-serp

https://www.google.com/search?q=prompter+pattern&sca_esv=9c61c6d612e03081&rlz=1C1GCEA_enIN1050IN1050&ei=CO3bZ5-oHuugvr0Pt6Gh-QU&ved=0ahUKEwifzNvsuJiMAxVrkK8BHbdQKF8Q4dUDCBI&uact=5&oq=prompter+pattern&gs_lp=Egxnd3Mtd2l6LXNlcnAiEHByb21wdGVyIHBhdHRlcm5IAFAAWABwAHgAkAEAmAEAoAEAqgEAuAEDyAEAmAIAoAIAmAMAkgcAoAcAsgcAuAcA&sclient=gws-wiz-serp




Azure AI Search 
---------------
AZURE_SEARCH_SERVICE="your-search-service-name"
AZURE_SEARCH_ADMIN_KEY="your-search-service-admin-key"
AZURE_SEARCH_INDEX="your-index-name"
AZURE_OPENAI_ENDPOINT="https://<your-openai>.openai.azure.com/"
AZURE_OPENAI_KEY="your-openai-key"
AZURE_OPENAI_DEPLOYMENT="embedding-deployment-name"


import openai
openai.api_type = "azure"
openai.api_base = os.getenv("AZURE_OPENAI_ENDPOINT")
openai.api_key = os.getenv("AZURE_OPENAI_KEY")
openai.api_version = "2023-05-15"

def get_embedding(text):
    response = openai.Embedding.create(
        input=text,
        engine=os.getenv("AZURE_OPENAI_DEPLOYMENT")
    )
    return response['data'][0]['embedding']

from azure.search.documents.indexes import SearchIndexClient
from azure.search.documents.indexes.models import (
    SearchIndex, SimpleField, SearchableField, VectorSearch, VectorSearchAlgorithmConfiguration, HnswVectorSearchAlgorithmConfiguration, VectorField
)
from azure.core.credentials import AzureKeyCredential

search_client = SearchIndexClient(
    endpoint=f"https://{os.getenv('AZURE_SEARCH_SERVICE')}.search.windows.net",
    credential=AzureKeyCredential(os.getenv("AZURE_SEARCH_ADMIN_KEY"))
)

index_name = os.getenv("AZURE_SEARCH_INDEX")

fields = [
    SimpleField(name="id", type="Edm.String", key=True),
    SearchableField(name="content", type="Edm.String"),
    VectorField(name="content_vector", dimensions=1536, vector_search_configuration="default")
]

vector_search = VectorSearch(
    algorithm_configurations=[
        VectorSearchAlgorithmConfiguration(
            name="default",
            kind="hnsw",
            parameters=HnswVectorSearchAlgorithmConfiguration()
        )
    ]
)

index = SearchIndex(
    name=index_name,
    fields=fields,
    vector_search=vector_search
)

search_client.create_or_update_index(index)


from azure.search.documents import SearchClient

docs = [
    {
        "id": "doc1",
        "content": "This is a sample document about financial statements.",
        "content_vector": get_embedding("This is a sample document about financial statements.")
    },
    # Add more docs...
]

search_client = SearchClient(
    endpoint=f"https://{os.getenv('AZURE_SEARCH_SERVICE')}.search.windows.net",
    index_name=index_name,
    credential=AzureKeyCredential(os.getenv("AZURE_SEARCH_ADMIN_KEY"))
)

search_client.upload_documents(docs)


query_text = "What are financial ratios?"
query_vector = get_embedding(query_text)

results = search_client.search(
    search_text=None,
    vectors=[
        {
            "value": query_vector,
            "fields": "content_vector",
            "k": 3
        }
    ]
)

for result in results:
    print(result['content'])


Vector & Hybrid Search
----------------------
An index in Azure AI Search is like a database schema or table definition

What fields your data will have (e.g. content, id, embedding)
What type each field is (text, number, vector, etc.)
Whether it should be searchable, filterable, sortable, etc.
Whether it supports vector search

Step 6 is like building the bookshelf (you decide shelf size and number of sections)
Step 7 is when you put books on it, each with a hidden "semantic barcode" (vector) for smart searching


1. [Start] Raw Documents
    |
    ‚îî‚îÄ‚îÄ> üìÑ Load documents (PDF, DOCX, TXT, etc.)
            |
            ‚îî‚îÄ‚îÄ> üß© Split into chunks (paragraphs, sentences, sliding windows)
                     - Use tools like `nltk`, `tiktoken`, or `Langchain`/`LlamaIndex` splitters
                     - Keep each chunk under the token limit (e.g., 512 tokens)

2. [Embedding] Generate vector embeddings
    |
    ‚îî‚îÄ‚îÄ> üîó Use Azure OpenAI embedding model
            |
            ‚îî‚îÄ‚îÄ> Send each chunk to `openai.Embedding.create()`
                     - Use `text-embedding-ada-002` or equivalent
                     - Get 1536-dim vector (default for ada)
                     - Output: { "chunk": "...", "embedding": [0.123, 0.234, ..., 0.456] }


3. [Azure AI Search Setup] Create the vector-enabled index
    |
    ‚îî‚îÄ‚îÄ> üß± Define index schema using Azure SDK:
            - Fields:
                - id (string, key)
                - content (string, searchable)
                - content_vector (vector, dimensions=1536, searchable via vector search)
            - Configure vector search:
                - Use HNSW algorithm
                - Define `VectorSearchAlgorithmConfiguration(name="default", kind="hnsw")`
            |
            ‚îî‚îÄ‚îÄ> ‚òÅÔ∏è Call `create_or_update_index()` using the `SearchIndexClient`
                     - This registers the index in Azure AI Search
                     - Still no data added yet


4. [Upload] Insert documents with vectors into the index
    |
    ‚îî‚îÄ‚îÄ> üì§ Construct document payloads:
            - id: unique id for each chunk
            - content: original text
            - content_vector: embedding generated from Step 2
    |
    ‚îî‚îÄ‚îÄ> üíæ Use `SearchClient.upload_documents()` to push these into Azure AI Search
            - Documents are now stored in the index
            - Search engine is now ready for both keyword and vector search


5. [Query] Run search against the vectorized index
    |
    ‚îî‚îÄ‚îÄ> üß† Take user query ‚Üí Generate query embedding (same embedding model)
            |
            ‚îî‚îÄ‚îÄ> üîç Call `search()` with:
                     - search_text=None
                     - vectors=[
                           { value: query_vector, fields: "content_vector", k: 5 }
                       ]
            |
            ‚îî‚îÄ‚îÄ> ‚úÖ Azure AI Search returns top-k most semantically similar chunks
                     - You can also mix keyword + vector search (hybrid search)


Index: The actual search database (structure + data) ‚Äî like a search table you define and populate.
Indexer: An automation component that pulls data from a data source (like Azure Blob Storage, Cosmos DB, SQL) and loads it into your index.


Automatically extracts, transforms, and loads data (ETL) from an Azure-supported data source into your search index.

Think of it like a data pipeline that handles:
Reading data (from a data source like Azure Blob Storage)
Cracking files (reading PDFs, extracting content)
Enriching with AI skills (e.g., OCR, language detection)
Embedding the results (if you configure vectorization)
Storing them in the search index

Connect to Azure Blob Storage, SQL DB, Cosmos DB, Table Storage, etc.
Automatically read, extract, and transform content (e.g., from PDFs)
Use cognitive skills (OCR, entity recognition, language detection)
Schedule refreshes (every hour, day, etc.)
Push structured content into your search index


‚ùå Skip the indexer if:
You‚Äôre manually controlling the data flow (e.g., in Python/LlamaIndex)
You want custom preprocessing (chunking, embeddings, etc.)
You‚Äôre working with non-Azure data sources (e.g., API, local files)

fields = [
    SimpleField(name="id", type="Edm.String", key=True),
    SearchableField(name="content", type="Edm.String"),
    VectorField(name="content_vector", dimensions=1536, vector_search_configuration="default"),

    # Metadata fields
    SimpleField(name="filename", type="Edm.String", filterable=True, sortable=True),
    SimpleField(name="page_number", type="Edm.Int32", filterable=True, sortable=True),
    SearchableField(name="section_title", type="Edm.String", searchable=True),
    SimpleField(name="document_type", type="Edm.String", filterable=True),
]


The text chunks are vectorized using embeddings
The metadata can be used to filter, sort, or even search alongside vectors


Define Metadata in Your Index Schema

fields = [
    SimpleField(name="id", type="Edm.String", key=True),
    SearchableField(name="content", type="Edm.String"),
    VectorField(name="content_vector", dimensions=1536, vector_search_configuration="default"),

    # Metadata fields
    SimpleField(name="filename", type="Edm.String", filterable=True, sortable=True),
    SimpleField(name="page_number", type="Edm.Int32", filterable=True, sortable=True),
    SearchableField(name="section_title", type="Edm.String", searchable=True),
    SimpleField(name="document_type", type="Edm.String", filterable=True),
]


{
    "id": "doc123_chunk5",
    "content": "Revenue increased by 15% year over year...",
    "content_vector": [...],  # embedding
    "filename": "Q1_Report_2024.pdf",
    "page_number": 3,
    "section_title": "Income Statement",
    "document_type": "Financial Report"
}


search_client.upload_documents([doc])


results = search_client.search(
    search_text=None,
    vectors=[
        {
            "value": embedding,
            "fields": "content_vector",
            "k": 5
        }
    ],
    filter="filename eq 'Q1_Report_2024.pdf' and page_number ge 2 and document_type eq 'Financial Report'"
)


also use the search text + vector together:

results = search_client.search(
    search_text="revenue growth",
    vectors=[
        {
            "value": embedding,
            "fields": "content_vector",
            "k": 5
        }
    ],
    filter="filename eq 'Q1_Report_2024.pdf'"
)


For faceted filters (like document type, categories), you can also set facetable=True
Task	How To Do It
Include metadata	Add fields in index schema with filterable=True
Add metadata to chunks	Enrich chunks with filename, page_number, etc. in Python
Upload with metadata	Use upload_documents() with full doc+metadata dicts
Query with filters	Use filter="filename eq 'X'" in search() calls

Multi-vector search per record
Native vector database features (like vector-only mode, advanced ANN tuning)



From PlayGround
---------------
[1] You connect a Blob Storage container with documents (PDF, DOCX, TXT, etc.)
       |
[2] Azure AI Search + Cognitive Search kicks in with an internal "indexer"
       |
[3] Automatic steps:
     üîπ Extract content from files (PDF parser, OCR if needed)
     üîπ Chunk the documents (default: ~1,000 characters with overlap)
     üîπ Add metadata (filename, page number, etc.)
     üîπ Generate embeddings using Azure OpenAI (e.g., text-embedding-ada-002)
       |
[4] Azure automatically creates a hidden Azure AI Search index for you
       |
[5] You map fields (content, title, vector, metadata)
       |
[6] Chatbot grounded on top-k matching chunks from this hidden index


Feature	What Azure Does
Chunking	Azure uses default internal logic (not customizable in UI yet) ‚Äî usually ~1,000 chars
Embedding model	Uses text-embedding-ada-002 or the selected model in the background
Storage	A new Search Index is created (not visible unless you check the Search service)
Metadata	Basic metadata like filename, page, and document type is extracted
Indexing pipeline	Uses the Azure Cognitive Search Indexer internally
Search	You get hybrid search by default (text + vector), unless you disable it

‚úÖ What You Should Do in Production
If you want:
Custom chunking logic (e.g., based on headers, sections)
Rich metadata (e.g., tags, authors, financial periods)
Control over chunk size/overlap
More transparent indexing process

üëâ Then build your own pipeline using:
LlamaParse + LlamaIndex
Azure OpenAI embeddings
Manual upload to Azure AI Search


Cosmos DB - Integrated Vector DB
--------------------------------

Pure Vector DB
A pure vector database is designed to efficiently store and manage vector embeddings, along with a small amount of metadata

Integrated Vector DB
The integrated vector database in a NoSQL or relational database can store, index, and query embeddings alongside the corresponding original data.
This approach eliminates the extra cost of replicating data in a separate pure vector database
Moreover, keeping the vector embeddings and original data together better facilitates multi-modal data operations

Use Case of Vector DB
---------------------
implement persistent memory for AI agents
identify data anomalies or fraudulent activities that are dissimilar from predominant or normal patterns
identify the best-fit potential options from a large pool of choices to meet complex requirements

production-level LLM caching using the Vector DB 


Azure Cosmos DB for NoSQL
Enable Azure Cosmos DB NoSQL Vector Index.
Setup a database and container with a container vector policy and vector index
Insert data into an Azure Cosmos DB for NoSQL database and container
Create embeddings from a data property using Azure OpenAI Embeddings
Link the Azure Cosmos DB for NoSQL.
Create a vector index over the embeddings properties
Create a function to perform vector similarity search based on a user prompt
Perform question answering over the data using an Azure OpenAI Completions model

Azure Cosmos DB for MongoDB

Azure Cosmos DB for PostgreSQL,


Azure Cosmos DB
Azure Cosmos DB is a globally distributed, multi-model database service


Vector Store of Cosmos DB 
Cosmos DB (NoSQL) with SQL API + Vector Indexing - Integrating with with Azure AI - native vector support 
Cosmos DB for MongoDB API  - poor as compared to pg vector 
Cosmos DB for PostgreSQL (Hyperscale) - leveages pg vector 


NoSQL 
from azure.cosmos import CosmosClient, PartitionKey

# Initialize the Cosmos client
client = CosmosClient("<COSMOS_ENDPOINT>", "<COSMOS_KEY>")
database_name = "vector_db"
container_name = "vector_container"

# Create database and container
database = client.create_database_if_not_exists(id=database_name)
container = database.create_container_if_not_exists(
    id=container_name,
    partition_key=PartitionKey(path="/id"),
    indexing_policy={
        "indexingMode": "consistent",
        "includedPaths": [{"path": "/*"}],
        "excludedPaths": [{"path": "/\"_etag\"/?"}],
        "vectorIndexes": [{"path": "/embedding", "type": "diskANN"}],
        "fullTextIndexes": [{"path": "/text"}]
    }
)

# Insert a document with embedding
document = {
    "id": "1",
    "text": "Sample text",
    "embedding": [0.1, 0.2, 0.3, 0.4]
}
container.upsert_item(document)

# Perform vector search
query = {
    "query": "SELECT * FROM c ORDER BY VectorDistance(c.embedding, @vector)",
    "parameters": [
        {"name": "@vector", "value": [0.1, 0.2, 0.3, 0.4]}
    ]
}
results = list(container.query_items(query=query["query"], parameters=query["parameters"], enable_cross_partition_query=True))
for item in results:
    print(item)


üîπ 2. Azure Cosmos DB for MongoDB vCore
Azure Cosmos DB for MongoDB vCore supports vector search using the Hierarchical Navigable Small World (HNSW) algorithm.
1.Enable HNSW Vector Index:
2.Create Collection and Index
3.Insert Documents with Embeddings
4.Perform Vector Search

from pymongo import MongoClient
from pymongo.operations import IndexModel

# Connect to MongoDB
client = MongoClient("<COSMOS_MONGO_CONNECTION_STRING>")
db = client["vector_db"]
collection = db["vector_collection"]

# Create HNSW vector index
collection.create_indexes([
    IndexModel([("embedding", "hnsw")], name="embedding_hnsw_index")
])

# Insert a document with embedding
document = {
    "text": "Sample text",
    "embedding": [0.1, 0.2, 0.3, 0.4]
}
collection.insert_one(document)

# Perform vector search
pipeline = [
    {
        "$vectorSearch": {
            "queryVector": [0.1, 0.2, 0.3, 0.4],
            "path": "embedding",
            "k": 5,
            "index": "embedding_hnsw_index"
        }
    }
]
results = collection.aggregate(pipeline)
for doc in results:
    print(doc)

3. Azure Cosmos DB for PostgreSQL with pgvector
Azure Cosmos DB for PostgreSQL supports vector search through the pgvector extension, allowing for efficient similarity searches within PostgreSQL

Enable pgvector Extension: Activate the pgvector extension in your PostgreSQL database.
Create Table with Vector Column: Define a table that includes a column of type vector
Insert Embeddings: Insert your data along with vector embeddings into the table
Create Index for Similarity Search: Create an index (e.g., ivfflat) on the vector column to optimize similarity searches.
Perform Vector Search: Use SQL queries with the <-> operator to find similar vectors

-- Enable pgvector extension
CREATE EXTENSION IF NOT EXISTS vector;

-- Create table with vector column
CREATE TABLE items (
    id SERIAL PRIMARY KEY,
    description TEXT,
    embedding VECTOR(4)
);

-- Insert a sample item
INSERT INTO items (description, embedding)
VALUES ('Sample text', '[0.1, 0.2, 0.3, 0.4]');

-- Create an index for similarity search
CREATE INDEX ON items USING ivfflat (embedding vector_l2_ops) WITH (lists = 100);

-- Perform vector search
SELECT id, description
FROM items
ORDER BY embedding <-> '[0.1, 0.2, 0.3, 0.4]'
LIMIT 5;




Introduction to Microsoft 365 Copilot 
-------------------------------------
Outline the working principles behind Microsoft 365 Copilot
Identify the core components integral to Microsoft 365 Copilot

1.Copilot is an intelligent assistant integrated into Microsoft 365 applications
2.Providing you with real-time support and insights
3.Whether you're drafting emails, creating documents, or managing your calendar, Copilot can help you work smarter and more efficiently.

Copilot - NLP  [Typing or Speaking]
  Perform tasks
  Find Information
  Provide Recommendations

Copilot - To provide personalized assistance tailored to your needs 

- Learn from your interactions, adapts to your preferences, ensuring that the support you receive is always relevant and helpful 


Copilot - Core Components 
LLM 
Microsoft Graph - Utilizing your business data from MG

Outlook. Summarize the content of a large email thread.
PowerPoint. Create a PowerPoint slide presentation based on a report.  
Word. Rewrite a paragraph in a different tone or style
Teams. Summarize meetings and chat threads

Copilot is a sophisticated orchestration engine and processor
  - It melds the strengths of LLMs with Microsoft 365 apps
  - Your proprietary business data that you as an individual user have access to within Microsoft Graph

1.Security, Compliance, and Privacy
2.Operating with real-time access to your content and context within Microsoft Graph
3.Copilot has the capacity to generate responses deeply ingrained in your business content to provide relevant answers and insights.
4.When paired with your ongoing work context - whether it's the meeting you're currently in, recent email exchanges, or last week's chat conversations

Requriements:
Microsoft Entra ID. Users must have Microsoft Entra ID accounts

Features in Microsoft 365 Copilot
---------------------------------
1.Such as file restore
2.OneDrive management

Microsoft Loop 
Microsoft Whiteboard

Office Feature Updates Task - is required for ensuring the proper installation and functionality of core Copilot experiences in apps such as Word, PowerPoint, Excel, and OneNote 


Microsoft Copilot How works 
---------------------------
1. Analyzing the Content - Meeting, drafting doc, composing email.
2. Getting the data from your data in Microsoft 365
3. Understanding the context and takes action 
4. Copilot to craft naturally phrased recommendations, ensuring that any content it generates aligns with your unique situation.
5. Copilot evaluates potential suggestions, refining them to ensure what you get is contextually relevant and specific

---------------------------
Copilot Features 

1.Enhanced meeting engagement - It also provides quick catch-ups for meetings you missed, ensuring you're always in the loop
2.Efficient email management - Copilot can help streamline your email communication by summarizing lengthy email threads and drafting responses.
3.Writing assistance - Copilot can transform your writing by drafting, editing, summarizing, and creating alongside you. This functionality can enhance the quality and efficiency of your documents.
4.Presentation development - Starting a new presentation is simplified with Copilot. You can begin with either a prompt or an outline using natural language commands, bringing your ideas to life
5.Data analysis and visualization - Copilot can help identify trends, create visualizations, and provide recommendations, thereby simplifying data analysis
6.Security and compliance
7.User Control - You maintain control over AI suggestions, deciding which to use, modify, or discard. This design ensures the human element remains at the forefront of AI interaction, with the latest enhancements providing even greater flexibility and control.


Copilot Architecture 
--------------------
1.uses your organization's data that you as an individual user have access to
For example, calendar events, 
             emails, chats, 
             documents, and 
             meetings from the 
             Microsoft Graph
2.It maps this data and relationships, providing personalized, relevant, and actionable information
3.Furthermore, communication between your tenant and Copilot components is encrypted.


Copilot Access Graph + Web + Other Services for grounding the data 
                     [Dataverse + Power Platform Services]


1. Grounding Improves the Specificity of the Prompt - to help you get answers that are relevant and actionable to your specific task. 
2. The prompt can include text from input files or other content discovered by Copilot, and Copilot sends this prompt to the LLM for processing


Explore the Core Components of Microsoft 365 Copilot 
----------------------------------------------------
Copilot to offer its insightful recommendations and suggestions.
1. Tokenization
2. Semantic Analysis
3. Sentiment Analysis - Assess the mood or emotion behind a text, Copilot can understand user intent more accurately. 
4. Language Translation

5. Microsoft Graph  - Connective Tissue that integrates all your Microsoft 365 Services and data. 
6. Microsoft 365 Copilot applies Microsoft Graph to synthesize and search content from multiple sources within your tenant. 
The Microsoft Graph API brings more context from user signals into the prompt, such as information from emails, chats, documents, and meetings. This information includes data from services like Outlook, OneDrive, SharePoint, Teams, and more.

Microsoft 365 Copilot Chat
--------------------------
1.Copilot Chat is the shared chat experience in Microsoft 365 Copilot.
2.Enabling users to leverage cross-app intelligence.

Usage of Copilot Chat 
---------------------
1.Catch up on things - Copilot Chat can synthesize and summarize large amounts of data into simple,
2.Create content and brainstorm - Copilot Chat can help you brainstorm ideas and draft new content based on anything from a storyboard or a script to an agenda or an executive summary.
3.Get quick answers - Copilot Chat enables you to act as your own personal search engine.
                Ask questions about specific files and messages 
                find information you know is out there

----------------------------------------
It's important to note that Copilot Chat differs from other Copilots available in Microsoft 365 Apps
----------------------------------------
1.Copilot Chat works across multiple apps and content, giving you the power of AI together with your secure work data.



----------------------------------------------------
Explore the possibilities with Microsoft 365 Copilot
----------------------------------------------------
1.Demonstrating how to streamline workflow and increase productivity
2.Identify and apply the key features of Microsoft 365 Copilot in each Microsoft 365 app
3.Understand how Microsoft 365 Copilot integrates with Teams to provide a centralized hub for all your work-related information.
4.Utilize the AI capabilities of Microsoft 365 Copilot to transform your approach to tasks in Microsoft 365, making them more intuitive and efficient.


Accentuating its role in:
--------------------------
Outlook: Streamline your email management by highlighting critical information, saving you time and maintaining focus. 
PowerPoint: Enhance your presentations, creating visually appealing slides more efficiently 
Excel:   Facilitate intelligent data handling by simplifying the analysis of complex datasets and the creation of reports through natural language queries.
Word:    Boost Content creation by aiding in more efficient drafting and offering insights and references to elevate your documents.
Teams (Centralized Hub):   Optimize collaboration through streamlined chats and on-demand meeting insights, making teamwork more productive. Centralize and use data from various sources to stay informed and make data-driven decisions more efficiently.


Identify and apply the key features of Microsoft 365 Copilot in each Microsoft 365 application to enhance your productivity and workflow 

Understand how Microsoft 365 Copilot integrates with Teams to provide a centralized hub for all your work-related information. 

Word - 365 [Draft with Copilot, Reference your content - add up to three existing files to your prompt that Copilot can reference when generating your document]
----------
1.Compose and Summarize documents 
2.Give Detailed instructions, including outlines, notes or file references 

i.Regenerate a new draft: Track the different version
ii.Update the current draft: partially satisfied with the current draft. 

Copilot pane - 
------------
1) Conversational interface to perform various tasks, making document creation and editing even more intuitive. 

 i.   Generate a summary of your current document.
 ii.  Adjust formatting and language within your document.
 iii. Pose questions about the content of your document. 

Linking files in Copilot in word
--------------------------------
you can have it create a document based on information in another document. 
i. Linking the file into a Copilot prompt. 
ii. Link a file stored in a OneDrive account or sharepoint site. 
iii. you can't link a file stored locally on your PC
iv. can also link files to prompts used in other Copilot applications, such as 
Copilot in Outlook, Copilot in PowerPoint, The same requirement and linking methods are also applicable in those apps. 


Facilitate Collaboration using real-time coauthoring
----------------------------------------------------
Content creation often involves collaboration with other creators, editors, and stakeholders. 

Microsoft 365 Copilot's integration within Microsoft Word facilitates seamless collaboration by allowing multiple users to access and edit the document simultaneously 

i. This real-time collaboration feature streamlines the review and approval process, ensuring that the final script meets everyone's expectations. 



----------------------------------------------------------------
Summarize and draft emails with Microsoft 365 Copilot in Outlook 
----------------------------------------------------------------
i.  It helps you understand long email threads, 
ii. See different viewpoints, and spot unanswered questions.
iii. With Copilot, you can quickly reply using prompts or turn short notes into full emails using resources from Microsoft 365
iv. you can also easily adjust your message's tone and length. 
v. Get Email Coaching


Features on outlook 
==-------======--=-=-
Thread summarization feature, 
Draft Email feature - draft Email faster, - offering suggestions context of your message

Design Captivating presentations with Microsoft 365 Copilot in PowerPoint
-----------------------------------------------
i.effortlessly convert written documents into presentation decks
ii.complete with speaker notes and references.
iii.provide either an outline or a prompt.
iv. Beyond creation, it also offers tools to streamline long presentations and uses intuitive commands to refine layouts, adjust text formatting, and synchronize animations.

""Add an image of a lakeside campfire" can infuse it with vibrancy""

Analyze and transform data with Microsoft 365 Copilot in Excel
--------------------------------------------------------------
i.Copilot can help you find pattern
ii.explore 'what-if' scenarios, or get new formula suggestions

"""""Are there any outliers in my data?"
"Find and plot the correlation between marketing spend and sales revenue."
"Help me forecast next quarter's sales based on current trends.""""""

Elevate productivity with Microsoft 365 Copilot in Teams
---------------------------------------------------------
i.facilitating seamless conversations and more productive meetings
ii. It helps in keeping you updated with discussions, summarizing key actions, and organizing focal points of conversations to promote a streamlined workflow within your team
iii.It enables you to extract meeting agendas from chat history, identify suitable individuals for follow-ups, and schedule ensuing meetings effortlessly.
iv.Copilot in Teams helps users capture and share the key points, action items, and outcomes of their online meetings
v.It's available in both chat threads and meetings. During a meeting, Copilot can summarize key discussion points, suggest action items, and identify where people are aligned or disagreeing in real-time
vi.Copilot in Teams can also get users caught up on missed meetings by providing a summary of a meeting‚Äôs key points and outcomes.
vii.After a meeting, Copilot can generate meeting notes, list action items, suggest follow-up questions, and more.



Copilot Studio Overview - Copilot Studio is a graphical, low-code tool designed for building agents and agent flows.
- It allows users to create custom agents that can automate and execute various business processes
- such as help desk support, change management, and managing meeting guests.
- The platform offers the ability to connect to other data sources using prebuilt or custom plugins
-----------------------
Graphical Low-Code tool for building agents and agent flows

Copilot Studio can be used in various scenarios, including sales support, employee inquiries, and public health tracking. 
Additionally, it allows for the creation of agent flows that automate repetitive tasks and integrate with other applications and services


Create 
Agents
Library

Copilot for Microsoft 365

Start with a Template 
----------------------
Safe Travels Agent
Team Navigator Agent
Store Operations
Website Q/A 
IT Helpdesk Agent
Case Management Agent 

Features 
--------
--> Copilot Studio is its ability to connect to other data sources using either prebuilt or custom plugins.
--> Flexibility - Orchestrate sophisticated logic 

Agent
-----
-> Companion that can handle a range of interactions and tasks for resolving issues 
-> Requiring complex conversations to autonomously determining the best action to take based on its instructions and context. 
-> Agent Coordinate a collection of language models, along with instructions, context, knowledge sources, topics, actions, inputs, trigger to accomplish your desired goals.

Agents can engage 
    websites
    mobile apps
    Facebook
    Microsoft Teams
    Any channel supported by Azure Bot Service 

Agents Use
----------
Sales help and support issues
Opening Hours and store information
Employee health and vacation benefits
Public health tracking information
Common Employee questions for businesses

Agents can be used on their own 
Extend Microsoft 365 Copilot with enterprise data and scenarios. 


Agent Flow  - create by natural language, visual editor 
----------
- Automate Repetitive tasks and integrate your apps and services
- Agent flows can be triggered manually, by other automated events or agents or based on a schedule 
- 

Access Copilot Studio 

---------------------
Standalone web app  - IT admin who wants to create agents to perform tasks or interact with customers 
i. Explore Advanced agent concepts
ii. such as entities and variables 
iii. create complex agents 
Discrete app within teams. - 
i.wants to use agents to answer common employee questions
ii.advanced concepts such as entities and variables and have an internally       available agent in Teams. 
iii.you want to create and distribute an agent in the shortest time possible 


Plan your agent
---------------
Extend Microsoft 365 Copilot with an agent 
 - you want to craft your own agent by declaring instructions, actions, and knowledge to customize Microsoft 365 Copilot for specific tasks and domain knowledge 
 - you wish to utilize the existing Copilot Orchestrator 
 - you want a standalone custom version of the Microsoft 365 Copilot chat experience 
 

Create an Agent - Describe the agent you want in plain language to create it  
----------------
-> Tell copilot studio what specific instructions, topic trigger, knowledge sources and actions you want for your agent. 
-> Test your agent before you apply. 
-> Publish your agent when you're ready across multiple channels

Add actions[Tool] - So your agent can do things for you 
Build Topics - To focus and guide how your agent answers
Publish your agent - so others can use it 

- You want an agent that can integrate company data and documents, retrieve real-time data from external APIs, take actions in response to external events, and be
 
Consider creating an agent if:
------------------------------- 
-> agent that can integrate company data and documents. retrieve real-time data from external API's 
-> take actions in response to external events, 
-> Embedded in company applications

-> Customized end-to-end solution for your web or mobile app or automation workflow that meets specific business needs and allows for complete control over product branding 

--> Surface your agent to other agents as their supported agent extension

--> Customer service chatbot for your e-commerce site 
--> Virtual assistant to schedule appointments for your healthcare service 
--> Gaming experiences that incorporate generative AI 


AI-Based agent authoring overview [Turn on generative Orchestration - Let the agent select the most appropriate topics, actions and knowledge source at runtime
----------------------------------
-> Copilot studio offers generative AI features to reduce manual authoring and daramtically expand the scope of an agent's knowledge and its ability to interact with users. 
-> 


i. Generative answers -  from multiple sources, internal or external, without created topics
ii. Generative answers - can be used as primary information sources or as a fallback source when authored topics can't answer a user's query. 
iii. You don't need to manually author multiple topics that might not address all customer questions.


create individual topics for
create individual topics for frequently asked questions. --> Take the questions from analytics from previous agents

iv. In addition to generative answers, you can use AI general Knowledge 


AI Features for Teams and Classic Chatbots 
------------------------------------------
 Copilot Studio app 
        Teams
        Classic Chatbots in the copilot studio web app. 


AI Models in Copilot Studio 
---------------------------
    chat transcript mining
    Interpretable ML 
    Machine Translation
    Reinforcement Learning
    Automatic Suggestion
    Prebuilt Entities 
    Custom Entities 
    Context Engine
    Dialog Management 

Copilot studio hosts multiple AI models and AI capabilities on a single service 



New Changes in Copilot Studio 
-----------------------------
Autonomous agents
Model Context Protocol
Generative Orchestration 
Deep Reasoning Models
Build Agent flows 

Hebrew (he-IL) Language support 
 - interaction between agents and users 
 - including support for generative answers, for text only 

Support for more enterprise data sources using Microsoft Graph connectors


Standalone Copilot Studio subscription 
--------------------------------------
Allows you to build agents on any supported channel and connect to any data using  premium connectors
->Orchestrate agent topics and actions with generative AI 
->Can extend Copilot Studio copilots with Microsoft Bot framework sills
->Chat add-ons for Dynamics 365 Customer Service 



Copilot Studio for Microsoft Teams plans
----------------------------------------
->customers to build conversational interfaces within teams. 
->agents can use data stored in Microsoft Data verse for teams or many other sources. 
->Capabilities available in the Copilot Studio app in teams are available as part of select Microsoft 365 copilot subscriptions with Microsoft Power Platform and teams capabilities 

Create and Deploy an Agent
---------------------------
Add Knowledge to your agent,
Test content changes in real time, deploy your agent to a test page you can share with others

Steps to Create Agent in Microsoft Copilot 
------------------------------------------
1. Create - Describe the Agent 
2. It will add the NAME
                   INSTRUCTIONS
                   DESCRIPTIONS
                   KNOWLEDGE SOURCES
3.Test the Agent
4.Publish the Agent 


Explore Lesson Topics in Classic Chatbots
-----------------------------------------
Classic chatbots created in the Microsoft copilot studio app for Microsoft teams

-> Include sample topics with every new agent.
   Conditional Branching, variables and custom entities 

i.Lesson Topic is designed to teach you how to use the authoring canvas to create basic and advanced bot conversations.


Upgrade to Copilot Studio Unified Authoring
-------------------------------------------
Classic Power Virtual Agents Experience to the Latest version of Copilot Studio. 


  New Agent 
  ---------
      Schema name for new topics
      Schema name, it directly utilizes the prefix from the selected solution

     Topic Creation: 
     Trigger Tags: all types of Triggers within the topic view and different triggers due to the events feature, such as Message Received
     Connectors within a Topic: Previously the connectors between nodes were curved. 
     Topics User Interface - 
     Variable Watch Window - 
     Flyout menu 

System Topics [agent automatically starts using the Conversation Start topic data]
     - the conversation Start system topic begins the conversation automatically 
       and is turned on by default. 

Multiple Topics Matched: 
Agent author/maker can then retrieve that object from Power Automate:
Variables, variable watch window improvements, and testing
       -> agent makers can include Power Fx formulas to manipulate data and perform calculations within the runtime of Copilot Studio.
       -> When testing, it's critical to be able to test variables within the topic process flow when creating conversational experiences.
     
Write your agent in YAML
------------------------
capabilities to author conversational experiences that use both the graphical user interface (GUI) authoring tool and code view
Developers can switch to the code view within a topic, in real-time, to directly build or edit the YAML referenced by the interface.

Events - 


Key Concepts - Authoring Agents
-------------------------------
-> An Agent Topic is a portion of a conversational thread b/w a user and the agent.
-> Topics are linked together to form nodes. 
-> 


Routing anchors



Use Entities and slot filling in agents 
--------------------------------------
-> Identify the Entities in a user dialog. 

Closed List Entities 
Regex Entities 


Slot filling - Slot filling is a natural language understanding concept that means saving an extracted entity to an object[in copilot stores in variable]

where the user can specify multiple pieces of information that map to multiple entities
  
Create and Delete Agents
-------------------------
Copilot Studio lets you create an agent, using built-in content building blocks containing topics, trigger phrases, and pre authored agent conversations.

Test your agent
---------------
-> you can turn on tracking between topics, follow the agent conversation step by step
-> If the text is similar to a trigger phrase for a topic, that topic begins


Install featured agents from Microsoft (preview)
-------------------------------------------------
Microsoft provides a catalog of featured agents you can browse and install from within Copilot Studio


Trigger Phrase
  ‚îî‚îÄ‚îÄ Topic: Check Order Status
       ‚îú‚îÄ‚îÄ [Message] "Sure, I can help with that."
       ‚îú‚îÄ‚îÄ [Question] "What's your order number?" ‚Üí userOrderNumber
       ‚îú‚îÄ‚îÄ [Message] "Thanks! Looking that up..."
       ‚îú‚îÄ‚îÄ [Call an action] ‚Üí Get order status from API
       ‚îî‚îÄ‚îÄ [Condition]
            ‚îú‚îÄ‚îÄ If orderStatus exists
            ‚îÇ     ‚îú‚îÄ‚îÄ [Message] "Your order is {orderStatus}..."
            ‚îÇ     ‚îî‚îÄ‚îÄ [Message] "Anything else I can help with?"
            ‚îî‚îÄ‚îÄ Else
                  ‚îî‚îÄ‚îÄ [Message] "Couldn‚Äôt find that order number."


Bot Enhancements (via Plugins & Actions)
‚îú‚îÄ‚îÄ Plugins
‚îÇ   ‚îú‚îÄ‚îÄ Pre-built Integrations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Microsoft 365 (emails, calendars)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Dynamics 365 (CRM data)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Custom APIs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Provide OpenAPI (Swagger) definition
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Configure authentication
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Test & validate connection
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ Output
‚îÇ       ‚îî‚îÄ‚îÄ Expose operations to Copilot Studio (e.g., GetOrderStatus, GetEmployeeInfo)
‚îÇ
‚îú‚îÄ‚îÄ Actions
‚îÇ   ‚îú‚îÄ‚îÄ Type 1: Power Automate Flows
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Trigger from inside a Topic Node
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Inputs: variables (e.g., userOrderNumber)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Steps: Call service, process logic
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Outputs: results (e.g., orderStatus, deliveryDate)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Type 2: Plugin Actions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Choose operation from a Plugin
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Pass input variables & capture output
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ Use in Topics
‚îÇ       ‚îú‚îÄ‚îÄ Call Action Node
‚îÇ       ‚îú‚îÄ‚îÄ Save output to new variables
‚îÇ       ‚îî‚îÄ‚îÄ Display results using Message Node



---------------------------------------
API Integration

GET - Retrieve
POST - Create a new resource
PUT  - Update
DELETE - Removes 
PATCH - Partially Update 

Request Anatomy
   URL{Base URL} - Uniform Resource Locator - address & access to resource with parameters 
   URI Uniform Resource Identifier - specified in the URL to specify which client resource the client would like to access in a request 
   Parameters - PATH Parameter   - pointing towards specific resource
                QUERY Parameter  - helps in querying/filtering through a list of resources 
   Headers - sending extra information in a request, such as authentication tokens, and content types 
             Authorization: 
             Accept: desired response format 

2XX: Success
3XX: Redirection
4XX: Problem with the client 
5XX: Problem with the server 


39 Different HTTP[Hyper Text Transfer Protocol] method 
Top 9 Method
GET Method
POST Method 
PUT  - update
PATCH - partial data to update partially
DELETE - 
HEAD - allows you to retrieve metadata from a server 
OPTIONS - allows you to describe the communication options for a resource 
TRACE - allows you to trace the communication path between a client and server 
CONNECT - allows you to establish a tunnel to a server 


HTTP methods into two main categories:
-> Safe HTTP Methods - doesn't change data on the server. It always returns the same request, no matter how many times it gets called. Eg: GET, HEAD
-> Idempotent Methods - It may change data on the server, it always returns the same response, no matter how many times it gets called. Eg: GET, HEAD, PUT, DELETE and Trace 

Options Method - This method is used to get information about the possible communication options(permitted HTTP methods) for the given URL or an asterisk to refer the entire server  
    Browser widely use the Options method to check whether the CORS(Cross-Origin Resource sharing) operation is restricted on the targeted API or not. 

     - we can test the server for the times of FATAL failure with the options method

output check - check the header and the status code that returns 
               Test the case of failure with a resource that doesn't support the options method


Trace[make http request like a get request - api status] - The Trace method is for diagnosis purposes, it creates a loop-back test with the same request body that the client sent to the server before, and the successful response code is 200 
      - could be dangerous because it could reveal credentials. A hacker could steal credentials. including internal authentication headers, using a client-side attack 

Connect method - is for making end-to-end connections between a client and a server. It makes a two-way connection like a tunnel between them. 
           we can use this method to safely transfer a large file b/w the client and the server. 



Azure Logic Apps 
-----------------
Connectors[Operations] 
Cutsom Connectors [OpenAPI description for the API]
      Connector a name, an icon, and a description for each operation
Trigger, action, and control action
  Trigger - A trigger is an event that happens when a specific condition is met
          - Triggers automatically activate or fire when the condition is satisfied. For example, when a timer expires or data becomes available.
  Action  - An action is an operation that executes a specific task in your workflow
A connector is a group with all related triggers and actions
Most connectors offer both triggers and actions

To build a workflow, you choose operations, specifically, a trigger and actions
Triggers and actions are essentially calls to an underlying API operation
Each operation has inputs and outputs
Control actions are special actions built into Azure Logic Apps that provide workflow control constructs
 -The Condition action, which is controlled by a Boolean expression.
 -The Switch action, which is controlled by cases and a default case.
 -The Until and For each loop actions, which repeat actions and are controlled by Boolean expressions.
 -Unconditional parallel branch instructions.


Four Criteria to Evaluate 
 1. Integration required
 2. Latency 
 3. Control 
 4. Connectors 



Azure App Service Functions 
---------------------------
App Service Authentication and Authorization
Configure app Settings
Scale Apps, 
Use Deployment Slots 

App Service Key components and value. 
Azure App Service manages Authentication and Authorization.
Methods to control inbound and outbound traffic to your web apps. 
Deploy an app to App Service using Azure CLI commands 

Azure App Service is an HTTP-Based service for 
 -> Hosting web applications, 
 -> REST API's and 
 -> Mobile back ends. 

1.Can Develop in various programming language or framework. 
2.Depending on the usage of the web app, you can scale the resources of the underlying machine that is hosting your web app
3.Container Support - you can deploy and run containerized web apps. 
                      you can pull container images from a private Azure Container Registry or Docker hub. 
                    - Azure App Service also supports multi-container apps, 
                      Windows containers and Docker Compose for orchestrating container instances
4.CI/CD - Azure DevOps services, GitHub, Bitbucket, 
        - Connect your web app with any of the above sources and App Services will do the rest for you by autosyncing code and any future changes on the code into the web app. 
        - Continuous integration and deployment for containerized web apps is also supported using either Azure Container Registry or Docker Hub
5.Deployment Slot - When deploying a web app, you can use a separate deployment slot instead of the default production slot when you're running in the Standard App Service Plan tier or better.
6.Deployment slots are live apps with their own host names. App content and configurations elements can be swapped between two deployment slots, including the production slot.
7.App Service can also host web apps natively on Linux for supported application stacks. It can also run custom Linux containers (also known as Web App for Containers).
8.App Service on Linux supports many language specific built-in images.
9.Volume 
      - When deployed to built-in images, your code and content are allocated a storage volume for web content, backed by Azure Storage. The disk latency of this volume is higher and more variable than the latency of the container filesystem. Apps that require heavy read-only access to content files might benefit from the custom container option, which places files in the container filesystem instead of on the content volume.


App Service Environment: 
App Service Environment is an Azure App Service feature that provides a fully isolated and dedicated environment for running App Service apps

Command 
- az webapp list-runtimes --os-type Linux


Doubts:
1. To Run the Containerized  application in windows - where the compute will used to process the app, and where the storage of artifacts, and storage from the application eg: API application have different storage services 
2. In YAML file we have storage option right, - what purpose that storage/volume will be used 
3. Azure Web App containerized means, compute will autoscale -? or have to mention in YAML file to use the compute resource, and for  orchestration scaling ? 
4. How Exactly Scaling and Scaling out happens in Azure web app, 
   eg. apps run in all 5 VM instances, every time
       or 
       Whenever user overloaded, at that time app instance will deploy to the given no.of VM's  


Doubts to Check 
---------------
Recap the Docker and Kubernetes 
 Specially 
     1) How we creating the Image - Dockerfile 
     2) How we creating the container 
     3) Docker need to be installed in all the compute to run the container 
     4) Building the Container using the YAML file 
     5) How we Orchestrating the container in Kubernetes - do we need the compute 
     6) What is Pod 


       
    


 







Work Explanation   - assess ESG risks 
----------------
ESG Insurance - Refers to insurance products and services that incorporate Environmental, Social, and Governance (ESG) principles into their design, underwriting, pricing and management 

Environmental : How company maintains the environment
Social: How company maintains the Social 
Governance: How it is governed(business ethics, transparency, 
    
Property insurance 
Liability Insurance 

If a company pollutes heavily and has bad labor practices, the insurer might charge them more, limit coverage, or even refuse insurance 
- Environmental Data 
    Carbon footprint  - 
    Energy Consumption - 
    Waste Management Practices -  
    Water Usage and Management 
    Pollution History 
    Climate change exposure 
  
- Social Data 
    Labor practices 
    Diversity, Equity, and Inclusion (DEI) metrics
    Community Engagement 
    Customer satisfaction 
    Human Right Policies 

- Governance Data (G) 
    Board Composition(Independence, Diversity)
    Transparency and Disclosure Practice 
    




A2A Protocol 
------------
Context Agentic AI Refers to Agent-to-Agent, an open communication protocol introduced by Google in April 2025

- This protocol is designed to enable seamless inter operability between autonomous AI Agents 
- Ability of computer systems or software to exchange and make use of information
- Allowing them to collaborate across diverse platforms, vendors and frameworks


Agent-to-Agent - is a standardized protocol that facilitates communication and coordination between AI agents. 
               - it allows agents to discover each other's capabilities, exchange information securely, and manage tasks collaboratively, regardless of their underlying technologies or organizational boundaries 


A2A operates on several key principles:
  Agent Discovery: Agents Publish an "Agent Card" in JSON format, detailing their capabilities, endpoints, and other metadata. 
                   This enables other agents to discover and interact with them dynamically.
  Task Management: Agents can delegate tasks to one another, track progress, and handle long-running operations through structured communication patterns. 

  Security: A2A incorporate enterprise-grade authentication and authorization mechanisms, ensuring secure interactions between agents.

While A2A focuses on external agent communication. 
      it complements Anthropic's (MCP), which standardizes how individual agents interact with tools and data sources internally. 

1) --> A2A is a common Language of AI agents to talk to each other
2) --> Each agent operates with defined roles(planner,executor,validator)
3) --> Agent Registry - sort of directory where agents can find each other and initiate collaboration
4) --> It supports structured protocols for communication; think intent-sharing, task assignment and negotiation
5) -> LangChain, CrewAI, AutoGen are already enabling LLM-based agents to work together. 
      But are they just toolkits ? Do they go for enough to define a standardized, cross-platform protocol ?



IBM Agentic Core Platform 
-------------------------
 Positioned as a foundation for Building and governing agent-based applications within enterprise environments. 
 ACP more focused on control and orchestration than on enabling open, dynamic agent-to-agent communication across systems. 

Meta - Academic Project - OAA  - Open Agent Architecture 
--------------------------------------------------------
Have touched on agent inter operability - but have any of them proposed something as holistic and open-ended as A2A


Model Context Protocol 
----------------------
Universal connector for AI applications, 
 -> allowing them to access and utilize external data and tools without the need for custom integrations, 
 -> By standardizing the way AI models communicate with other systems, MCP simplifies the development process and enhances the capabilities of AI Agents 
 

MCP - Client-Server Architecture 
MCP Clients: These are AI Applications or agents that initiate requests for data or actions
MCP Servers: These expose external tools, data sources, or services to the AI Agents 

-> Communication b/w clients and servers is facilitated using JSON-RPC 2.0, a lightweight remote procedure call protocol. 
-> This allows structured exchanges
      Resource Access: Retrieving documents, database or other data sources 
      Tool Invocation: Executing functions or operations provided by external services 
      Prompt Template: Utilizing predefined templates to guide AI response 

MCP - Supports - Progress tracking 
                 Error reporting
                 Logging 

MCP into its Agents SDK
ChatGPT desktop application
Google Deepmind: MCP in its Gemini Models and related infrastructure 

-----------
MCP 
   Prompt 
   Resource 
   Context 
   Tool 

1) Create MCP tool for the Retrieval 



*** All this will bring along a composable and cohesive end to end platform for Agentic AI workflows and infrastructure****




AI Agents vs Agentic AI 
-----------------------
AI Agent - Large models answering prompting to intelligent agent acting with autonomy, memory and purpose.
Agentic AI - Where systems aren't just reactive but can plan, reason, and collaborate like digitial teammates.

How will these agents actually work together in a scalable, structured way ?


- Could Enterprise workflows be executed by a mesh of agents spanning HR, finance, compliance and IT-without human micromanagement 
- Could Healthcare agents work together to triage, analyze and personalize patient care across systems 
- Could researchers deploy agent teams that autonomously review literature, design experiments, and even collaborate across institutions 


How API revolutionized system interoperability,


        -------------------** End - AI Agen**----------------


---------------------------------------------------------------------
Azure ML 
Azure Prompt Flow 


Orchestrate agent topics and actions with generative AI 

About Copilot 
-------------
Build Own Custom AI with Microsoft Copilot 
Build Own Copilot with with Copilot Studio 
Build Agent with Extending the M365 Copilot Orchestration 
Build Custom Agents Created in Copilot Studio directly in M365 Copilot Chat 
Agent you created in Studio can be used in teams or M365 

Microsoft 365 Copilot Chat
Teams Chat 


Azure AI Models
---------------
Financial Models - https://ml.azure.com/models/financial-reports-analysis-v2/version/1/catalog/registry/azureml?wsid=/subscriptions/18785ec2-502b-4018-9ddc-73bad18cdbd2/resourcegroups/bfsi-ani-aqua/providers/Microsoft.MachineLearningServices/workspaces/aqua_ankur1&tid=404b1967-6507-45ab-8a6d-7374a3f478be
MAI -DS - https://ml.azure.com/models/MAI-DS-R1/version/1/catalog/registry/azureml?wsid=/subscriptions/18785ec2-502b-4018-9ddc-73bad18cdbd2/resourcegroups/bfsi-ani-aqua/providers/Microsoft.MachineLearningServices/workspaces/aqua_ankur1&tid=404b1967-6507-45ab-8a6d-7374a3f478be

https://learn.microsoft.com/en-us/azure/ai-services/openai/


Azure Notebooks 
----------------
ML RAG - https://github.com/Azure/azureml-examples/tree/main/sdk/python/generative-ai/rag/notebooks


Doubts
------
How I'm going to rewrite the query 
How I'm going to use the metadata to filter based on page range in LlamaParse 
How I'm going to use the LlamaParse Effectively, check the content 

Doubts in Copilot 
-----------------
https://learn.microsoft.com/en-us/microsoft-copilot-studio/nlu-gpt-overview

AI-based agent authoring
  analytics
  generative Orchestration 

Get Started
   Microsoft Power Platform Licensing Guide 




Meetings to Attend
-------------------
April 24 - Scaling Vector DB Operations with MongoDB and Voyage AI 
April 24 - Data Modeling for GenAI Apps , DataStax
April 29 - AI Database Comparison
           MongoDB vs PostgreSQL and PgVector 



April 23 - Domain-Specific Agents 
           12PM SGT


Links 
Courses to Learn - https://www.microsoft.com/en-us/microsoft-copilot/microsoft-copilot-studio
https://www.microsoft.com/en-us/microsoft-copilot/microsoft-copilot-studio
https://www.microsoft.com/en-us/microsoft-copilot/microsoft-copilot-studio

----------------- 
https://learn.microsoft.com/en-us/training/paths/power-virtual-agents-workshop/
https://learn.microsoft.com/en-us/training/modules/power-virtual-agents-create-online-workshop/?source=recommendations
https://learn.microsoft.com/en-us/training/modules/copilot-declarative-agent-challenge-chat-with-docs/ - Workshop 
https://learn.microsoft.com/en-us/training/paths/build-plugins-connectors-microsoft-copilot-microsoft-365/
https://learn.microsoft.com/en-us/training/modules/copilot-declarative-agent-challenge-chat-with-external-data/
https://learn.microsoft.com/en-us/training/modules/copilot-declarative-agent-challenge-chat-with-data/
https://learn.microsoft.com/en-us/training/modules/copilot-declarative-agent-challenge-chat-with-docs/
https://learn.microsoft.com/en-us/viva/learning/academy-copilot#prerequisites - Copilot Academy 

https://learn.microsoft.com/en-us/training/modules/introduction-microsoft-365-copilot/ - Done
https://learn.microsoft.com/en-us/training/paths/get-started-with-microsoft-365-copilot/

-----
https://learn.microsoft.com/en-us/training/paths/build-plugins-connectors-microsoft-copilot-microsoft-365/
https://learn.microsoft.com/en-us/training/paths/get-started-with-microsoft-365-copilot/

Dynamics 365 - https://learn.microsoft.com/en-us/dynamics365/release-plan/2024wave1/sales/microsoft-copilot-sales/use-copilot-studio-extend-copilot-sales

What's New in the Copilot Studio - https://build.microsoft.com/en-US/sessions/BRK140?source=sessions
Reimaging the Software Developement - https://build.microsoft.com/en-US/sessions/BRK100?source=sessions
AI Agent Factory - https://build.microsoft.com/en-US/sessions/BRK155?source=sessions
Architecting the Multi Agent Solution - https://build.microsoft.com/en-US/sessions/BRK176?source=sessions
Windows Copilot Runtime - https://build.microsoft.com/en-US/sessions/BRK223?source=sessions

Microsoft Build - https://build.microsoft.com/en-US/sessions?filter=topic%2FlogicalValue%3EAI%2C+Copilot+%26+Agents


Links for Query Search 
https://learn.microsoft.com/en-us/azure/search/hybrid-search-how-to-query?source=recommendations
https://learn.microsoft.com/en-us/azure/search/vector-search-ranking?source=recommendations

Link To Choose the Copilot 
https://adoption.microsoft.com/en-us/copilot/

Link To Which Copilot 
https://learn.microsoft.com/en-us/copilot/microsoft-365/which-copilot-for-your-organization#microsoft-copilot-studio

Link To Learn Agents in Copilot 
https://learn.microsoft.com/en-us/training/paths/work-power-virtual-agents/?source=recommendations
https://learn.microsoft.com/en-us/microsoft-copilot-studio/nlu-gpt-quickstart?source=recommendations

Link to Microsoft Fabric 
https://learn.microsoft.com/en-us/shows/learn-live/microsoft-fabric-learn-together/?ocid=fabric25_learntogether_wwlblog_azdata&wt.mc_id=certsustainedmkt_portfolioupdate_blog_wwl
https://learn.microsoft.com/en-us/training/courses/dp-700t00?wt.mc_id=certsustainedmkt_portfolioupdate_blog_wwl
https://learn.microsoft.com/en-us/credentials/certifications/resources/study-guides/dp-700?wt.mc_id=certsustainedmkt_portfolioupdate_blog_wwl

Link to ML RAG 
https://learn.microsoft.com/en-us/azure/machine-learning/how-to-use-pipelines-prompt-flow?view=azureml-api-2


LLM Related - Financial Domain
https://meisinlee.medium.com/llm-related-resources-for-the-finance-domain-210413bbae0a

Preprocessing PDF 
Amazon Nova - https://forum.uipath.com/t/how-to-extract-manipulate-a-pdf-table-with-no-borders/553838/2
https://www.google.com/search?q=Docling+Open+source+library&rlz=1C1GCEA_enIN1050IN1050&oq=Docling+Open+source+library+&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIHCAEQIRigAdIBCDkzNDRqMGo3qAIAsAIA&sourceid=chrome&ie=UTF-8


Copilot Resume
https://learn.microsoft.com/en-us/training/modules/explore-possibilities-microsoft-365-copilot/7-empower-employees-microsoft-copilot
https://learn.microsoft.com/en-us/training/paths/design-dream-destination-ai/
https://learn.microsoft.com/en-us/training/modules/create-draft-content-with-microsoft-copilot-microsoft-365/
https://learn.microsoft.com/en-us/search/?terms=copilot%20studio


-------------------------------------****Research*****------------------------------------
Agentic AI Memory Research Paper
--------------------------------
Brain Functionalities 
    
   L1 - Well Developed in Current AI 
   L2 - 


Advances and Challenges in Foundation Agents 
---------------------------------------------
L1 - Well-Developed in current AI 
L2 - Moderately explored, with partial progress 
L3 - Rarely explored, significant room for research 


Executive Control & Cognition           - Frontal Lobes 
         Inhibitory Control (L3)
         Self-awareness (L3)
         Contextual Memory & Emotional Coloring (L3)
         Emotional Processing (L3)
         Empathy (l3)
         Cognitive Flexibility (L3)
Spatial Processing and Multisensory     - Parietal Lobes 
         Tactile Perception (L3)
Visual Processing                       - Occipital Lobes 
Language Memory and Auditory Processing - Temporal Lobes 
         Motivational Drives (L3)
Reflex Activities                       - Brain Stem
         Autonomic Regulation - (L3)
         Arousal and Attention States (L3)
Coordination and Motor Learning         - Cerebellum
     L3 - Cognitive Timing and Predictive Modeling(L3)
     




AI Agent - Meta, Yale, Standford, DeepMind, Microsoft
-----------------------------------------------------
AI Agent - Perception
           Memory 
           World Modeling 
           Reasoning
           Planning
             - To Human Brain 


Perception - The ability to notice or understand something.
           - A particular way of looking at or understanding something; an opinion.

Core Components of an Intelligent Agent
    - 1. Cognition[the process by which knowledge and understanding is developed in the mind.]
            - Learning Objectives, structured and unstructured reasoning, planning frameworks 
            - Mental action or process of acquiring knowledge and understanding through thought, experience, and the senses.
            - It encompasses all aspects of intellectual functions and processes such as: perception, attention, thought, imagination, intelligence, the formation of knowledge, memory and working memory, judgment and evaluation, reasoning and computation, problem-solving and decision-making, comprehension and production of language. Cognitive processes use existing knowledge to discover new knowledge.
            - Cognitive processes are analyzed from very different perspectives within different contexts
     
    - 2. 

Research 
https://arxiv.org/pdf/2504.01990 - ADVANCES AND CHALLENGES IN FOUNDATION AGENTS
https://arxiv.org/pdf/2503.16252 - Fin-R1: A Large Language Model for Financial
Reasoning through Reinforcement Learning





Azure Cloud Native Solution
---------------------------
Azure Bus Service 
Azure Function App
Azure Web App
Azure MQ

For Developing the Document Uploading from front end to backend 
What are the Azure services I will use

DSA Algorithm Continue 
----------------------
Finding the Largest Number 
Transferring the Zero to all last 

DSA Algorithm 
Low Level  Algorithm
Medium Level Algorithm


Project Work 
-------------
1) Data Engineering Pipeline 
   Data Ingestion Pipeline 
   Data Preprocessing Pipeline
   Data Loading to VectorDB Pipeline 

   Using the MCP Pipeline for All the Vector Store 

LlamaParse vs Document Intelligence 
-----------------------------------


Markitdown
---------
https://github.com/microsoft/markitdown


Cosmos DB 
MongoDB-compatible database service
Code for Langchain - Cosmos DB
https://python.langchain.com/docs/integrations/vectorstores/azure_cosmos_db/


1) Complex PDF - Markdown Format - Structuring with Metadata - 


Completely Understanding the LlamaParse 
---------------------------------------


Document Intelligence vs Llamaparse 
-----------------------------------

1) No SOTA vision Transformer, 
2) No markdown format response 
3) 

Limitations of Document Intelligence usage 
------------------------------------------
1) No SOTA vision Transformer, 
2) No markdown format response natively




Project Issue 
--------------  [All Sections Chatbot] [Agentic RAG Approach]  [How many agents] 
1) Getting the User Query - Classify the User Query - and Decompose the User Query and Choose the Specific Prompt, Send the User Query one by one or Totally to - Retrieval 
      1.1 One by One - Retrieval Redundant 
                How to cancel the duplicate and get the original document , How to maintain the order of document 
      1.2 Total Decompose Query - How to get the proper document, based on metadata hybrid search 
                      in decomposing the query itself we will get the metadata 
      
2) Next Query - follow up on first query - How to use the same context or memory , without going search the Retrieval 
3) So How to maintain the memory - Which are the memory we need to maintain 
4) How we are going to maintain the prompt, or maintaining the prompt template from the MCP server 
5) How to use the MCP server to get the resource, prompt, tool[vector DB] etc
6) I Need to Separate the Retreival and LLM Call separately to Create the Agent, and pass the Retrieval as a Tool 



                        Take History
                        Take Question from User 
                        Create a Standlone Question 



1) Reasoning over and over, or asking questions to the user until it is clear for the LLM 
2) Now what are the messages we have to store 
                               prompt, context, results, question & answer, asking questions to the user and getting the answer 


Project Tested
--------------
1) Have tested the conversationchain and memory storing 
2) Sometime we need to use the conversationchain & conversationRetrievalChain [It needs to decide based on the question]




Flow of Message 
  System 
  User Query 
  Place Holder 

  Tool Selected 
 
  result: function_call

  call the function 
 
  observation 

  now result + observation =--> format to the openAI function 

intermediate_steps.append((result, observation))
Finally add the message 
  result2 = chain.invoke({
    "input": "what is the weather is sf?", 
    "agent_scratchpad": format_to_openai_functions([(result1, observation)])
})


Project Proceed
---------------
1) Routing the Prompt Template and Decomposing the query based on that in one chain and Retrieval Tool Based on the UI Keyword
2) Agentic RAG 
         X Agent 1 - Using Chain - Decompose the User Query Based on the Specific Prompt template -
           Storing that chain for that section, to decompose the user query with that specific prompt template  
         Retrieving  the Document - X Agent 2 - Retrieval tool only - To Select the Top Documents 
         Agent 3 - Generating the Answer for the Response Generation Prompt + Documents + User Query - How to store the memory for this response 
                           
So 

Key Point
---------
This chat bot is different , user will use all the section
Other Business chat bot is different, based on user query it will route the different business use case 



Tested
------
Per Session Memory for all the sections - storing the chain & memory for per section to use 




Agent SetUp
-----------
CoT
  1.input query 
  2.How it's taking that input and processing 
  3. First Sequence to LLMChain - 
                       Input 
                       Agent Scracth Pad
                       Stop Sequence 

   4. Final Prompt 
You are an agent designed to write and execute python code to answer questions.\nYou have access to a python REPL, which you can use to execute python code.\nIf you get an error, debug your code and try again.\nOnly use the output of your code to answer the question. \nYou might know the answer without running any code, but you should still run the code to get the answer.\nIf it does not seem like you can write code to answer the question, just return \"I don't know\" as the answer.\n\n\nPython REPL: A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\n\nUse the following format:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [Python REPL]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n\n
  5. Gives the Action and Action Input 
  6. Gives the observation from the tool 
  7. Now Call the LLMChain with again with the input + agent_scratchpad 


Add to Git 
----------
1) Build a Tool based on Open Specification 
2) Predating LLMs, the OpenAPI Specification is routinely used by service providers to describe the API's 
3) OpenAPI specification to open AI Specification to find the functions in API and how the input needs to give and how the output needs to give 
4) Tool  - define the tool , argument schema by pydantic definition 
5) Router - just a function takes the result and have condition, agent.finish agent.action - by function agent parser 
6) from langchain.prompts import ChatPromptTemplate --> from messages 
8) Take the Screenshot of Agent Basics 
7) from langchain.prompts import MessagesPlaceholder
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are helpful but sassy assistant"),
    ("user", "{input}"),
    MessagesPlaceholder(variable_name="agent_scratchpad")
])
8) Look the prompt , that we we need to place in the prompt to pass back in this history of tools that are called and the corresponding outputs 
7) from langchain.agents.format_scratchpad import format_to_openai_functions --> formatting after calling the tool 
                 result + observation 
9) Runnable Pass - will add the intermediate message with proper format, 
10) Right now agent [agentchain,tools,message_placeholder] is not have memory, it have only tool call history and observation , adding with messageplaceholder
11) Now Actual Message history is coming 
         prompt = ChatPromptTemplate.from_messages([
    ("system", "You are helpful but sassy assistant"),
    MessagesPlaceholder(variable_name="chat_history"),
    ("user", "{input}"),
    MessagesPlaceholder(variable_name="agent_scratchpad")
])
12) Prompt is main, In Langchain Prompt will have the message_placeholder 
13) agent_executor = AgentExecutor(agent=agent_chain, tools=tools, verbose=True, memory=memory)
14) Now Understood the complete conversation chatbot 
15) Chat: Learn how to track and select pertinent information from conversations and data sources, as you build your own chatbot using LangChain.
16) Restricting the Retrieval 
       only for particular sections 
       don't include the duplicates 
17) Similarity Search - MMR[Diverse Set of Documents - Top_k - Top_diverse] - Including Metadata 
18) Self Aided LLM - include the metadata that you want to do a filter 
19) Why we need embedding for getting data from the saved directory 
    embedding = OpenAIEmbeddings()
vectordb = Chroma(
    persist_directory=persist_directory,
    embedding_function=embedding
)

20) qa_chain = RetrievalQA.from_chain_type(
    llm,
    retriever=vectordb.as_retriever()
)   --> There is no prompt and memory , how it will call the LLM 

21) I Need to Separate the Retreival and LLM Call separately to Create the Agent, and pass the Retrieval as a Tool 
22) Conversational Retrieval Chain -- for storing memory and follow up questions
23) QA_CHAIN_PROMPT = PromptTemplate(input_variables=["context", "question"],template=template,)
    template = """Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say "thanks for asking!" at the end of the answer. 
{context}
Question: {question}
Helpful Answer:"""
QA_CHAIN_PROMPT = PromptTemplate.from_template(template)
qa_chain = RetrievalQA.from_chain_type(
    llm,
    retriever=vectordb.as_retriever(),
    return_source_documents=True,
    chain_type_kwargs={"prompt": QA_CHAIN_PROMPT}
)


from langchain.document_loaders import PyPDFLoader
from langchain.document_loaders.generic import GenericLoader,  FileSystemBlobLoader
from langchain.document_loaders.parsers import OpenAIWhisperParser
from langchain.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader

24) Take history
    Question again
    Build a Standlone question to search the vector DB 
25) Use agents, chained calls, and memories to expand your use of LLMs



------------------------------------------------------------------
Award Wining RAG
----------------
Custom PDF Parsing with Docling
Vector Search with Parent Document Retrieval 
LLM Reranking for improved context relevance 
Structured output prompting with chain-of-thought reasoning 
**Query routing for multi-company comparisons 








Magic
Youtube video 
Arxiv option in Langchain 
Markdown Text Splitter 
We Can add the metadata based on the Markdown Header 

Enumerate 


Goldman Sachs Interview Preparation
-----------------------------------
Complexity Type	Measures	Concerned With
Time Complexity	Growth of execution time	Speed of algorithm
Space Complexity	Growth of memory usage	Memory efficiency

Space Complexity	Description	Example
O(1)	Constant space	In-place sorting
O(n)	Linear space	Storing output in array
O(n^2)	Quadratic space	2D matrix storage

Includes:
Input storage
Temporary variables
Function call stack (especially with recursion)
Data structures used (e.g., arrays, hash tables)



Arrays & Strings
HashMaps\Sets 
Basic Recursion
Sorting & Searching 

Two Sum 
Valid Anagram 
Merge Sorted Arrays 
Binary Search 
Group Anagrams(medium, but doable)
Top K Frequent Elements 

1.Clarify the question 
2.Discuss Edge Cases 
3.Write Brute-Force first if stuck
4.Optimize 

Pythons Tricks
--------------
collections.Counter
List Comprehension
sorted() with key=
set() operations
zip(), enumerate(), dict.get()


Possible Coding Problem Types 
Array Manipulation (prefix sums, sliding window)
Hashing(anagrams, frequency counting)
Two Pointers(sorted arrays, duplicates)
Simple recursion/backtracking(subsets, permutations)
Sorting and custom comparators 
Matrix traversal (BFS/DFS for 2D grids - less likely,possible)


LeetCode - Problem 1 - Two Sum
Input:
nums = [2,7,11,15], target = 9
Output:
[0,1]
Explanation:
Because nums[0] + nums[1] == 9, we return [0, 1]

num_map = {}
for i,num in enumerate(a):
    complement = target - num 
    if complement in num_map:
         return [num_map[complement],i]
    num_map[num] = i 

LeetCode - Problem 217 - Contains Duplicate
def containsDuplicate(nums):
    seen = set()
    for num in nums:
        if num in seen:
            return True
        seen.add(num)
    return False

LeetCode - Problem 350 - Intersection of Two Arrays II - With [collections import Counter] & with sorted technique 
from collections import Counter

def intersect(nums1, nums2):
    count1 = Counter(nums1)
    result = []

    for num in nums2:
        if count1[num] > 0:
            result.append(num)
            count1[num] -= 1

    return result

copying 
-------
def intersect(nums1, nums2):
    result = []
    nums2_copy = list(nums2)

    for num in nums1:
        if num in nums2_copy:
            result.append(num)
            nums2_copy.remove(num)
    
    return result


Two Pointers
------------
def intersect(nums1, nums2):
    nums1.sort()  # If not already sorted
    nums2.sort()

    i = j = 0
    result = []

    while i < len(nums1) and j < len(nums2):
        if nums1[i] == nums2[j]:
            result.append(nums1[i])
            i += 1
            j += 1
        elif nums1[i] < nums2[j]:
            i += 1
        else:
            j += 1

    return result

LeetCode - Problem 242 - Valid Anagram 
LeetCode - Problem 49 - Group Anagram  
  - Optimal Solution (Using Hash Map + Sorted Key)
Store words in a dictionary where:
Key = sorted string
Value = list of anagrams that match that sorted key

from collections import defaultdict 

defaultdict(int)	default value is 0
defaultdict(set)	default value is set()
defaultdict(str)	default value is ""
defaultdict(dict)	default value is {}


from collections import defaultdict

def groupAnagrams(strs):
    anagram_map = defaultdict(list)

    for word in strs:
        key = ''.join(sorted(word))  # Step 1
        anagram_map[key].append(word)  # Step 2

    return list(anagram_map.values())  # Step 3
    
    
strs = ["eat", "tea", "tan", "ate", "nat", "bat"]


LeetCode - Problem - 121 - Finding Maximum Profit 

def maximize(nums):
    min_price = float('inf')
    max_profit = 0
    for price in nums:
        if price < min_price:
            min_price = price 
        else:
            max_profit = max(max_profit, price - min_price)
    return max_profit
    
    
    
a = [7,1,5,3,6,4]
b = maximize(a)
print(b)




Glolbal Variable 
----------------
a = 10 

def say_hello():
    global a
    a = a + 1
    print(a)

    
say_hello()



LeetCode Problem - 53 - Maximum Subarray - Dynamic Programming - Kadane's Algorithm 
you track the maximum sum ending at each index, and the global maximum so far 


def maxSubarray(array):
    max_current = max_global = array[i]
    for i in range(1,(len(array)):
         max_current = max(array[i],

array = [1,2,4,5]
maxSubarray(array)


Searching & Sorting 
-------------------
Binary Search - LeetCode 704  - Classic Implementation with left/right pointer 
-----------------------------------------------
Top K Frequent Elements - LeetCode 347 - 
    - Use Counter - Heap or Bucket Sort


Recursion/BackTracking
----------------------
Subsets LeetCode - 78 - Use Backtracking to generate all subsets

Backtracking / DFS
Use recursion to include or exclude each element.


Iterative Approach 
------------------
def subsets(nums):
    result = [[]]

    for num in nums:
        result += [curr + [num] for curr in result]

    return result


BackTracking Approach - Understood 
---------------------
def subsets(nums):
    result = []

    def backtrack(start, path):
        result.append(path[:])  # Step 1: Add current subset
        for i in range(start, len(nums)):
            path.append(nums[i])          # Step 2: Choose
            backtrack(i + 1, path)        # Step 3: Explore
            path.pop()                    # Step 4: Un-choose

    backtrack(0, [])
    return result






















---------------------------------------------------------------Links------------------------------------------------------------------------------------
Links
Fin-R1
https://github.com/SUFE-AIFLM-Lab/Fin-R1#%E9%87%91%E8%9E%8D%E4%BB%A3%E7%A0%81
https://github.com/SUFE-AIFLM-Lab/Fin-R1/blob/main/README_en.md#use

Azure ML Examples 
https://github.com/Azure/azureml-examples/tree/main/sdk/python/generative-ai/rag/notebooks
https://github.com/Azure/azureml-examples/blob/main/sdk/python/generative-ai/rag/notebooks/mlindex_with_testgen.ipynb
https://github.com/Azure/azureml-examples/tree/main

OpenAI
https://github.com/MicrosoftLearning/mslearn-openai/blob/main/Labfiles/06-use-own-data/Python/ownData.py

Langchain
https://js.langchain.com/v0.1/docs/get_started/introduction/

ReRank - Langchain
https://python.langchain.com/v0.2/docs/concepts/


Prompt Flow 
https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/get-started-prompt-flow?view=azureml-api-2

LLM Inference Parameter 
https://www.linkedin.com/pulse/llm-inference-parameters-explained-visually-wiroai-bwxgf

Memory is - Azure
https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/memory/

LlamaIndex
https://www.analyticsvidhya.com/blog/2024/09/memory-and-hybrid-search-in-rag-using-llamaindex/

Llamaindex
https://docs.llamaindex.ai/en/stable/understanding/using_llms/using_llms/

Llamaindex usage pattern
https://llama-index.readthedocs.io/zh/stable/guides/primer/usage_pattern.html

Custom Retrievers
https://docs.llamaindex.ai/en/stable/examples/query_engine/CustomRetrievers/

ImageDocument
https://docs.llamaindex.ai/en/v0.10.17/api/llama_index.core.schema.ImageDocument.html

Cosmos DB 
https://stochasticcoder.com/2024/03/08/azure-cosmos-db-for-mongodb-hnsw-vector-search/?utm_source=chatgpt.com

Llamaparse Multimodal 
https://github.com/fkemeth/llama_parse_multimodal_llm/blob/main/run_example1_multimodal.ipynb

Llamaparse Multimodal
https://www.google.com/search?q=llamaparse+multimodal&rlz=1C1CHBD_enUS1155US1155&oq=llamaparse+multimodal+&gs_lcrp=EgZjaHJvbWUyCggAEEUYFhgeGDkyCggBEAAYgAQYogQyCggCEAAYgAQYogQyCggDEAAYgAQYogQyCggEEAAYgAQYogQyBggFEEUYPNIBCTEyMjA5ajBqN6gCALACAA&sourceid=chrome&ie=UTF-8

Llamaindex tru lens
https://stackoverflow.com/questions/78258989/how-to-return-the-source-document-of-retrieved-nodes-following-llama-index-engin

Llamaparse Multimodal
https://felixkemeth.medium.com/using-llamaparse-and-multimodal-llms-for-extracting-and-interpreting-text-and-images-from-pdfs-d201093b0e19
https://github.com/run-llama/llama_cloud_services/discussions/445

SemiStructured Retreival 
https://docs.llamaindex.ai/en/stable/examples/multi_modal/structured_image_retrieval/

Llamaparse Vector Store 
https://docs.llamaindex.ai/en/stable/community/integrations/vector_stores/

https://docs.llamaindex.ai/en/stable/understanding/querying/querying/

langchain Tool
https://docs.llamaindex.ai/en/v0.10.17/community/integrations/using_with_langchain.html

Azure MultiModal RAG 
https://techcommunity.microsoft.com/blog/azure-ai-services-blog/multimodal-parsing-for-rag-azure-openai-gpt-4o-llamaparse-and-azure-ai-search/4330399/
Sub Question Query Engine 
https://techcommunity.microsoft.com/blog/azure-ai-services-blog/advanced-rag-with-azure-ai-search-and-llamaindex/4115007 


LLMapps
https://github.com/Shubhamsaboo/awesome-llm-apps/blob/main/rag_tutorials/agentic_rag/rag_agent.py


RAGA AI Catalyst
https://github.com/raga-ai-hub/RagaAI-Catalyst

https://github.com/asinghcsu/AgenticRAG-Survey


Retrieval Augmented Generation 
https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/concept/retrieval-augmented-generation?view=doc-intel-4.0.0
https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/concept/retrieval-augmented-generation?view=doc-intel-4.0.0

MCP 
https://github.com/modelcontextprotocol/python-sdk
https://modelcontextprotocol.io/docs/concepts/prompts#python
https://devblogs.microsoft.com/azure-sdk/introducing-the-azure-mcp-server/
Blog Post for MCP Server 
https://devblogs.microsoft.com/azure-sdk/introducing-the-azure-mcp-server/

MCP Market 
https://x.com/AtomSilverman/status/1916934022288314650?t=fIbWvsnsuoZLCyJ8VLFw8Q&s=08

MCP to multi agent 
https://github.blog/open-source/maintainers/from-mcp-to-multi-agents-the-top-10-open-source-ai-projects-on-github-right-now-and-why-they-matter/

Explore Authentication and Authorization 
https://learn.microsoft.com/en-us/training/modules/introduction-to-azure-app-service/5-authentication-authorization-app-service


Azure Machine Learning RAG Pipeline
https://learn.microsoft.com/en-us/azure/machine-learning/how-to-use-pipelines-prompt-flow?view=azureml-api-2


Azure Copilot Intelligent Plugin Design Pattern 
https://www.linkedin.com/pulse/copilot-microsoft-365-plugins-intelligent-plugin-design-hassan-7olce

Azure Copilot Technical Architecture 
https://labs.zenity.io/p/inside-microsoft-365-copilot-technical-breakdown
https://almbok.com/microsoft/copilot/copilot_system

Azure Copilot Connectors 
https://www.linkedin.com/pulse/microsoft-copilot-studio-connectors-what-how-really-works-hassan-2tw0f

Azure Copilot Implementation Guide
https://www.microsoft.com/en-us/microsoft-copilot/blog/copilot-studio/new-microsoft-copilot-studio-implementation-guide/

https://www.google.com/search?sca_esv=9c7ec454ad9eda0c&rlz=1C1GCEA_enIN1050IN1050&q=microsoft+copilot+studio+architecture&udm=2&fbs=ABzOT_CWdhQLP1FcmU5B0fn3xuWpA-dk4wpBWOGsoR7DG5zJBkzPWUS0OtApxR2914vrjk60CMaA0jPMd-1UByCaBk9RkR5TYqOiT6ScESjovcjoQvczWGzfl03gMqmV3swMnlaHY7h-y8maWsVlqqHOlm1aI-VEUVb8-3QW0ZUss5AXthH1HmZDs_Q0k_NzasqwQkpVyZMWLACnc0yvEfyljhT7myXSpQ&sa=X&ved=2ahUKEwiU767PzfCMAxWjZ_UHHVJRBZkQtKgLegQIEhAB&biw=1366&bih=633

https://www.linkedin.com/posts/mahmoudhamedhassan_microsoft365copilot-microsoftcopilot-copilot-activity-7287061337145233409-5GdO


Langchain cosmos - https://python.langchain.com/docs/integrations/vectorstores/azure_cosmos_db/
https://python.langchain.com/docs/integrations/vectorstores/azure_cosmos_db_no_sql/
https://docs.llamaindex.ai/en/stable/examples/vector_stores/AzureCosmosDBMongoDBvCoreDemo/

Vector Search NoSQL Cosmos DB
https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/vector-search
https://learn.microsoft.com/en-us/azure/cosmos-db/ai-agents

https://learn.microsoft.com/en-us/azure/cosmos-db/free-tier
https://learn.microsoft.com/en-us/azure/cosmos-db/index-policy#vector-indexes
https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/vector-search?source=recommendations

https://learn.microsoft.com/en-us/azure/cosmos-db/vector-database

Semantic Ranking Azure AI Search
https://learn.microsoft.com/en-us/azure/search/semantic-search-overview

AI Engineer Career path
https://learn.microsoft.com/en-us/plans/odgoumq07e4x83

Java Agents
https://learn.microsoft.com/en-us/training/modules/build-enterprise-ai-agents-with-java-spring/


AI Agent Report 
https://www.microsoft.com/en-us/worklab/work-trend-index/2025-the-year-the-frontier-firm-is-born
https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/microsoft/final/en-us/microsoft-brand/documents/1036834-what-is-an-autonomous-agent.pdf
https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/microsoft/final/en-us/microsoft-brand/documents/what-is-an-agent.pdf
https://reprint.forrester.com/reports/the-state-of-ai-agents-2024-3efea0a4/index.html
https://info.microsoft.com/ww-thankyou-The-State-of-AI-Agents.html
https://www.microsoft.com/en-us/microsoft-copilot/microsoft-copilot-studio#get-started
https://ignite.microsoft.com/en-US/sessions/BRK168?source=sessions
https://news.microsoft.com/source/features/ai/how-world-class-athletes-are-turning-to-the-power-of-ai-and-data-2/?OCID=lock2


Hugging Face
https://huggingface.co/datasets/legacy-datasets/c4

OG Project 
https://github.com/parody25/OG_Project1/tree/main/AI_Engine

openwebui
https://docs.openwebui.com/



