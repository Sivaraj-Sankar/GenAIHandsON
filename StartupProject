command 
  python -m venv llmstudio
  python -m venv llmstudio
  python.exe -m pip install --upgrade pip
  venv\Scripts\activate 
  deactivate 
  pip freeze > requirements.txt 
  uvicorn main:app --reload 

pip install mcp
pip install "mcp[cli]"
pip install uv
pip install mcp-use mcp_use

Powershell Command
Invoke-RestMethod -Uri "http://127.0.0.1:1234/v1/models/"
curl http://127.0.0.1:1234/v1/models/
lms get llama-3.2-1b-instruct

different types of running mcp server 
   -> running with python file.py --server_type=stdio, sse 
   -> running with mcp cli command - with inspector -> mcp dev file.py 
   -> running with uv command -> uv run 
                uv run mcp run MCP_HandsOn.py
                uv run mcp dev MCP_HandsOn.py 
                uv run mcp install MCP_HandsOn.py
   -> Connecting server with different server_type = http_client, sse_client[Server-Sent Events], local_client[stdio client] - passing arguments python server.py

1.What is resources passing in the MCPToolSpec
2.Prompts in MCP are predefined templates that can:
Accept dynamic arguments
Include context from resources
3.Have to Learn the Context and import Contextlib
  What is the context providing for the tools and resources
https://github.com/modelcontextprotocol/python-sdk?tab=readme-ov-file#context
4.mcp-use - Multiple MCP Server, using the config for client connection 
https://github.com/mcp-use/mcp-use?utm_source=chatgpt.com
5.Context Management 
https://huggingface.co/blog/lynn-mikami/mcp-use?utm_source=chatgpt.com
6. You host MCP server with --server_type = sse using FastAPI/uvicorn server 
 


1) Docker Installed 
2) LLMStudio Installed 

Things to Check in LLM Models
1.Autoregressive transformer (decoder‑only), using Grouped‑Query Attention (GQA) for faster inference
2.Instruction tuning: Uses supervised fine‑tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with user intent
3.1.23 billion (1B class)
4.Feature	Details
Parameters	~1.23 billion (1B class)
Context length	128,000 tokens (~192 pages)—same full-length support as the 3B model
Quantized variant	Instruction-tuned 1B model quantized to reduce memory; context reduced to 8k tokens
Languages supported	Multilingual; officially: English, German, French, Italian, Portuguese, Hindi, Spanish, Thai (but trained on many more)
Input/output	Text-only (multilingual); fine-tuned for dialogue, retrieval, summarization, reasoning
Release date	September 25, 2024
Training tokens	Up to ~9 trillion tokens
Knowledge cutoff	December 2023
License	Llama 3.2 Community License — commercial-friendly with policy restrictions

Performance & Use-Cases
MMLU (multi-choice): ~49.3
GSM8K (math reasoning): ~44.4%
ARC-C (science reasoning): ~59.4% 

Multilingual dialogue and conversational agents
Knowledge retrieval
Summarization tasks
Basic arithmetic and logic reasoning
On-device inference (edge/mobile friendly) 

Reasoning ability: Yes — performs CoT (chain-of-thought) reasoning and logic tasks within its capabilities at small scale


-----------------------------------------------------------------------

