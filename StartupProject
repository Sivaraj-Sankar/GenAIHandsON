command 
  python -m venv llmstudio
  python -m venv llmstudio
  python.exe -m pip install --upgrade pip
  venv\Scripts\activate 
  deactivate 
  pip freeze > requirements.txt 
  uvicorn main:app --reload 

pip install mcp
pip install "mcp[cli]"
pip install uv
pip install mcp-use mcp_use

Powershell Command
Invoke-RestMethod -Uri "http://127.0.0.1:1234/v1/models/"
curl http://127.0.0.1:1234/v1/models/
lms get llama-3.2-1b-instruct

different types of running mcp server 
   -> running with python file.py --server_type=stdio, sse 
   -> running with mcp cli command - with inspector -> mcp dev file.py 
   -> running with uv command -> uv run 
                uv run mcp run MCP_HandsOn.py
                uv run mcp dev MCP_HandsOn.py 
                uv run mcp install MCP_HandsOn.py
   -> Connecting server with different server_type = http_client, sse_client[Server-Sent Events], local_client[stdio client] - passing arguments python server.py

1.What is resources passing in the MCPToolSpec
2.Prompts in MCP are predefined templates that can:
Accept dynamic arguments
Include context from resources
3.Have to Learn the Context and import Contextlib
  What is the context providing for the tools and resources
https://github.com/modelcontextprotocol/python-sdk?tab=readme-ov-file#context
4.mcp-use - Multiple MCP Server, using the config for client connection 
https://github.com/mcp-use/mcp-use?utm_source=chatgpt.com
5.Context Management 
https://huggingface.co/blog/lynn-mikami/mcp-use?utm_source=chatgpt.com
6. You host MCP server with --server_type = sse using FastAPI/uvicorn server 
 


1) Docker Installed 
2) LLMStudio Installed 

Things to Check in LLM Models
1.Autoregressive transformer (decoder‑only), using Grouped‑Query Attention (GQA) for faster inference
2.Instruction tuning: Uses supervised fine‑tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with user intent
3.1.23 billion (1B class)
4.Feature	Details
Parameters	~1.23 billion (1B class)
Context length	128,000 tokens (~192 pages)—same full-length support as the 3B model
Quantized variant	Instruction-tuned 1B model quantized to reduce memory; context reduced to 8k tokens
Languages supported	Multilingual; officially: English, German, French, Italian, Portuguese, Hindi, Spanish, Thai (but trained on many more)
Input/output	Text-only (multilingual); fine-tuned for dialogue, retrieval, summarization, reasoning
Release date	September 25, 2024
Training tokens	Up to ~9 trillion tokens
Knowledge cutoff	December 2023
License	Llama 3.2 Community License — commercial-friendly with policy restrictions

Performance & Use-Cases
MMLU (multi-choice): ~49.3
GSM8K (math reasoning): ~44.4%
ARC-C (science reasoning): ~59.4% 

Multilingual dialogue and conversational agents
Knowledge retrieval
Summarization tasks
Basic arithmetic and logic reasoning
On-device inference (edge/mobile friendly) 

Reasoning ability: Yes — performs CoT (chain-of-thought) reasoning and logic tasks within its capabilities at small scale


-----------------------------------------------------------------------
POST /api/documents/upload
Body: {
  fileName: string;
  fileBase64: string;
  fileType: string;
}
Response: { id: string; fileName: string; fileUrl: string; fileType: string; uploadedAt: string; }


POST /api/documents/upload

GET /api/documents
Response: Array of documents:

[
  {
    id: string;
    fileName: string;
    fileUrl: string;
    fileType: string;
    uploadedAt: string;
    reports: [
      {
        id: string;
        status: string;
        createdAt: string;
      }
    ]
  }
]

POST /api/reports/generate
Body: { documentId: string }
Response: { reportId: string; taskId: string }


GET /api/reports/{reportId}/status
Response: 
{
  id: string;
  status: string;
  content: string;
  fileUrl: string | null;
  taskStatus: any;
}


GET /api/reports
Response: 
[
  {
    id: string;
    document: {
      id: string;
      fileName: string;
      fileUrl: string;
      fileType: string;
    };
    status: string;
    content: string;
    fileUrl: string | null;
    createdAt: string;
  }
]

POST /api/chat/send
Body: { content: string; documentId?: string }
Response:
{
  userMessage: { id, content, createdAt, role },
  aiMessage: { id, content, createdAt, role }
}

GET /api/chat/history?documentId=<docId>
Response:
[
  { id, role, content, createdAt }
]


You can use marked (lightweight), or react-markdown (better)
→ For simplicity here: use react-markdown (very easy)

Endpoint	Method	Description
/api/documents	GET	Get list of documents
/api/documents/upload	POST	Upload a document
/api/reports	GET	Get list of reports
/api/reports/generate	POST	Start report generation
/api/reports/{report_id}/status	GET	Get status of report
/api/chat/{document_id}	GET	Get chat history for document
/api/chat/{document_id}/send	POST	Send chat message


React Query
---------
Keep React Query for caching + state
Keep toast notifications

Chat For That Document
=======================
import { useQuery, useMutation, useQueryClient } from "@tanstack/react-query";
Perfect — you want Chat with UserDocuments → meaning:
📄 Chat per document — so user can click on document → open chat panel just for that document.
Plan:
✅ In UserDocuments list → for each document
✅ Add "Chat" button
✅ When clicked → show ChatBox for that document (pass documentId)

Step 1: Update UserDocuments.tsx
Add showChatForDocId state:

import React, { useEffect, useState } from "react";
import { getUserDocuments } from "../client/api";
import GenerateReportButton from "./GenerateReportButton";
import ReportStatus from "./ReportStatus";
import ChatBox from "./ChatBox";

type Document = {
  id: string;
  fileName: string;
  fileUrl: string;
  fileType: string;
  uploadedAt: string;
  reports: {
    id: string;
    status: string;
    createdAt: string;
  }[];
};

export default function UserDocuments() {
  const [documents, setDocuments] = useState<Document[]>([]);
  const [loading, setLoading] = useState(true);
  const [error, setError] = useState<string | null>(null);
  const [showChatForDocId, setShowChatForDocId] = useState<string | null>(null);

  useEffect(() => {
    async function fetchDocuments() {
      try {
        const docs = await getUserDocuments();
        setDocuments(docs);
      } catch (err: any) {
        setError(err.message || "Failed to load documents");
      } finally {
        setLoading(false);
      }
    }

    fetchDocuments();
  }, []);

  return (
    <div style={{ border: "1px solid #ccc", padding: "16px", marginTop: "24px" }}>
      <h2>Your Documents</h2>

      {loading && <p>Loading documents...</p>}
      {error && <p style={{ color: "red" }}>Error: {error}</p>}

      {!loading && documents.length === 0 && <p>No documents uploaded yet.</p>}

      {!loading && documents.length > 0 && (
        <ul>
          {documents.map((doc) => (
            <li key={doc.id} style={{ marginBottom: "16px" }}>
              <strong>{doc.fileName}</strong> ({doc.fileType}) <br />
              Uploaded: {new Date(doc.uploadedAt).toLocaleString()} <br />
              <a href={doc.fileUrl} target="_blank" rel="noopener noreferrer">
                View Document
              </a>
              <br />
              {doc.reports.length > 0 ? (
                <ReportStatus reportId={doc.reports[0].id} />
              ) : (
                <p>No report generated yet.</p>
              )}

              <GenerateReportButton documentId={doc.id} />

              <button
                onClick={() =>
                  setShowChatForDocId(showChatForDocId === doc.id ? null : doc.id)
                }
                style={{ marginTop: "8px" }}
              >
                {showChatForDocId === doc.id ? "Hide Chat" : "Chat with AI"}
              </button>

              {showChatForDocId === doc.id && (
                <ChatBox documentId={doc.id} />
              )}
            </li>
          ))}
        </ul>
      )}
    </div>
  );
}

Now in UserDocuments:
Each document has "Chat with AI" button
If clicked → opens ChatBox for that document
AI will see document context (documentId passed to API)
You can remove global ChatBox from App.tsx if you want → now chat is per-document.

Summary:
You can now chat with individual documents
ChatBox is tied to documentId
The API already supports this


Update ChatBot.tsx
import React, { useEffect, useState } from "react";
import { sendChatMessage, getChatHistory } from "../client/api";

export default function ChatBox({ documentId }: { documentId?: string }) {
  const [input, setInput] = useState("");
  const [messages, setMessages] = useState<
    { role: string; content: string; createdAt?: string }[]
  >([]);
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const [historyLoading, setHistoryLoading] = useState(true);

  useEffect(() => {
    async function fetchHistory() {
      try {
        const history = await getChatHistory({ documentId });
        setMessages(history);
      } catch (err: any) {
        console.error("Failed to load chat history", err);
      } finally {
        setHistoryLoading(false);
      }
    }

    fetchHistory();
  }, [documentId]);

  const handleSend = async () => {
    if (!input.trim()) return;

    setLoading(true);
    setError(null);

    try {
      const res = await sendChatMessage({
        content: input,
        documentId,
      });

      setMessages((prev) => [
        ...prev,
        { role: "user", content: res.userMessage.content },
        { role: "assistant", content: res.aiMessage.content },
      ]);

      setInput("");
    } catch (err: any) {
      setError(err.message || "Failed to send message");
    } finally {
      setLoading(false);
    }
  };

  return (
    <div style={{ border: "1px solid #ccc", padding: "16px", marginTop: "12px" }}>
      <h3>Chat with AI {documentId ? `(Document Context)` : ``}</h3>

      <div
        style={{
          minHeight: "150px",
          border: "1px solid #999",
          padding: "8px",
          marginBottom: "12px",
          backgroundColor: "#fafafa",
          overflowY: "auto",
          maxHeight: "300px",
        }}
      >
        {historyLoading ? (
          <p>Loading chat history...</p>
        ) : messages.length === 0 ? (
          <p>No messages yet.</p>
        ) : (
          messages.map((msg, index) => (
            <div
              key={index}
              style={{
                marginBottom: "8px",
                color: msg.role === "user" ? "black" : "blue",
              }}
            >
              <strong>{msg.role === "user" ? "You" : "AI"}:</strong> {msg.content}
            </div>
          ))
        )}
      </div>

      <textarea
        rows={3}
        value={input}
        onChange={(e) => setInput(e.target.value)}
        style={{ width: "100%", marginBottom: "8px" }}
        placeholder="Type your message..."
      />

      <button onClick={handleSend} disabled={loading || !input.trim()}>
        {loading ? "Sending..." : "Send"}
      </button>

      {error && (
        <div style={{ marginTop: "8px", color: "red" }}>Error: {error}</div>
      )}
    </div>
  );
}

✅ Now:

When you click "Chat with AI" →
👉 It will load previous chat history
👉 Show in chat panel
👉 You can continue chatting — new messages added live


✅ Now:

When you click "Chat with AI" →
👉 It will load previous chat history
👉 Show in chat panel
👉 You can continue chatting — new messages added live

Summary:

✅ Upload document
✅ View documents
✅ Generate report
✅ Check report status
✅ View reports + preview
✅ Chat per document with history

Next step?

1️⃣ Styling — make it nice
2️⃣ Refresh UserDocuments after GenerateReport → auto show new status
3️⃣ Any other feature
4️⃣ You test first

Just tell me — ready for next? 🚀



