command 
  python -m venv llmstudio
  python -m venv llmstudio
  python.exe -m pip install --upgrade pip
  venv\Scripts\activate 
  deactivate 
  pip freeze > requirements.txt 
  uvicorn main:app --reload

pip install mcp
pip install "mcp[cli]"
pip install uv
pip install mcp-use mcp_use

Powershell Command
Invoke-RestMethod -Uri "http://127.0.0.1:1234/v1/models/"
curl http://127.0.0.1:1234/v1/models/
lms get llama-3.2-1b-instruct

different types of running mcp server 
   -> running with python file.py --server_type=stdio, sse 
   -> running with mcp cli command - with inspector -> mcp dev file.py 
   -> running with uv command -> uv run 
                uv run mcp run MCP_HandsOn.py
                uv run mcp dev MCP_HandsOn.py 
                uv run mcp install MCP_HandsOn.py
   -> Connecting server with different server_type = http_client, sse_client[Server-Sent Events], local_client[stdio client] - passing arguments python server.py

1.What is resources passing in the MCPToolSpec
2.Prompts in MCP are predefined templates that can:
Accept dynamic arguments
Include context from resources
3.Have to Learn the Context and import Contextlib
  What is the context providing for the tools and resources
https://github.com/modelcontextprotocol/python-sdk?tab=readme-ov-file#context
4.mcp-use - Multiple MCP Server, using the config for client connection 
https://github.com/mcp-use/mcp-use?utm_source=chatgpt.com
5.Context Management 
https://huggingface.co/blog/lynn-mikami/mcp-use?utm_source=chatgpt.com
6. You host MCP server with --server_type = sse using FastAPI/uvicorn server 
 


1) Docker Installed 
2) LLMStudio Installed 

