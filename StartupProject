command 
  python -m venv llmstudio
  python -m venv llmstudio
  python.exe -m pip install --upgrade pip
  venv\Scripts\activate 
  deactivate 
  pip freeze > requirements.txt 
  uvicorn main:app --reload 

pip install mcp
pip install "mcp[cli]"
pip install uv
pip install mcp-use mcp_use

Powershell Command
Invoke-RestMethod -Uri "http://127.0.0.1:1234/v1/models/"
curl http://127.0.0.1:1234/v1/models/
lms get llama-3.2-1b-instruct

different types of running mcp server 
   -> running with python file.py --server_type=stdio, sse 
   -> running with mcp cli command - with inspector -> mcp dev file.py 
   -> running with uv command -> uv run 
                uv run mcp run MCP_HandsOn.py
                uv run mcp dev MCP_HandsOn.py 
                uv run mcp install MCP_HandsOn.py
   -> Connecting server with different server_type = http_client, sse_client[Server-Sent Events], local_client[stdio client] - passing arguments python server.py

1.What is resources passing in the MCPToolSpec
2.Prompts in MCP are predefined templates that can:
Accept dynamic arguments
Include context from resources
3.Have to Learn the Context and import Contextlib
  What is the context providing for the tools and resources
https://github.com/modelcontextprotocol/python-sdk?tab=readme-ov-file#context
4.mcp-use - Multiple MCP Server, using the config for client connection 
https://github.com/mcp-use/mcp-use?utm_source=chatgpt.com
5.Context Management 
https://huggingface.co/blog/lynn-mikami/mcp-use?utm_source=chatgpt.com
6. You host MCP server with --server_type = sse using FastAPI/uvicorn server 
 


1) Docker Installed 
2) LLMStudio Installed 

Things to Check in LLM Models
1.Autoregressive transformer (decoderâ€‘only), using Groupedâ€‘Query Attention (GQA) for faster inference
2.Instruction tuning: Uses supervised fineâ€‘tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with user intent
3.1.23â€¯billion (1B class)
4.Feature	Details
Parameters	~1.23â€¯billion (1B class)
Context length	128,000 tokens (~192 pages)â€”same full-length support as the 3B model
Quantized variant	Instruction-tuned 1B model quantized to reduce memory; context reduced to 8k tokens
Languages supported	Multilingual; officially: English, German, French, Italian, Portuguese, Hindi, Spanish, Thai (but trained on many more)
Input/output	Text-only (multilingual); fine-tuned for dialogue, retrieval, summarization, reasoning
Release date	September 25, 2024
Training tokens	Up to ~9 trillion tokens
Knowledge cutoff	December 2023
License	Llama 3.2 Community License â€” commercial-friendly with policy restrictions

Performance & Use-Cases
MMLU (multi-choice): ~49.3
GSM8K (math reasoning): ~44.4%
ARC-C (science reasoning): ~59.4% 

Multilingual dialogue and conversational agents
Knowledge retrieval
Summarization tasks
Basic arithmetic and logic reasoning
On-device inference (edge/mobile friendly) 

Reasoning ability: Yes â€” performs CoT (chain-of-thought) reasoning and logic tasks within its capabilities at small scale


-----------------------------------------------------------------------
POST /api/documents/upload
Body: {
  fileName: string;
  fileBase64: string;
  fileType: string;
}
Response: { id: string; fileName: string; fileUrl: string; fileType: string; uploadedAt: string; }


POST /api/documents/upload

GET /api/documents
Response: Array of documents:

[
  {
    id: string;
    fileName: string;
    fileUrl: string;
    fileType: string;
    uploadedAt: string;
    reports: [
      {
        id: string;
        status: string;
        createdAt: string;
      }
    ]
  }
]

POST /api/reports/generate
Body: { documentId: string }
Response: { reportId: string; taskId: string }


GET /api/reports/{reportId}/status
Response: 
{
  id: string;
  status: string;
  content: string;
  fileUrl: string | null;
  taskStatus: any;
}


GET /api/reports
Response: 
[
  {
    id: string;
    document: {
      id: string;
      fileName: string;
      fileUrl: string;
      fileType: string;
    };
    status: string;
    content: string;
    fileUrl: string | null;
    createdAt: string;
  }
]

POST /api/chat/send
Body: { content: string; documentId?: string }
Response:
{
  userMessage: { id, content, createdAt, role },
  aiMessage: { id, content, createdAt, role }
}


You can use marked (lightweight), or react-markdown (better)
â†’ For simplicity here: use react-markdown (very easy)

Endpoint	Method	Description
/api/documents	GET	Get list of documents
/api/documents/upload	POST	Upload a document
/api/reports	GET	Get list of reports
/api/reports/generate	POST	Start report generation
/api/reports/{report_id}/status	GET	Get status of report
/api/chat/{document_id}	GET	Get chat history for document
/api/chat/{document_id}/send	POST	Send chat message


React Query
---------
Keep React Query for caching + state
Keep toast notifications

Chat For That Document
=======================
import { useQuery, useMutation, useQueryClient } from "@tanstack/react-query";
Perfect â€” you want Chat with UserDocuments â†’ meaning:
ðŸ“„ Chat per document â€” so user can click on document â†’ open chat panel just for that document.
Plan:
âœ… In UserDocuments list â†’ for each document
âœ… Add "Chat" button
âœ… When clicked â†’ show ChatBox for that document (pass documentId)

Step 1: Update UserDocuments.tsx
Add showChatForDocId state:

import React, { useEffect, useState } from "react";
import { getUserDocuments } from "../client/api";
import GenerateReportButton from "./GenerateReportButton";
import ReportStatus from "./ReportStatus";
import ChatBox from "./ChatBox";

type Document = {
  id: string;
  fileName: string;
  fileUrl: string;
  fileType: string;
  uploadedAt: string;
  reports: {
    id: string;
    status: string;
    createdAt: string;
  }[];
};

export default function UserDocuments() {
  const [documents, setDocuments] = useState<Document[]>([]);
  const [loading, setLoading] = useState(true);
  const [error, setError] = useState<string | null>(null);
  const [showChatForDocId, setShowChatForDocId] = useState<string | null>(null);

  useEffect(() => {
    async function fetchDocuments() {
      try {
        const docs = await getUserDocuments();
        setDocuments(docs);
      } catch (err: any) {
        setError(err.message || "Failed to load documents");
      } finally {
        setLoading(false);
      }
    }

    fetchDocuments();
  }, []);

  return (
    <div style={{ border: "1px solid #ccc", padding: "16px", marginTop: "24px" }}>
      <h2>Your Documents</h2>

      {loading && <p>Loading documents...</p>}
      {error && <p style={{ color: "red" }}>Error: {error}</p>}

      {!loading && documents.length === 0 && <p>No documents uploaded yet.</p>}

      {!loading && documents.length > 0 && (
        <ul>
          {documents.map((doc) => (
            <li key={doc.id} style={{ marginBottom: "16px" }}>
              <strong>{doc.fileName}</strong> ({doc.fileType}) <br />
              Uploaded: {new Date(doc.uploadedAt).toLocaleString()} <br />
              <a href={doc.fileUrl} target="_blank" rel="noopener noreferrer">
                View Document
              </a>
              <br />
              {doc.reports.length > 0 ? (
                <ReportStatus reportId={doc.reports[0].id} />
              ) : (
                <p>No report generated yet.</p>
              )}

              <GenerateReportButton documentId={doc.id} />

              <button
                onClick={() =>
                  setShowChatForDocId(showChatForDocId === doc.id ? null : doc.id)
                }
                style={{ marginTop: "8px" }}
              >
                {showChatForDocId === doc.id ? "Hide Chat" : "Chat with AI"}
              </button>

              {showChatForDocId === doc.id && (
                <ChatBox documentId={doc.id} />
              )}
            </li>
          ))}
        </ul>
      )}
    </div>
  );
}

Now in UserDocuments:
Each document has "Chat with AI" button
If clicked â†’ opens ChatBox for that document
AI will see document context (documentId passed to API)
You can remove global ChatBox from App.tsx if you want â†’ now chat is per-document.

Summary:
You can now chat with individual documents
ChatBox is tied to documentId
The API already supports this



