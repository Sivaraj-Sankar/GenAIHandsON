command 
  python -m venv llmstudio
  python -m venv llmstudio
  python.exe -m pip install --upgrade pip
  venv\Scripts\activate 
  deactivate 
  pip freeze > requirements.txt 

pip install mcp
pip install "mcp[cli]"
pip install uv
pip install mcp-use mcp_use

Powershell Command
Invoke-RestMethod -Uri "http://127.0.0.1:1234/v1/models/"
curl http://127.0.0.1:1234/v1/models/
lms get llama-3.2-1b-instruct

different types of running mcp server 
   -> running with python file.py --server_type=stdio, sse 
   -> running with mcp cli command - with inspector -> mcp dev file.py 
   -> running with uv command -> uv run 
                uv run mcp run MCP_HandsOn.py
                uv run mcp dev MCP_HandsOn.py 
                uv run mcp install MCP_HandsOn.py
   -> Connecting server with different server_type = http_client, sse_client[Server-Sent Events], local_client[stdio client] - passing arguments python server.py

What is resources passing in the MCPToolSpec
 


1) Docker Installed 
2) LLMStudio Installed 

