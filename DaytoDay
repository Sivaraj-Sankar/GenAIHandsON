July 2025 

Top Reasearcher 
---------------
Trapit Bansal - RL CoT - o-series model at openAI
Shuchao Bi - co-creater of GPT-4o voice mode and o4-mini 
             Multi-modal Post-Training at OpenAI 
Huiwen Chang - co-creator of GPT-4o's image generation, 
             - MaskGIT and Muse text-to-image architectures at Google Research 
Ji Lin - o3/o4-mini, GPT-4o, GPT-4.1, GPT-4.5, 4o-imagegen and Operator Reasoning stack 
Joel Pobar - Inference at Anthropic. 
Jack Rae - pre-training tech lead for Gemini and reasoning for Gemini 2.5. Led Gopher and chinchilla early 
           LLM efforts at Deepmind
Hongyu Ren - Co-Creator of GPT-4o, 4o-mini, o1-mini o3-mini, o3 and o4-mini. Previously leading a group for post-training of OpenAI 
Johan Schalkwyk - former Google Fellow, 
Pei Sun - post-Training, coding, and reasoning of Gemini at Google Deepmind
          Waymo's Perception models 
Jiahui Yu - co-creator of o3, o4-mini, GPT-4.1 and GPT-4o. Previously led the perception team at OpenAI, and co-led multimodal at Gemini
Shengjia Zhao - ChatGPT, GPT-4, all mini models, 4.1 and o3. Previously synthetic data at openAI
Alexandr Wang: Chief AI Officer Meta - Scale AI Company

Evals and Benchmarks - LLM
--------------------
Benchmarks - Standardized tests / Evaluation suites used to measure and compare the performance of different models
             across a variety of tasks.
 - These Benchmarks help researchers, developers, and users understand how well an LLM performs in specific areas 
 - Reasoning - Comprehension - Coding - Translation - Safety 

Common Benchmarks
MMLU
HELLASWAG
ARC
GSM8K 
BIG-Bench (BBH) 


Evaluation Metrics - NLP Evaluation
-----------------------------------
The model's prediction is compared to the labeled ground truth 
Metrics like Accuracy, BLEU, F1, pass@k, ROUGE etc are used depending on the task 

Classification        - Accuracy, F1      
QA (Short answer)     - Exact Match (EM), F1
Translation           - BLEU, METEOR 
Code Generation       - pass@k, correctness 
Summarization         - ROUGE, BLEU 
Math (GSM8K)          - Exact answer matching 

RAG Evaluation 
--------------
Evaluate Retrieval Quality - Are you retrieving the right context documents
Recall@k    - % of times the correct doc is in top-k 
Precision@k - % of top-k docs that are relevant 
MMR - Mean Reciprocal Rank 
nDCG - Ranking Quality of relevant documents

Evaluate Generation Quality - Is the final answer accurate, complete or grounded 






