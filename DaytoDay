July 2025 

Index
Top Reasearcher 
Evals and Benchmarks - LLM
RAG Evaluation 
Evaluating LLM-Based Agents


Top Reasearcher 
---------------
Trapit Bansal - RL CoT - o-series model at openAI
Shuchao Bi - co-creater of GPT-4o voice mode and o4-mini 
             Multi-modal Post-Training at OpenAI 
Huiwen Chang - co-creator of GPT-4o's image generation, 
             - MaskGIT and Muse text-to-image architectures at Google Research 
Ji Lin - o3/o4-mini, GPT-4o, GPT-4.1, GPT-4.5, 4o-imagegen and Operator Reasoning stack 
Joel Pobar - Inference at Anthropic. 
Jack Rae - pre-training tech lead for Gemini and reasoning for Gemini 2.5. Led Gopher and chinchilla early 
           LLM efforts at Deepmind
Hongyu Ren - Co-Creator of GPT-4o, 4o-mini, o1-mini o3-mini, o3 and o4-mini. Previously leading a group for post-training of OpenAI 
Johan Schalkwyk - former Google Fellow, 
Pei Sun - post-Training, coding, and reasoning of Gemini at Google Deepmind
          Waymo's Perception models 
Jiahui Yu - co-creator of o3, o4-mini, GPT-4.1 and GPT-4o. Previously led the perception team at OpenAI, and co-led multimodal at Gemini
Shengjia Zhao - ChatGPT, GPT-4, all mini models, 4.1 and o3. Previously synthetic data at openAI
Alexandr Wang: Chief AI Officer Meta - Scale AI Company

Evals and Benchmarks - LLM
--------------------
Benchmarks - Standardized tests / Evaluation suites used to measure and compare the performance of different models
             across a variety of tasks.
 - These Benchmarks help researchers, developers, and users understand how well an LLM performs in specific areas 
 - Reasoning - Comprehension - Coding - Translation - Safety 

Common Benchmarks
MMLU
HELLASWAG
ARC
GSM8K 
BIG-Bench (BBH) 


Evaluation Metrics - NLP Evaluation
-----------------------------------
The model's prediction is compared to the labeled ground truth 
Metrics like Accuracy, BLEU, F1, pass@k, ROUGE etc are used depending on the task 

Classification        - Accuracy, F1      
QA (Short answer)     - Exact Match (EM), F1
Translation           - BLEU, METEOR 
Code Generation       - pass@k, correctness 
Summarization         - ROUGE, BLEU 
Math (GSM8K)          - Exact answer matching 

RAG Evaluation 
--------------
Evaluate Retrieval Quality - Are you retrieving the right context documents
Recall@k    - % of times the correct doc is in top-k 
Precision@k - % of top-k docs that are relevant 
MMR - Mean Reciprocal Rank 
nDCG - Ranking Quality of relevant documents

def recall_at_k(retrieved_docs,relevant_docs,k=5):
  hits = sum([1 for doc in retrieved_docs[:k] if doc in relevant_docs])
  return hits / min(k,len(relevant_docs))

Evaluate Generation Quality - Is the final answer accurate, complete or grounded 
Exact Match - Text match with groun-truth answer 
ROUGE/BLEU  - Token overlap (for summaries/QA) 
Faithfulness/Groundedness - Is the answer supported by retrieved docs
F1 Score - Partial Match (word overlap, e.g. for QA) 
Human Eval - Best for complex answers (correct, useful) 

def exact_match(pred, label):
  return pred.strip().lower() == label.strip().lower()

Evaluate Faithfulness / Hallunciation - Specific to RAG
Is the Answer based only on retrieved documents, or is it hallucinating 
Attribution/Grounding Tests : Does Every fact appear in the source 
RAGAS (Hugging Face): End-to-End RAG Evaluation Tool 

from ragas.evaluation import evaluate 
from ragas.metrics import faithfulness, answer_relevancy
results = evaluate(questions=questions,contexts=contexts,answers=answers,metrics=[faithfulness,answer_relevancy],)
print(results)

Pipeline for RAG - Evaluation 
  {"query":
  "retrieved_docs":
  "generated_answer":
  gold_answer":
}
you can evaluate 
Retrieval: Does Retrieved_docs include the sentence about Paris ?
Generation: Does generated_answer match gold_answer
Grounding: Is "Paris" found in retrieved_docs ?


Retrieval - FAISS BM25 ColBERT
Generation -  GPT, T5, LLaMA, etc 
Evaluation - RAGS, Llamaindex Evals, Trulens, Langchain evals 

Building a Test set for RAG evaluation 
Implementing custom metrics 
Comparing two RAG versions (e.g. BM25 vs embedding retriever) 

Evaluating LLM-Based Agents 
---------------------------

Evaluating AI Agents 
Agent Capabilities Evaluation - Planing and Multi-Step Reasoning  - AQUA-RAT
                                Function Calling - Tool Use 
                                Self-Reflection 
                                Memory 
                          
Application-Specific Agent Evaluation - 
Generalist Agents Evaluation 
Frameworks for Agent Evaluation 


Agentic AI's Web Search and Reasoning capabilities involves a multi-dimensional approach. 
- Compile a dataset of queries that cover
  Typical Searches 
  Edge Cases - vague, ambiguous or trick questions
  Multi-Step Tasks - requiring search + synthesis 

Quantitative Metrics 
Apply both Information Retrieval (IR) and agenti-specific metrics
IR Metrics 
* Precision@k, Recall, nDCG - measure relevance and ranking quality 
* Session success rate/ CTR - track how often user clicks or engages meaninfully 

Agentic AI Metrics (from Galileo's taxonomy) 
System Metrics: Latency, token usage, cost per task 
Task Success: Percentage of tasks completed correctly
Quality - instruction adherence, output format, factual accuracy
Tool Interaction - correct tool selection and parameter usage 

Tool-call accuracy - Did it choose and use the search tool appropriately 
Plan-Exectuion - Where intermediate steps logical and necessary ? Logging Traces/ 

Benchmark with known Evaluations
Designed for web-browsing agents
WebGames, BEARCUBS - simulate realistic browsing and retrieval scenarios 
InfoDeepSeek - evaluates retrieval + generation quality 


Drift in Tool Usage 
Retrieval quality 

Compare different versions v1 and v2 using statistical significance tests to validate improvements

















